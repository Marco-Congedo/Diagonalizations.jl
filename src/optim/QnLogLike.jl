#  Unit "QnLogLike.jl" of the Diagonalization.jl Package for Julia language
#
#  MIT License
#  Copyright (c) 2020,
#  Marco CongedoÂ°, Ronald Phlypo, CNRS, UGA, Grenoble-INP, France
#  Alexandre GramfortÂ¨, INRIA, U. Paris Saclay, France
#  Â° https://sites.google.com/site/marcocongedo/
#  Â¨ http://alexandre.gramfort.net/

# ? CONTENTS :
#  Quasi-Newton Approximate Joint Diagonalization (AJD) algorithm of:
#  P. Ablin, J.F. Cardoso and A. Gramfort. Beyond Pham's algorithm
#  for joint diagonalization. Proc. ESANN 2019.
#  https://hal.archives-ouvertes.fr/hal-01936887v1
#
#  Code adapted in Julia from Python code provided from the authors at:
#  https://github.com/pierreablin/qndiag/blob/master/qndiag/qndiag.py
#
#  NOTATION
#  lower-case letter: a scalar, e.g.: a
#  Upper case letter: a matrix, e.g., A
#  Bold lover case letter: a vector of matrices, e.g., ğ€
#
#  The algorithm takes as input a vector of k real symmetric matrices
#  ğ‚ and find a non-singular matrix B such that the congruences
#  B'*ğ‚_Îº*B are as diagonal as possible for all Îº=1:k.
#
#  `w` is an optional vector of k positive weights for each matrix in ğ‚.
#  if `w` is different from `nothing` (default), the input matrices are
#  weighted with these weights.
#  A function can be passed as the `w` argument, in which case the kth weight
#  is found as the output of the function applied to the kth matrix in ğ‚.
#  A good choice in general is the `nonD` function declared in tools.jl unit.
#
#  if `whitening` = true is passed, the Jeffrey mean of the matrices in ğ‚ is
#  computed (using the PosDefManifold.jl package) and the matrices in ğ‚
#  are pre-transformed using the whitening matrix of the mean.
#  Dimensionality reduction can be obtained at this stage using optional
#  arguments `eVar` and `eVarMeth` (see documentation of the AJD constructors).
#
#  if sort=true (default) the column vectors of the B matrix are normalized
#  to unit norm and permuted so as to sort in descending order the mean over
#  Îº=1:k of the diagonal elements of B'*ğ‚_Îº*B.
#  Note that if `whitening` is true the output B will not have unit norm
#  columns, as it is multiplied by the whitener after being scaled and sorted
#  and before being returned.
#
#  if  `whitening` = false (default), a matrix can be provided with the `init`
#  argument in order to initialize B. In this case the actual AJD
#  will be given by init*B, where B is the output of the algorithm.
#
#  `tol` is the convergence to be attained. It default to the square root of
#  the machine epsilon for the data input type.
#
#  `maxiter` is the maximum number of iterations allowed.
#
#  ğœ†min is used to reguarized Hessian coefficients; all coefficients smaller
#  than ğœ†min  will be set to ğœ†min
#
#  lsmax is the maximum number of step in the line search.
#
#  if `verbose`=true, the convergence attained at each iteration and other
#  information will be printed.
#
#  return: B, its pseudo-inverse, the mean diagonal elements of B'*mean(ğ‚)*B,
#           the number of iterations and the convergence attained.
#
#  Note on the implementation:
#  GRADIENT:
#  D_1,...,D_n are the matrices in ğƒ;
#  the jth column in D_i is divided by D_i[j, j],
#  the mean of these matrices is then taken and finally
#  the identity is subtracted
#
#  HESSIAN COEFFICIENTS
#  for each COLUMN vector dg_1,...,dg_n
#  (the diagonal part of the n matrices in ğƒ)
#  we form a mÂ·m matrix stacking vertically m copies of these vectors
#  transposed and dividing each of them by the mth element, as
#   _                   _
#  |  (dg_i)'/dg_i[1]   |
#  |  (dg_i)'/dg_i[...] |
#  |  (dg_i)'/dg_i[m]   |
#  _                   _
#  finally we take the mean of all the n matrices created in this way.

# function to get the weights from argment `w`
function _qnlogLikeWeights!(w, ğ‚)
	if w isa Function w=[w(C) for Câˆˆğ‚] end
	return w./mean(w)
end

function qnLogLike( ğ‚::Union{Vector{Hermitian}, Vector{Symmetric}};
                    w           :: 	Twf   = â—‹,
                    preWhite    :: 	Bool = false,
                    sort        :: 	Bool = true,
                    init        :: 	Union{Matrix, Nothing} = â—‹,
                    tol         :: 	Real = 0.,
                    maxiter     :: 	Int  = 1000,
                    ğœ†min        ::	 Real = 1e-4,
                    lsmax       :: 	Int  = 10,
                    verbose     :: 	Bool = false,
					threaded	:: 	Bool =
									begin
										thr=Threads.nthreads()
										length(ğ‚) â‰¥ 2*thr && thr>1
									end,
                 eVar     :: TeVaro = â—‹,
                 eVarMeth :: Function = searchsortedfirst)

    # internal functions
    function _linesearch!(B, ğƒ, â†’, ğ¯, loss, lsmax)
		i = 2.
		@label linesearch
        M = (i/2. * â†’) + I
		Bâ‚Š = B * M
		if threaded
			ğƒâ‚Š = HermitianVector(undef, length(ğƒ))
			@threads for j âˆˆ 1:length(ğƒ) ğƒâ‚Š[j] = Hermitian(M'*ğƒ[j]*M) end
		else
			ğƒâ‚Š = [Hermitian(M'*D*M) for D âˆˆ ğƒ]
		end
		# Kullback-Leibler loss
		lossâ‚Š =	w===â—‹ ? -(logabsdet(Bâ‚Š)[1]) + 0.5*sum(mean(log, [ğ”»(D) for D âˆˆ ğƒâ‚Š])) :
						-(logabsdet(Bâ‚Š)[1]) + 0.5*sum(mean(log, [ğ”»(D)*v for (D, v) âˆˆ zip(ğƒâ‚Š, ğ¯)]))
        if lossâ‚Š â‰¥ loss && i < lsmax
			i /= 2.
			@goto linesearch
		end
        return ğƒâ‚Š, Bâ‚Š, lossâ‚Š
    end

	# pre-whiten or initialize or just copy
    W, ğƒ = _preWhiteOrInit(ğ‚, preWhite, Jeffrey, eVar, eVarMeth, init, :Hvector)
	ğ¯ = w===â—‹ ? â—‹ : _qnlogLikeWeights!(w, ğ‚) # if w is `nonD` function, apply it to the original input ğ‚

    # set variables
    iter, conv, lossâ‚Š, ğŸ˜‹, sqrtn = 1, 0., 0., false, âˆšsize(ğƒ[1], 1)
    B = Matrix{eltype(ğƒ[1])}(I, size(ğƒ[1]))
	loss = w===â—‹ ? 	0.5*sum(mean(log, [ğ”»(D) for D âˆˆ ğƒ])) :
					0.5*sum(mean(log, [ğ”»(D)*v for (D, v) âˆˆ zip(ğƒ, ğ¯)]))

    verbose && println("Iterating "*(threaded ? "multi-threaded" : "")*" quasi-Newton LogLike algorithm...")
    while true
        diagonals = [diag(D) for D âˆˆ ğƒ]

		# Gradient
		if w===â—‹
			âˆ‡ = mean(d./diagd for (d, diagd) âˆˆ zip(ğƒ, diagonals)) - I
		else
			âˆ‡ = mean(v.*(d./diagd) for (v, d, diagd) âˆˆ zip(ğ¯, ğƒ, diagonals)) - I
		end
        conv = norm(âˆ‡)/sqrtn # relative norm of âˆ‡ with respect to the identity : ||âˆ‡-I||/||I||

        verbose && println("iteration: ", iter, "; convergence: ", conv)
        (overRun = iter > maxiter) && @warn("qnLogLike: reached the max number of iterations before convergence:", iter-1)
        (ğŸ˜‹ = conv <= tol) || overRun==true ? break : iter += 1

		# Hessian Coefficients
		if w===â—‹
			â„Œ = mean(diagd'./diagd for diagd âˆˆ diagonals)
		else
			â„Œ = mean(v.*(diagd'./diagd) for (v, diagd) âˆˆ zip(ğ¯, diagonals))
		end

		# Quasi-Newton Direction â†’
        â†’ = -(âˆ‡' .* â„Œ - âˆ‡)./replace(x -> x<ğœ†min ? ğœ†min : x, @. (â„Œ'*â„Œ) - 1.)

		# Line Search
        ğƒ, B, loss = _linesearch!(B, ğƒ, â†’, ğ¯, loss, lsmax)
    end
    verbose && @info("Convergence has "*(ğŸ˜‹ ? "" : "not ")*"been attained.\n\n")

    # scale and permute the vectors of B
    Î» = sort ? _permute!(_scale!(B, mean(ğ”»(D) for D âˆˆ ğƒ), size(ğƒ[1], 1))...) :
                diag(mean(ğ”»(D) for D âˆˆ ğƒ))

    return preWhite ? (W.F*B, pinv(B)*W.iF, Î», iter, conv) :
                      (B, pinv(B), Î», iter, conv)
end

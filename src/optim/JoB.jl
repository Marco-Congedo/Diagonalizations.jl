#   Unit "JoB.jl" of the Diagonalization.jl Package for Julia language
#
#   MIT License
#   Copyright (c) 2019-2025,
#   Marco Congedo, CNRS, Grenoble, France:
#   https://sites.google.com/site/marcocongedo/home

# ? CONTENTS :
#   This unit implements a very general form of the OJoB and NoJoB algoithms
#   (Congedo et al., 2012)

# """
#  Orthogonal Joint Blond Source Separation (OJoB) iterative algorithm and
#  Nonorthogonal Joint Blond Source Separation (NoJoB) iterative algorithm:
#  'Orthogonal and Non-Orthogonal Joint Blind Source Separation in the
#  Least-Squares Sense'
#  by Marco Congedo, Ronald Phlypo, Jonas Chatel-Goldman, EUSIPCO 2012
#  https://hal.archives-ouvertes.fr/hal-00737835/document
#
# if `algo` is :OJoB, runs the OJoB algorithm
# if `algo` is :NoJoB, runs the NoJoB algorithm

#  These algorithms handle several use cases corresponding to several
#  combinations of the number of datasets (m) and number of observations (k):
#  1) m=1, k>2
#  2) m>1, k=1
#  3) m>1, k>1
#
#  They take as input either the data matrices or the covariance and cross-
#  covariance matrices. Argument `input` tells the algos to take as input
#  data matrices (`input=:d`) or cov/cross-cov matrices (`input=:c`)
#
#                                   INPUT
# ---------------------------------------------------------------------------
#             Data Matrices                  Cov/cross-cov matrices
# ---------------------------------------------------------------------------
#  1) a vector of k data matrices     an array (k, 1, 1) of covariance matrices
#  2) a vector of m data matrices     an array (1, m, m) of covariance matrices
#  3) k vectors of m data matrices    an array (k, m, m) of covariance matrices
#
#  If data matrices are given as input, the covariance matrices ğ’ are computed
#  with optional keyword arguments `covEst`, `dims`, `meanX`, `trace1` and `w`.
#
#  OJoB seeks m orthogonal matrices ğ”=U_1,...,U_m such that the sum of the
#  squares of the diagonal part of matrices U_i'ğ’(Îºij)U_j
#  is maximized for all iâ‰ j=1:m and Îº=1:k.
#  The covariance is maximized rotating each data matrix as: U_i' X_Îºi,
#  for all i=1:m and Îº=1:k.
#
#  NoJoB seeks m non-singular matrices ğ”=U_1,...,U_m with vectors
#  u_qi, for all i=1:n scaled such that the sum u_qi'(ğ’(Îºj)u_qju_qj^Hğ’(Îºj)^H)u_qi=1
#  where the sum is over Îº=1:k and j=1:m, separetedly for all q=1:m
#  The covariance is maximized transforming each data matrix as: U_i' X_Îºi,
#  for all i=1:m and Îº=1:k.
#
#  if `fullModel=true` is passed as optional keyword argument,
#  then also the sum of the squares of the diagonal part of matrices
#  U_i'ğ’(Îºij)U_j is maximized for all ``i=1:m`` and ``Îº=1:k``.
#  This amounts to require the intra-covariance to be diagonalized as well.
#
#  if `whitening` = true is passed as optional keyword argument,
#  for OJoB:
#       the ğ”=U_1,...,U_m are no longer constrained to be orthogonal;
#       instead they will be invertible and a scaling constraint is imposed on them
#       so as to satisfy mean_Îº=1:k U_i'ğ’(Îºii)U_j=I for all i=1:m.
#  for NoJoB:
#       The same constraint is imposed
#
#  For both algorithms, given m whitening matrices W_i,
#  the covariance matrices will be transformed
#  as ğ’¢(Îºij) = W_i'ğ’(Îºij)W_j, for all i,j=1:m and Îº=1:k.
#  Then, the OJoB or NoJob algos will be run on the set ğ’¢(Îºij)
#  and the final solutions will be given by U_iW_i, for all i=1:m
#  Using OjOB, this amounts to requiring a generalized Canonical Correlation
#  Analysis instead of a generalized Maximum Covariance Analysis.
#  By this pre-whitening the dimension of the problem can be reduced
#  requesting matrices W_i to be rectangular. This is achieved using
#  argument `eVar`. If eVar is an integer p, the matrices in ğ’¢(Îºij) will all
#  be pxp. If it is equal to nothing the explained variance is set to
#  the default level (0.999), otherwise if eVar is a float in [0..1], this
#  will be taken as explained variance.
#  if m=1 the eigenvalues of mean_Îº=1^k ğ’(Îº11) is considered, otherwise
#         the eigenvalues of mean_Îº=1^k, i=1^m ğ’(Îºii) is considered.
#  These eigenvalues are normalized to unit sum and the cumulated sum is
#  considered. Finally p is found on this accumulated regularized ev vector
#  using the `eVarMethod` function.
#
#  if sort=true (default) the column vectors of the ğ” matrices are reordered
#  so as to allow positive and sorted in descending order
#  diagonal elements of the products ``U_i'ğ’(Îºij)U_j`` as much as possible.
#  If the NoJoB algorithm is used the column vectors of the matrices U_i
#  are normalized to unit norm.
#  Note that if `whitening` is true the output matrices U_i will not have
#  unit norm columns as they are multiplied by the whiteners after being
#  scaled and sorted and before being returned.

#  By passing a matrix if k=1 or a vector of k matrices if k>1 as `init`,
#  you can smartly initialize `ğ”`, for example, when excluding some subjects
#  and running the OJoB again.
#
#  `tol` is the convergence to be attained.
#
#  `maxiter` is the maximum number of iterations allowed.
#
#  if `verbose`=true, the convergence attained at each iteration and other
#  information will be printed.
#
#  if `threaded`=true (default) and n>x and x>1, where x is the number
#  of threads Julia is instructed to use (the output of Threads.nthreads())
#  the algorithms run in multithreaded mode paralellising several comptations
#  over n.
#
#  Besides optionally multi-threaded, these algorithms heavely use BLAS.
#  Before running this function you may want to set the number of threades
#  Julia is instructed to use to the number of logical CPUs of your machine
#  and set `BLAS.set_num_threads(Sys.CPU_THREADS)`. See:
#  https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#Threads-1
# """
function JoB(ğ—::AbstractArray, m::Int, k::Int, input::Symbol, algo::Symbol, type;
              covEst   :: StatsBase.CovarianceEstimator=SCM,
              dims     :: Int64 = 1,
              meanX    :: Tmean = 0,
              trace1   :: Bool = false,
              w        :: Twf = â—‹,
          fullModel :: Bool = false,
          preWhite  :: Bool = false,
          sort      :: Bool = true,
          init      :: Union{Matrix, Nothing} = â—‹,
          tol       :: Real = 0.,
          maxiter   :: Int  = 1000,
          verbose   :: Bool = false,
          threaded  :: Bool = true,
      eVar     :: TeVaro = â—‹,
      eVarMeth :: Function = searchsortedfirst)

    k<3 && m<2 && throw(ArgumentError("Either `k` must be equal to at least 2 or `m` must be equal to at least 2"))
    #if m==1 fullModel=false end

    # if input â‰ :d the input is supposed to be the cross-covariance matrices
    input==:d ? ğ’=_crossCov(ğ—, m, k;
                            covEst=covEst, dims=dims, meanX=meanX) : ğ’=ğ—

    ğ’¢=deepcopy(ğ’)
    if trace1 || w â‰  â—‹ _normalize!(ğ’¢, m, k, trace1, w) end

    if eVar isa Int && size(ğ’¢[1, 1, 1], 1) â‰¤ eVar â‰¤ 0
        eVar=size(ğ’¢[1, 1, 1], 1)
    end

    iter, conv, oldconv, ğŸ˜‹, conv_ = 1, 0., 1.01, false, 0.
    tol==0. ? tolerance = âˆšeps(real(type)) : tolerance = tol

    # pre-whitening
    if preWhite
        # for the moment being computed on the average across k and m
        # if eVar is not an Int.
        # This way for all i=1:m the dimensionality reduction is fixed
        # for the case m=1 this is the classical whitening
        if !(eVar isa Int)
            G=â„(mean(ğ’¢[Îº, i, i] for Îº=1:k, i=1:m))
            p, arev = _getssd!(eVar, eigvals(G), size(ğ’¢[1, 1, 1], 1), eVarMeth)
            ğ‘¾=[whitening(â„(mean(ğ’¢[Îº, i, i] for Îº=1:k)); eVar=p) for i=1:m]
        else
            ğ‘¾=[whitening(â„(mean(ğ’¢[Îº, i, i] for Îº=1:k)); eVar=eVar) for i=1:m]
        end

        if m==1
            @inbounds for Îº=1:k
                ğ’¢[Îº, 1, 1] = ğ‘¾[1].F' * ğ’¢[Îº, 1, 1] * ğ‘¾[1].F
            end
        else
            # off-diagonal blocks
            @inbounds for Îº=1:k, i=1:m-1, j=i+1:m
                ğ’¢[Îº, i, j] = ğ‘¾[i].F' * ğ’¢[Îº, i, j] * ğ‘¾[j].F
                ğ’¢[Îº, j, i] = ğ’¢[Îº, i, j]'
            end
            # diagonal blocks
            @inbounds for Îº=1:k, i=1:m
                ğ’¢[Îº, i, i] = ğ‘¾[i].F' * ğ’¢[Îº, i, i] * ğ‘¾[i].F
            end
        end
    end
    n=size(ğ’¢[1, 1, 1], 1)

    # determie whether running in multi-threaded mode
    â© = n>=Threads.nthreads() && Threads.nthreads()>1 && threaded

    # initialization of ğ”[i], i=1:m, as the eigenvectors of sum_k,j(ğ’¢_k,i,j*ğ’¢_k,i,j')
    # see Eq. 17 of Congedo, Phlypo & Pham(2011), or with the provided matrices
    # note: BLAS.gemm supports complex matrices
    ggt(Îº::Int, i::Int, j::Int) = BLAS.gemm('N', 'T', ğ’¢[Îº, i, j], ğ’¢[Îº, i, j])
    if m>1 # gmca, gcca, majd
        if init === nothing
            if algo==:NoJoB
                ğ” = [Matrix{type}(I, n, n) for i=1:m]
            else
                if fullModel
                    ğ” = [eigvecs(mean(ggt(Îº, i, j) for j=1:m, Îº=1:k)) for i=1:m]
                else
                    ğ” = [eigvecs(mean(ggt(Îº, i, j) for j=1:m, Îº=1:k if jâ‰ i)) for i=1:m]
                end
            end
        else
            ğ” = init
        end
    else  # ajd
        if init === nothing
            if algo==:NoJoB
                ğ” = [Matrix{type}(I, n, n)]
            else
                ğ” = [eigvecs(mean(ggt(Îº, 1, 1) for Îº=1:k))]
            end
        else
            ğ” = [init]
        end
    end

    function updateR!(Î·, i, j)
        # ğ‘[Î·] += (ğ’¢[Îº, i, j] * ğ”[j][:, Î·]) times its transpose
        # don't use BLAS for complex data
        if â© # if threaded don't share memory for Î© but use ğ›€
            if type<:Real
                @inbounds for Îº=1:k
                    ğ›€[Î·][:, Îº] = BLAS.gemv('N', ğ’¢[Îº, i, j], ğ”[j][:, Î·])
                end
                ğ‘[Î·] += Hermitian(BLAS.gemm('N', 'T', ğ›€[Î·], ğ›€[Î·])) # (Î© * Î©')
            else
                @inbounds for Îº=1:k
                    ğ›€[Î·][:, Îº] = ğ’¢[Îº, i, j] * ğ”[j][:, Î·]
                end
                ğ‘[Î·] += Hermitian(ğ›€[Î·] * ğ›€[Î·]') # (Î© * Î©')
            end
        else
            if type<:Real
                @inbounds for Îº=1:k
                    Î©[:, Îº] = BLAS.gemv('N', ğ’¢[Îº, i, j], ğ”[j][:, Î·])
                end
                ğ‘[Î·] += Hermitian(BLAS.gemm('N', 'T', Î©, Î©)) # (Î© * Î©')
            else
                @inbounds for Îº=1:k
                    Î©[:, Îº] = ğ’¢[Îº, i, j] * ğ”[j][:, Î·]
                end
                ğ‘[Î·] += Hermitian(Î© * Î©') # (Î© * Î©')
            end
        end
    end

    function update!(m, n, i)

        function udR1!(Î·) # case m=1
            fill!(ğ‘[Î·], type(0))
            updateR!(Î·, 1, 1)
        end

        function udRm!(Î·, i)  # case m>1
            fill!(ğ‘[Î·], type(0))
            for j=1:m iâ‰ j ? updateR!(Î·, i, j) : nothing end # j â‰  i
            fullModel ? updateR!(Î·, i, i) : nothing         # j = i
        end

        if m==1
            â© ? (@threads for Î·=1:n udR1!(Î·) end) : (for Î·=1:n udR1!(Î·) end)
        else
            â© ? (@threads for Î·=1:n udRm!(Î·, i) end) : (for Î·=1:n udRm!(Î·, i) end)
        end
    end

    # pre-allocate memory
    ğ‘ = HermitianVector([Hermitian(zeros(type, n, n)) for Î·=1:n])
    â© ? (ğ›€ = [Matrix{type}(undef, n, k) for Î·=1:n]) :
         (Î© = Matrix{type}(undef, n, k))

    # # # # # here starts the algorithm

    if algo==:OJoB
        verbose && @info("Iterating OJoB algorithm...")
        while true
            conv_ =0.
            @inbounds for i=1:m # m optimizations for updating ğ”[1]...ğ”[m]

                update!(m, n, i)

                # do 1 power iteration, not worth threading here
                for Î·=1:n ğ”[i][:, Î·] = ğ‘[Î·] * ğ”[i][:, Î·] end

                # sum of squares of the norm of power iteration vectors
                conv_ += PosDefManifold.ss(ğ”[i])

                # Lodwin Orthogonalization and update ğ”[i]<-UV', with svd(ğ”[i])=UwV'
                ğ”[i] = PosDefManifold.nearestOrth(ğ”[i])
            end

            conv_ =sqrt(conv_ /(n^2*m))
            iter==1 ? conv=1. : conv = abs((conv_-oldconv)/oldconv)  # relative change

            verbose && println("iteration: ", iter, "; convergence: ", conv)
            (diverging = conv < 0) && verbose && @warn("OJoB diverged at:", iter)
            (overRun = iter == maxiter) && @warn("OJoB: reached the max number of iterations before convergence:", iter)
            (ğŸ˜‹ = 0. <= conv <= tolerance) || overRun==true ? break : nothing
            oldconv=conv_
            iter += 1
        end # while
    else # NoJoB
        verbose && @info("Iterating NoJoB algorithm...")

        # solve Lx=ğ‘[Î·]*ğ”[i][:, Î·] for x and L'y=x for y
        # and scale the Î·th column as ğ”[i][:, Î·] <- y/sqrt(y'ğ‘[Î·]t)
        function triS!(cho, Î·,  i)
            y=cho.U\(cho.L\(ğ‘[Î·]*ğ”[i][:, Î·]))
            ğ”[i][:, Î·]=y*inv(sqrt(PosDefManifold.qf(y, ğ‘[Î·])))
        end

        # thread sum of ğ‘[Î·] and the solutions to triangular systems
        # only if n is at least Threads.nthreads()*2 (no worth otherwise)
        â©x = â© && nâ‰¥Threads.nthreads()*2

        while true
            conv_ =0.
            @inbounds for e2=1:2, i=1:m # m optimizations for updating ğ”[1]...ğ”[m]
                                        # double loop to avoid oscillating convergence
                update!(m, n, i)

                # do 1 power iteration
                cho=cholesky(â©x ? fVec(sum, ğ‘) : sum(ğ‘)) # Cholesky LL'of ğ‘[1]+...+ğ‘[n]

                â©x ? (@threads for Î·=1:n triS!(cho, Î·,  i) end) :
                            (for Î·=1:n triS!(cho, Î·,  i) end)

                conv_+=PosDefManifold.ss(ğ”[i])
            end

            conv_=conv_/(2*n^2*m) # 2 because of the double loop
            iter==1 ? conv=1. : conv = abs((conv_-oldconv)/oldconv)  # relative change

            verbose && println("iteration: ", iter, "; convergence: ", conv)
            (diverging = conv < 0) && verbose && @warn("NoJoB diverged at:", iter)
            (overRun = iter == maxiter) && @warn("NoJoB: reached the max number of iterations before convergence:", iter)
            (ğŸ˜‹ = 0. <= conv <= tolerance) || overRun==true ? break : nothing
            oldconv=conv_
            iter += 1
        end # while
    end

    verbose && @info("Convergence has "*(ğŸ˜‹ ? "" : "not ")*"been attained.\n\n")


    # scale and permute the vectors of U_1,...,U_m
    if sort
        algo==:NoJoB ? for i=1:m normalizeCol!(ğ”[i], 1:size(ğ”[i], 2)) end : nothing
        Î» = m==1 ? _permute!(ğ”[1], ğ’¢, k, :c) :
                   _flipAndPermute!(ğ”, ğ’¢, m, k, :c)
    else
        Î» = m==1 ? mean([ğ”[1][:, Î·]'*ğ’¢[l, 1, 1]*ğ”[1][:, Î·] for Î·=1:n] for l=1:k) :
                   mean([ğ”[i][:, Î·]'*ğ’¢[l, i, j]*ğ”[j][:, Î·] for Î·=1:n] for l=1:k, j=1:m, i=1:m if iâ‰ j)
    end

    if preWhite
        algo==:OJoB ? ğ•=[ğ”[i]'*ğ‘¾[i].iF for i=1:m] :
                      ğ•=[pinv(ğ”[i])*ğ‘¾[i].iF for i=1:m] # algo==:NoJoB
        for i=1:m ğ”[i] = ğ‘¾[i].F * ğ”[i] end
    else
        algo==:OJoB ? ğ•=[Matrix(ğ”[i]') for i=1:m] :
                      ğ•=[pinv(ğ”[i]) for i=1:m] # algo==:NoJoB
    end

    return m>1 ? (ğ”, ğ•, Î», iter, conv) : (ğ”[1], ğ•[1], Î», iter, conv)
end

#   Unit "JoB.jl" of the Diagonalization.jl Package for Julia language
#
#   MIT License
#   Copyright (c) 2019,
#   Marco Congedo, CNRS, Grenoble, France:
#   https://sites.google.com/site/marcocongedo/home

# ? CONTENTS :
#   This unit implements a very general form of the OJoB algoithm
#   (Congedo et al., 2012)

# """
#  Orthogonal Joint Blond Source Separation (OJoB) iterative algorithm and
#  Nonorthogonal Joint Blond Source Separation (NoJoB) iterative algorithm:
#  'Orthogonal and Non-Orthogonal Joint Blind Source Separation in the
#  Least-Squares Sense'
#  by Marco Congedo, Ronald Phlypo, Jonas Chatel-Goldman, EUSIPCO 2012
#  https://hal.archives-ouvertes.fr/hal-00737835/document
#
# if `algo` is :OJoB, runs the OJoB algorithm
# if `algo` is :NoJoB, runs the NoJoB algorithm

#  These algorithms handle several use cases corresponding to several
#  combinations of the number of datasets (m) and number of observations (k):
#  1) m=1, k>2
#  2) m>1, k=1
#  3) m>1, k>1
#
#  They take as input either the data matrices or the covariance and cross-
#  covariance matrices. Argument `input` tells the algos to take as input
#  data matrices (`input=:d`) or cov/cross-cov matrices (`input=:d`)
#
#                                   INPUT
# ---------------------------------------------------------------------------
#             Data Matrices                  Cov/cross-cov matrices
# ---------------------------------------------------------------------------
#  1) a vector of k data matrices     an array (k, 1, 1) of covariance matrices
#  2) a vector of m data matrices     an array (1, m, m) of covariance matrices
#  3) k vectors of m data matrices    an array (k, m, m) of covariance matrices
#
#  If data matrices are given as input, the covariance matrices ğ’ are computed
#  with optional keyword arguments `covEst`, `dims`, `meanX`, `trace1` and `w`.
#
#  OJoB seeks m orthogonal matrices ğ”=U_1,...,U_m such that the sum of the
#  squares of the diagonal part of matrices U_i'ğ’(Îºij)U_j
#  is maximized for all iâ‰ j=1:m and Îº=1:k.
#  The covariance is maximized rotating each data matrix as: U_i' X_Îºi,
#  for all i=1:m and Îº=1:k.
#
#  NoJoB seeks m non-singular matrices ğ”=U_1,...,U_m with vectors
#  u_qi, for all i=1:n scaled such that the sum u_qi'(ğ’(Îºj)u_qju_qj^Hğ’(Îºj)^H)u_qi=1
#  where the sum is over Îº=1:k and j=1:m, separetedly for all q=1:m
#  The covariance is maximized transforming each data matrix as: U_i' X_Îºi,
#  for all i=1:m and Îº=1:k.
#
#  if `fullModel=true` is passed as optional keyword argument,
#  then also the sum of the squares of the diagonal part of matrices
#  U_i'ğ’(Îºij)U_j is maximized for all ``i=1:m`` and ``Îº=1:k``.
#  This amounts to require the intra-covariance to be diagonalized as well.
#
#  if `whitening` = true is passed as optional keyword argument,
#  for OJoB:
#       the ğ”=U_1,...,U_m are no longer constrained to be orthogonal;
#       instead they will be invertible and a scaling constraint is imposed on them
#       so as to satisfy mean_Îº=1:k U_i'ğ’(Îºii)U_j=I for all i=1:m.
#  for NoJoB:
#       The same constraint is imposed
#
#  For both algorithms, given m whitening matrices W_i,
#  the covariance matrices will be transformed
#  as ğ’¢(Îºij) = W_i'ğ’(Îºij)W_j, for all i,j=1:m and Îº=1:k.
#  Then, the OJoB or NoJob algos will be run on the set ğ’¢(Îºij)
#  and the final solutions will be given by U_iW_i, for all i=1:m
#  Using OjOB, this amounts to requiring a generalized Canonical Correlation
#  Analysis instead of a generalized Maximum Covariance Analysis.
#  By this pre-whitening the dimension of the problem can be reduced
#  requesting matrices W_i to be rectangular. This is achieved using
#  argument `eVar`. If eVar is an integer p, the matrices in ğ’¢(Îºij) will all
#  be pxp. If it is equal to nothing the explained variance is set to
#  the default level (0.999), otherwise if eVar is a float in [0..1], this
#  will be taken as explained variance.
#  if m=1 the eigenvalues of mean_Îº=1^k ğ’(Îº11) is considered, otherwise
#         the eigenvalues of mean_Îº=1^k, i=1^m ğ’(Îºii) is considered.
#  These eigenvalues are normalized to unit sum and the cumulated sum is
#  considered. Finally p is found on this accumulated regularized ev vector
#  using the `eVarMethod` function.
#
#  if sort=true (default) the column vectors of the ğ” matrices are reordered
#  so as to allow positive and sorted in descending order
#  diagonal elements of the products ``U_i'ğ’(Îºij)U_j``.

#  By passing a matrix if k=1 or a vector of k matrices if k>1 as `init`,
#  you can smartly initialize `ğ”`, for example, when excluding some subjects
#  and running the OJoB again.
#
#  `tol` is the convergence to be attained.
#
#  `maxiter` is the maximum number of iterations allowed.
#
#  if `verbose`=true, the convergence attained at each iteration and other
#  information will be printed.
#
#  These algorithms are not multi_threaded, instead they heavely use BLAS.
#  Before running this function you may want to set:
#  `BLAS.set_num_threads(Sys.CPU_THREADS)`
# """
function JoB(ğ—::AbstractArray, m::Int, k::Int, input::Symbol, algo::Symbol, type;
              covEst   :: StatsBase.CovarianceEstimator=SCM,
              dims     :: Int64 = 1,
              meanX    :: Tmean = 0,
              trace1   :: Bool = false,
              w        :: Union{Tw, Function} = â—‹,
          fullModel :: Bool = false,
          preWhite  :: Bool = false,
          sort      :: Bool = true,
          init      = nothing,
          tol       :: Real=0.,
          maxiter   :: Int=1000,
          verbose   :: Bool=false,
      eVar     :: TeVaro = â—‹,
      eVarMeth :: Function = searchsortedfirst)

    k<3 && m<2 && throw(ArgumentError("Either `k` must be equal to at least 3 or `m` must be equal to at least 2"))
    #if m==1 fullModel=false end

    # if input â‰ :d the input is supposed to be the cross-covariance matrices
    input==:d ? ğ’=_crossCov(ğ—, m, k;
                            covEst=covEst, dims=dims, meanX=meanX) : ğ’=ğ—

    if trace1 || w â‰  â—‹ _Normalize!(ğ’, m, k, trace1, w) end

    if eVar isa Int && size(ğ’[1, 1, 1], 1) â‰¤ eVar â‰¤ 0
        eVar=size(ğ’[1, 1, 1], 1)
    end

    iter, conv, oldconv, converged, conv_ = 1, 0., 1.01, false, 0.
    tol==0. ? tolerance = âˆšeps(real(type)) : tolerance = tol

    # pre-whitening
    if preWhite
        # for the moment being computed on the average across k and m
        # if eVar is not an Int.
        # This way for all i=1:m the dimensionality reduction is fixed
        if !(eVar isa Int)
            C=â„(ğ›(ğ’[Îº, i, i] for Îº=1:k, i=1:m))
            #W=whitening(C; eVar=eVar, eVarMeth=eVarMeth)
            p, arev = _getssd!(eVar, eigvals(C), size(ğ’[1, 1, 1], 1), eVarMeth)
            ğ‘¾=[whitening(â„(ğ›(ğ’[Îº, i, i] for Îº=1:k)); eVar=p) for i=1:m]
        else
            ğ‘¾=[whitening(â„(ğ›(ğ’[Îº, i, i] for Îº=1:k)); eVar=eVar) for i=1:m]
        end

        ğ’¢=Array{Matrix}(undef, k, m, m)
        #println("k, m: ", k, " ", m)
        if m==1
            @inbounds for Îº=1:k
                ğ’¢[Îº, 1, 1] = ğ‘¾[1].F' * ğ’[Îº, 1, 1] * ğ‘¾[1].F
            end
        else
            @inbounds for Îº=1:k, i=1:m-1, j=i+1:m
                ğ’¢[Îº, i, j] = ğ‘¾[i].F' * ğ’[Îº, i, j] * ğ‘¾[j].F
                ğ’¢[Îº, j, i] = ğ’¢[Îº, i, j]'
            end
            @inbounds for Îº=1:k, i=1:m
                ğ’¢[Îº, i, i] = ğ‘¾[i].F' * ğ’[Îº, i, i] * ğ‘¾[i].F
            end
        end
    else
        ğ’¢=ğ’
    end
    n=size(ğ’¢[1, 1, 1], 1)

    # initialization of ğ”[i], i=1:m, as the eigenvectors of sum_k,j(ğ’¢_k,i,j*ğ’¢_k,i,j')
    # see Eq. 17 of Congedo, Phlypo and Pham, 2011, or with the provided matrices
    # note: gemm supports complex matrices
    ggt(Îº::Int, i::Int, j::Int) = BLAS.gemm('N', 'T', ğ’¢[Îº, i, j], ğ’¢[Îº, i, j])
    if m>1 #gmca, gcca, majd
        if init === nothing
            if algo==:NoJoB
                ğ” = [Matrix{type}(I, n, n) for i=1:m]
            else
                if fullModel
                    ğ” = [eigvecs(ğ›(ggt(Îº, i, j) for j=1:m, Îº=1:k)) for i=1:m]
                else
                    ğ” = [eigvecs(ğ›(ggt(Îº, i, j) for j=1:m, Îº=1:k if jâ‰ i)) for i=1:m]
                end
            end
        else
            ğ” = init
        end
    else  #ajd
        if init === nothing
            if algo==:NoJoB
                ğ” = [Matrix{type}(I, n, n)]
            else
                ğ” = [eigvecs(ğ›(ggt(Îº, 1, 1) for Îº=1:k))]
            end
        else
            ğ” = [init]
        end
    end

    function updateR!(Î·, i, j)  # ğ‘[Î·] += (ğ’¢[Îº, i, j] * ğ”[j][:, Î·]) times its transpose
        #println("k, i, j ", k, " ", i, " ", j)
        # both gemv and gemm supports complex input
        @inbounds for Îº=1:k
            Î©[:, Îº] = BLAS.gemv('N', ğ’¢[Îº, i, j], ğ”[j][:, Î·])
        end
        ğ‘[Î·] += BLAS.gemm('N', 'T', Î©, Î©) # (Î© * Î©')
    end

    ğ‘ = [Matrix{type}(undef, n, n) for Î·=1:n]
    Î© = Matrix{type}(undef, n, k)

    if algo==:OJoB
        verbose && @info("Iterating OJoB algorithm...")
        while true
            conv_ =0.
            @inbounds for i=1:m # m optimizations for updating ğ”[1]...ğ”[m]
                for Î·=1:n
                    fill!(ğ‘[Î·], zero(type))
                    if m==1
                        updateR!(Î·, 1, 1)
                    else
                        for j=1:m iâ‰ j ? updateR!(Î·, i, j) : nothing end # j â‰  i
                        fullModel ? updateR!(Î·, i, i) : nothing         # j = i
                    end
                    # 1 power iteration
                    ğ”[i][:, Î·] = BLAS.gemv('N', ğ‘[Î·], ğ”[i][:, Î·])
                end
                conv_ += PosDefManifold.ss(ğ”[i])/n # square of the norms of power iteration vectors

                # Lodwin Orthogonalization and update ğ”[i]<-UV', with svd(ğ”[i])=UwV'
                ğ”[i] = PosDefManifold.nearestOrth(ğ”[i])
            end

            conv_ =sqrt(conv_ /m)
            iter==1 ? conv=1. : conv = abs((conv_-oldconv)/oldconv)  # relative change

            verbose && println("iteration: ", iter, "; convergence: ", conv)
            (diverging = conv < 0) && verbose && @warn("OJoB diverged at:", iter)
            (overRun = iter == maxiter) && @warn("OJoB: reached the max number of iterations before convergence:", iter)
            (converged = 0. <= conv <= tolerance) || overRun==true ? break : nothing
            oldconv=conv_
            iter += 1
        end # while
    else
        verbose && @info("Iterating NoJoB algorithm...")
        while true
            conv_ =0.
            @inbounds for e2=1:2, i=1:m # m optimizations for updating ğ”[1]...ğ”[m]
                for Î·=1:n      # double loop to avoid oscillating convergence
                    fill!(ğ‘[Î·], type(0))
                    if m==1
                        updateR!(Î·, 1, 1)
                    else
                        for j=1:m iâ‰ j ? updateR!(Î·, i, j) : nothing end # j â‰  i
                        fullModel ? updateR!(Î·, i, i) : nothing         # j = i
                    end
                end

                #1 power iteration
                cho=cholesky(Hermitian(sum(ğ‘))) # Cholesky LL'of ğ‘[1]+...+ğ‘[n]
                for Î·=1:n
                    # solve Lx=ğ‘[Î·]*ğ”[i][:, Î·] for x and L'y=x for y
                    y=cho.U\(cho.L\(ğ‘[Î·]*ğ”[i][:, Î·]))
                    # ğ”[i][:, Î·] <- y/sqrt(y'ğ‘[Î·]t)
                    ğ”[i][:, Î·]=y*inv(sqrt(quadraticForm(y, ğ‘[Î·])))
                end
                conv_+=PosDefManifold.ss(ğ”[i])
            end

            conv_=conv_/(2*n^2*m) # 2 because of the double loop
            iter==1 ? conv=1. : conv = abs((conv_-oldconv)/oldconv)  # relative change

            verbose && println("iteration: ", iter, "; convergence: ", conv)
            (diverging = conv < 0) && verbose && @warn("NoJoB diverged at:", iter)
            (overRun = iter == maxiter) && @warn("NoJoB: reached the max number of iterations before convergence:", iter)
            (converged = 0. <= conv <= tolerance) || overRun==true ? break : nothing
            oldconv=conv_
            iter += 1
        end # while
    end

    verbose ? (converged ? @info("Convergence has been attained.\n") : @warn("Convergence has not been attained.")) : nothing
    verbose && println("")

    # auto-sort the eigenvectors
    if sort
        Î» = m==1 ? _permute!(ğ”[1], ğ’¢, k, :c) :
                   _scaleAndPermute!(ğ”, ğ’¢, m, k, :c)
    else
        Î» = m==1 ? ğ›([ğ”[1][:, Î·]'*ğ’¢[l, 1, 1]*ğ”[1][:, Î·] for Î·=1:n] for l=1:k) :
                   ğ›([ğ”[i][:, Î·]'*ğ’¢[l, i, j]*ğ”[j][:, Î·] for Î·=1:n] for l=1:k, j=1:m, i=1:m if iâ‰ j)
    end

    if preWhite
        algo==:OJoB ? ğ•=[ğ”[i]'*ğ‘¾[i].iF for i=1:m] :
                      ğ•=[pinv(ğ”[i])*ğ‘¾[i].iF for i=1:m]
        for i=1:m ğ”[i] = ğ‘¾[i].F * ğ”[i] end
    else
        algo==:OJoB ? ğ•=[Matrix(ğ”[i]') for i=1:m] :
                      ğ•=[pinv(ğ”[i]) for i=1:m]
    end

    return m>1 ? (ğ”, ğ•, Î», iter, conv) : (ğ”[1], ğ•[1], Î», iter, conv)
end

#  Unit "Gajd.jl" of the Diagonalization.jl Package for Julia language
#
#  MIT License
#  Copyright (c) 2019,
#  Marco Congedo, CNRS, Grenoble, France:
#  https://sites.google.com/site/marcocongedo/home

# ? CONTENTS :
#  This unit implements the Gauss AJD algorithm of Congedo (unpublished).

#  The algorithm handles the AJD diagonalization procedure, corresponding
#  to the case m=1, k>1 according to the taxonomy adopted in this package.


#  PRIMITIVE GAJD algorithm:
#  It takes as input a lower triangular matrix
#  holding in its elements vectors of k real numbers.
#  From data in matrix form given as k symmetric C_Îº matrices of size nxn,
#  we have ğ‹[i, j][Îº] = C_Îº[i, j], for Îº=1:k, i>j=1:n.
#  It find a non-singular matrix B such that the
#  congruences B'*C_Îº*B are as diagonal as possible for all Îº=1:k.
#  `tol` is the convergence to be attained.
#  `maxiter` is the maximum number of iterations allowed.
#  if `verbose`=true, the convergence attained at each iteration and other
#  information will be printed.
#  RETURN: B, the number of iterations and the convergence attained (a 3-tuple)
function gajd(ğ‹::AbstractArray; tol = 0., maxiter = 60, verbose = false)

   function congedoSweep!()
      âˆ¡ = T(0.)
      @inbounds for i âˆˆ 1:n
         láµ¢áµ¢ = ğ‹[i, i]
         h = -inv(sum(láµ¢áµ¢.^2))

         # transform all other columns of B with respect to its ith column
         for j âˆˆ filter(x->xâ‰ i, 1:n)
            âŠ¶ = j>i ? (j, i) : (i, j) # pick from lower triangular part only

            Î¸  = h*sum(ğ‹[(âŠ¶ )...].*láµ¢áµ¢) # find optimal theta
            Î¸Â² = Î¸^2
            âˆ¡ += Î¸Â²         # update convergence (âˆ¡)

            # update ğ‚ (lower triangular part only)
            # this is RECURSIVE, hence no multi-threading is possible
            ğ‹[j, j] += Î¸Â²*láµ¢áµ¢ + (2*Î¸)*ğ‹[(âŠ¶ )...]
            for p = 1:j-1 ğ‹[j, p] += iâ‰¥p ? Î¸*ğ‹[i, p] : Î¸*ğ‹[p, i] end
            for p = j+1:n ğ‹[p, j] += iâ‰¥p ? Î¸*ğ‹[i, p] : Î¸*ğ‹[p, i] end

            B[:, j] += Î¸*B[:, i]    # update B
         end # for j
      end
      return âˆ¡*e # convergence: average squared theta over all n(n-1) pairs
   end

   T, n = eltype(ğ‹[1, 1]), size(ğ‹, 1)
   tolerance = tol==0. ? âˆšeps(real(T)) : tol
   iter, conv, ğŸ˜‹, e = 1, 0., false, inv(n*(n-1))

   # initialize AJD
   B=Matrix{T}(I, n, n)

   verbose && @info("Iterating GAJD algorithm...")
   while true
      conv=congedoSweep!()
      verbose && println("iteration: ", iter, "; convergence: ", conv)
      (overRun = iter == maxiter) && @warn("GAJD: reached the max number of iterations before convergence:", iter)
      (ğŸ˜‹ = conv <= tolerance) || overRun==true ? break : iter += 1
   end
   verbose && @info("Convergence has "*(ğŸ˜‹ ? "" : "not ")*"been attained.\n\n")

   return B, iter, conv
end

#  ADVANCED GAJD algorithm:
#  It takes as input a vector of k real symmetric matrices ğ‚ and find a
#  non-singular matrix B such that the congruences B'*ğ‚_Îº*B are as diagonal
#  as possible for all Îº=1:k.

#  if `trace1` is true (false by default), all input matrices are normalized
#  so as to have unit trace. Note that the diagonal elements
#  of the input matrices must be all positive.
#
#  `w` is an optional vector of k positive weights for each matrix in ğ‚.
#  if `w` is different from `nothing` (default), the input matrices are
#  weighted with these weights (after trace normalization if `trace1` is true).
#  A function can be passed as the `w` argument, in which case the kth weight
#  is found as the output of the function applied to the kth matrix in ğ‚.
#  A good choice in general is the `nonD` function declared in tools.jl unit.
#
#  if `whitening` = true is passed, the Arithmetic mean of the matrices in ğ‚ is
#  computed (using the PosDefManifold.jl package) and the matrices in ğ‚
#  are pre-transformed using the whitening matrix of the mean.
#  Dimensionality reduction can be obtained at this stage using optional
#  arguments `eVar` and `eVarMeth` (see documentation of the AJD constructors).
#
#  if sort=true (default) the column vectors of the B matrix are normalized
#  to unit norm and permuted so as to sort in descending order the mean over
#  Îº=1:k of the diagonal elements of B'*ğ‚_Îº*B.
#  Note that if `whitening` is true the output B will not have unit norm columns
#  as it is multiplied by the whitener after being scaled and sorted
#  and before being returned.
#
#  if  `whitening` = false (default), a matrix can be provided with the `init`
#  argument in order to initialize B. In this case the actual AJD
#  will be given by init*B, where B is the output of the algorithm.
#
#  `tol` is the convergence to be attained.
#
#  `maxiter` is the maximum number of iterations allowed.
#
#  if `verbose`=true, the convergence attained at each iteration and other
#  information will be printed.
#
#  return: B, its pseudo-inverse, the mean diagonal elements of B'*mean(ğ‚)*B,
#          the number of iterations and the convergence attained
#
#  NB: Differently from other AJD algorithms, this algorithm proceeds
#  by transformations of one vector at a time given a pair of vectors of B.
#  A sweep goes over all n*(n+1) ij pairs, with i,j âˆˆ 1:n, jâ‰ i.
#  The update of the input matrices is RECURSIVE, thus it is not suitable
#  for multi-threading. This algorithm has the lowest complexity per iteration
#  among all algorithms here implemented and scale extremely well over k, i.e.,
#  for n small and k large it offers the best performance.
function gajd( ğ‚::Union{Vector{Hermitian}, Vector{Symmetric}};
               trace1   :: Bool  = false,
               w        :: Twf   = â—‹,
               preWhite :: Bool  = false,
               sort     :: Bool  = true,
               init     :: Union{Symmetric, Hermitian, Nothing} = â—‹,
               tol      :: Real  = 0.,
               maxiter  :: Int   = 120,
               verbose  :: Bool  = false,
            eVar     :: TeVaro = â—‹,
            eVarMeth :: Function = searchsortedfirst)

   # trace normalization and weighting
   trace1 || w â‰  â—‹ ? begin
      ğ† = deepcopy(ğ‚)
      _Normalize!(ğ†, trace1, w)
   end : ğ† = ğ‚

   # pre-whiten or initialize
   if preWhite
      W = whitening(mean(Euclidean, ğ†); eVar=eVar, eVarMeth=eVarMeth)
      for Îº=1:length(ğ†) ğ†[Îº]=Hermitian(W.F'*ğ†[Îº]*W.F) end
   else
      if initâ‰ nothing for Îº=1:length(ğ†) ğ†[Îº]=Hermitian(W.F'*ğ†[Îº]*W.F) end end
   end

   T, n, k = eltype(ğ†[1]), size(ğ†[1], 1), length(ğ†)

   # arrange data in a LowerTriangular matrix of k-vectors
   ğ‹ = LowerTriangular(Matrix(undef, n, n))
   for i=1:n, j=i:n
      ğ‹[j, i] = Vector{T}(undef, k)
      for Îº=1:k ğ‹[j, i][Îº] = ğ†[Îº][j, i] end
   end

   B, iter, conv = gajd(ğ‹; tol=tol, maxiter=maxiter, verbose=verbose)

   # scale and permute the vectors of B
   D=Diagonal([mean(ğ‹[i, i]) for i=1:n])
   Î» = sort ? _permute!(_scale!(B, D, n)...) : diag(D)

   return preWhite ? (W.F*B, pinv(B)*W.iF, Î», iter, conv) :
                     (B, pinv(B), Î», iter, conv)
end

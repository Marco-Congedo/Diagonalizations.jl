<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Main Module · Diagonalizations</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="Diagonalizations logo"/></a><div class="docs-package-name"><span class="docs-autofit">Diagonalizations</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Documentation</a></li><li class="is-active"><a class="tocitem" href>Main Module</a><ul class="internal"><li><a class="tocitem" href="#dependencies-1"><span>dependencies</span></a></li><li><a class="tocitem" href="#types-1"><span>types</span></a></li><li><a class="tocitem" href="#LinearFilter-methods-1"><span>LinearFilter methods</span></a></li><li><a class="tocitem" href="#data-input-1"><span>data input</span></a></li><li><a class="tocitem" href="#covariance-matrix-estimations-1"><span>covariance matrix estimations</span></a></li><li><a class="tocitem" href="#mean-covariance-matrix-estimations-1"><span>mean covariance matrix estimations</span></a></li><li><a class="tocitem" href="#subspace-dimension-1"><span>subspace dimension</span></a></li><li><a class="tocitem" href="#scale-and-permutation-1"><span>scale and permutation</span></a></li><li><a class="tocitem" href="#notation-and-nomenclature-1"><span>notation &amp; nomenclature</span></a></li><li><a class="tocitem" href="#acronyms-1"><span>acronyms</span></a></li></ul></li><li><span class="tocitem">Filters</span><ul><li><input class="collapse-toggle" id="menuitem-3-1" type="checkbox"/><label class="tocitem" for="menuitem-3-1"><span class="docs-label">One dataset (m=1)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../pca/">PCA</a></li><li><a class="tocitem" href="../whitening/">Whitening</a></li><li><a class="tocitem" href="../csp/">CSP</a></li><li><a class="tocitem" href="../cstp/">CSTP</a></li><li><a class="tocitem" href="../ajd/">AJD</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-2" type="checkbox"/><label class="tocitem" for="menuitem-3-2"><span class="docs-label">Two datasets (m=2)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../mca/">MCA</a></li><li><a class="tocitem" href="../cca/">CCA</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-3-3" type="checkbox"/><label class="tocitem" for="menuitem-3-3"><span class="docs-label">Several datasets (m&gt;2)</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../gmca/">gMCA</a></li><li><a class="tocitem" href="../gcca/">gCCA</a></li><li><a class="tocitem" href="../majd/">mAJD</a></li></ul></li><li><a class="tocitem" href="../tools/">Tools</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Main Module</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Main Module</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/Marco-Congedo/Diagonalizations.jl/blob/master/docs/src/Diagonalizations.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Diagonalizations.jl-1"><a class="docs-heading-anchor" href="#Diagonalizations.jl-1">Diagonalizations.jl</a><a class="docs-heading-anchor-permalink" href="#Diagonalizations.jl-1" title="Permalink"></a></h1><p>This is the main unit containing the <strong>Diagonalizations</strong> <em>module</em>.</p><h2 id="dependencies-1"><a class="docs-heading-anchor" href="#dependencies-1">dependencies</a><a class="docs-heading-anchor-permalink" href="#dependencies-1" title="Permalink"></a></h2><table><tr><th style="text-align: center">standard Julia packages</th><th style="text-align: center">external packages</th></tr><tr><td style="text-align: center"><a href="https://bit.ly/2W5Wq8W">LinearAlgebra</a></td><td style="text-align: center"><a href="https://github.com/mateuszbaran/CovarianceEstimation.jl">CovarianceEstimation</a></td></tr><tr><td style="text-align: center"><a href="https://bit.ly/2Oem3li">Statistics</a></td><td style="text-align: center"><a href="https://github.com/Marco-Congedo/PosDefManifold.jl">PosDefManifold</a></td></tr><tr><td style="text-align: center"><a href="https://github.com/JuliaStats/StatsBase.jl">StatsBase</a></td><td style="text-align: center"></td></tr></table><p>The main module does not contain functions.</p><h2 id="types-1"><a class="docs-heading-anchor" href="#types-1">types</a><a class="docs-heading-anchor-permalink" href="#types-1" title="Permalink"></a></h2><pre><code class="language-none">abstract type LinearFilters end</code></pre><p>is the abstract type for all filters.</p><p>All filters are instances of the following immutable structure:</p><h3 id="LinearFilter-1"><a class="docs-heading-anchor" href="#LinearFilter-1">LinearFilter</a><a class="docs-heading-anchor-permalink" href="#LinearFilter-1" title="Permalink"></a></h3><pre><code class="language-none">struct LinearFilter &lt;: LinearFilters
   F     :: AbstractArray
   iF    :: AbstractArray
   D     :: Diagonal
   eVar  :: Float64
   ev    :: Vec
   arev  :: Vec
   name  :: String</code></pre><p><strong>Fields</strong>:</p><p><code>.F</code>: for simple filters (<a href="../pca/#PCA-1">PCA</a>, <a href="../whitening/#Whitening-1">Whitening</a>, <a href="../csp/#CSP-1">CSP</a>, <a href="../ajd/#AJD-1">AJD</a>), this is an <span>$n⋅p$</span> matrix, where <span>$n$</span> is the number of variables in the data the filter has been derived from and <span>$p$</span> is the <a href="#subspace-dimension-1">subspace dimension</a>. For composite filters this is a vector of <span>$m$</span> of such <span>$n⋅p$</span> matrices, where</p><ul><li><span>$m=2$</span> for <a href="../mca/#MCA-1">MCA</a>, <a href="../cca/#CCA-1">CCA</a> and <a href="../cstp/#CSTP-1">CSTP</a> filters</li><li><span>$m≥2$</span> for <a href="../gmca/#gMCA-1">gMCA</a>, <a href="../gcca/#gCCA-1">gCCA</a> and <a href="../majd/#mAJD-1">mAJD</a> filters.</li></ul><p><code>.iF</code>: the <span>$p⋅n$</span> left-inverse of the filter(s) in <code>.F</code>, that is, multiplying on the right all matrices in <code>.iF</code> by the corresponding matrices in <code>.F</code> yields the <span>$p⋅p$</span> identity matrix.   </p><p>The following fields are populated by default, but may be set altogether to <code>nothing</code> by all constructors using the <code>simple</code> optional keyword argument:</p><p><code>.D</code>: a <span>$p⋅p$</span> <a href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.Diagonal">Diagonal</a> matrix holding the (generalized) eigenvalues or singular values, depending on the filter, of the last diagonalization that has been used to derive the filter. Since <code>.D</code> is in diagonal form, this matrix can be used directly in algebraic computations. For example, if <code>a</code> is a PCA filter computed from covariance matric <span>$C$</span> and <span>$p=n$</span>, then <code>C≈a.F*a.D*a.iF</code> is true. In a similar way, if <code>b</code> is a MCA filter computed from cross-covariance matric <span>$C_{xy}$</span> and <span>$p=n$</span>, then <code>C_xy≈b.F[1]*b.D*b.F[2]&#39;</code> is true.</p><p><code>.eVar</code>: the actual explained variance or variance ratio of the filter(s) in <code>.F</code>. This depends on the filter, see the documentation of each filter for details.</p><p><code>.ev</code>: a vector holding all the <code>n</code> (generalized) eigenvalues or singular values, depending on the filter, of the last diagonalization that has been used to derive the filter. If <span>$p=n$</span>, this is a vector holding the diagonal of <code>.D</code>, otherwise <code>.D</code> holds in diagonal form only the first <code>p</code> elements of <code>.ev</code>.</p><p><code>arev.</code>: a vector holding an accumulated regularized function of <code>.ev</code> used to find the <a href="#subspace-dimension-1">subspace dimension</a>. This depends on the filter, see the documentation of each filter for details.</p><h2 id="LinearFilter-methods-1"><a class="docs-heading-anchor" href="#LinearFilter-methods-1">LinearFilter methods</a><a class="docs-heading-anchor-permalink" href="#LinearFilter-methods-1" title="Permalink"></a></h2><p>The <a href="#LinearFilter-1">LinearFilter</a> structure supports the following methods:</p><pre><code class="language-none">size(f::LF)</code></pre><p>Return the size of <code>f.F</code> if it is a matrix, an iterator over the sizes of all matrices in <code>f.F</code> if it is a vector of matrices.</p><pre><code class="language-none">length(f::LF)</code></pre><p>Return 1 if <code>f.F</code> is a matrix, the number of matrices in <code>f.F</code> if it is a vector of matrices. Referring to Table 1 and Fig. 1 (see <a href="../#Overview-1">Overview</a>), this is the number of datasets <span>$m$</span>.</p><pre><code class="language-none">eltype(f::LF)</code></pre><p>Return the element type of the matrix(ces) in <code>f.F</code>.</p><pre><code class="language-none">==(f::LF, g::LF), ≈(f::LF, g::LF)</code></pre><p>Return <code>true</code> if all fields of LinearFilter <code>f</code> and <code>g</code> are equivalent, <code>false</code> otherwise. All but the <code>.F</code> and <code>.iF</code> fields are requested to be equal, where for vector fields approximate equality is ascetrained using the <code>≈</code> function. For the equivalence of the matrices in fields <code>.F</code> and <code>.iF</code>, it is requested that the mean <a href="../tools/#Diagonalizations.spForm"><code>spForm</code></a> index of the matrices in <code>f.iF</code> times the corresponding matrices in <code>g.F</code> and of the matrices in <code>g.iF</code> times the corresponding matrices in <code>f.F</code> is smaller then 0.05.</p><pre><code class="language-none">≠(f::LF, g::LF), ≉ (f::LF, g::LF)</code></pre><p>The negation of <code>==</code>.</p><pre><code class="language-none">function cut(f::LinearFilter, p::Int64)</code></pre><p>Create another <a href="#LinearFilter-1">LinearFilter</a> object with a smaller <a href="#subspace-dimension-1">subspace dimension</a> given by argument <code>p</code>. This applies to the matrix(ces) in fields <code>.F</code>, <code>.iF</code> and <code>.D</code>. All other fields remain the same.</p><h2 id="data-input-1"><a class="docs-heading-anchor" href="#data-input-1">data input</a><a class="docs-heading-anchor-permalink" href="#data-input-1" title="Permalink"></a></h2><p>All filter constructors take as input either data matrices or covariance matrices. Covariance matrices must be flagged by Julia as either <a href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.Symmetric">Symmetric</a>, if they are real, or <a href="https://docs.julialang.org/en/v1/stdlib/LinearAlgebra/#LinearAlgebra.Hermitian">Hermitian</a>, if they are real or complex, e.g.,</p><pre><code class="language-none">X=randn(100, 30)
C=(X&#39;*X)/100
p=pca(Symmetric(C))
# p=pca(C) will throw an ArgumentError</code></pre><p>the above call to the pca constructor is equivalent to</p><pre><code class="language-none">X=randn(100, 30)
p=pca(X)</code></pre><p>Some methods take as input a vector of Hermitian matrices, of type <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#%E2%84%8DVector-type-1">ℍVector</a>, see <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/MainModule/#typecasting-matrices-1">typecasting matrices</a>.</p><p>Using data matrices as input, shrinked covariance matrix estimators can be used for several filters (e.g., <a href="../pca/#PCA-1">PCA</a>, <a href="../whitening/#Whitening-1">Whitening</a>, <a href="../csp/#CSP-1">CSP</a>, <a href="../cstp/#CSTP-1">CSTP</a>). See here below.</p><h2 id="covariance-matrix-estimations-1"><a class="docs-heading-anchor" href="#covariance-matrix-estimations-1">covariance matrix estimations</a><a class="docs-heading-anchor-permalink" href="#covariance-matrix-estimations-1" title="Permalink"></a></h2><p>By default, when data matrices are used as <a href="#data-input-1">data input</a>, <em>Diagonalizations.jl</em> computes covariance matrices along the larger dimension of the data matrices. That is, for <span>$r⋅c$</span> data matrix <span>$X$</span>, if <span>$r&gt;c$</span> <span>$\frac{1}{r}X^{T}X$</span> is computed, otherwise <span>$\frac{1}{c}XX^{T}$</span> is computed. Hence, the default behavior assumes that the number of observations is larger than the number of variables, as it is usually appropriate. Covariance matrices can be computed along a specific dimension using optional keyword argument <code>dims</code>, as in <a href="https://github.com/JuliaStats/StatsBase.jl">StatsBase</a>.</p><p>Many filter constructors allow to use <em>shrinked</em> covariance matrix estimations by means of the <a href="https://github.com/mateuszbaran/CovarianceEstimation.jl">CovarianceEstimation</a> package.    The following constants are provided to allow quick access to the most popular choices among the many estimators implemented therein:</p><pre><code class="language-none">SCM=SimpleCovariance()</code></pre><p>This is the <em>default</em> for all constructors and corresponds to the standard <em>sample covariance matrix (SCM)</em> estimation.</p><pre><code class="language-none">LShrLW=LShr(ConstantCorrelation())</code></pre><p>This corresponds to the SCM estimator shrinked by the linear method of Ledoit and Wolf (2004) <a href="../#-1">🎓</a>.</p><pre><code class="language-none">LShr=LinearShrinkage</code></pre><p>This is a shortcut for requesting other types of linear Shrinkage. See the <a href="https://github.com/mateuszbaran/CovarianceEstimation.jl">CovarianceEstimation</a> package for details.</p><pre><code class="language-none">NShrLW=AnalyticalNonlinearShrinkage()</code></pre><p>This corresponds to the SCM estimator shrinked by the analytical non-linear method of Ledoit and Wolf (2018) <a href="../#-1">🎓</a>.</p><p>Also, several filter constructors allow to use a <code>mean</code> and <code>w</code> keyword arguments:</p><p><code>mean</code> can be used to subtract the mean from the variables of data matrices (e.g., data matrix <code>X</code>).</p><ul><li>if <code>mean=0</code>, the mean will not be subtracted (default);</li><li>if <code>mean=nothing</code>, the mean will be computed and subtracted;</li><li><code>mean</code> can be a vector of means to be subtracted:<ul><li>it must have length=size(<code>X</code>, 2) if <code>dims=1</code>, length=size(<code>X</code>, 2) if <code>dims=2</code>;</li></ul></li><li><code>mean</code> can also be a matrix of means to be subtracted:<ul><li>it must have size=(1, size(<code>X</code>, 2)) if <code>dims=1</code>, size=(size(<code>X</code>, 1), 1) if <code>dims=2</code>.</li></ul></li></ul><div class="admonition is-info"><header class="admonition-header">Nota Bene</header><div class="admonition-body"><p>For filter constructors taking as input sets of data matrices, the <code>mean</code> argument can be set only to <code>0</code> (default) or <code>nothing</code>.</p></div></div><p><code>w</code> can be <code>nothing</code> (default) or a <a href="https://github.com/JuliaStats/StatsBase.jl/blob/master/docs/src/weights.md">StatsBase.AbstractWeights</a> object to weights the samples of data matrices (e.g., data matrix <code>X</code>). It must have length=size(<code>X</code>, <code>dims</code>), where <code>dims</code> by default is set to the larger dimesnion of <code>X</code>. For some constructors <code>w</code> can also be a function. This is documented in the concerned constructors.</p><p>Note that if several data matrices can be given as input to filter constructors, for example <code>X</code>, <code>Y</code>,..., then you will find arguments named such as <code>meanX</code>, <code>meanY</code>,... and <code>wX</code>, <code>wY</code>,... to differentiate the mean ad weights of the several input data matrices.</p><p><strong>Examples</strong>:</p><pre><code class="language-none">using Diagonalizations

X=randn(100, 30) # X is &#39;tall&#39;, the default format
p=pca(X)</code></pre><p>The call here above uses the default SCM estimator and computes the PCA from the <span>$30⋅30$</span> covariance matrix <span>$\frac{1}{100}X^{T}X$</span>. The &#39;filter&#39; <code>p.F</code> is <span>$30⋅p$</span>, where <span>$p$</span> is the <a href="#subspace-dimension-1">subspace dimension</a>.</p><p>This call</p><pre><code class="language-none">p=pca(X; covEst=LShrLW)</code></pre><p>uses the linear shrinked estimator of Ledoit and Wolf (2004).</p><p>The call</p><pre><code class="language-none">p=pca(X; dims=2)</code></pre><p>uses the default SCM estimator and compute the PCA from the <span>$100⋅100$</span> (rank-deficient) covariance matrix <span>$\frac{1}{30}XX^{T}$</span>. The &#39;filter&#39; <code>p.F</code> is in this case <span>$100⋅p$</span>.</p><h2 id="mean-covariance-matrix-estimations-1"><a class="docs-heading-anchor" href="#mean-covariance-matrix-estimations-1">mean covariance matrix estimations</a><a class="docs-heading-anchor-permalink" href="#mean-covariance-matrix-estimations-1" title="Permalink"></a></h2><p>Some filters can take as input data a set of data matrices (a vector of matrices). In this case a covariance matrix is estimated for each data matrix in the set and then a mean of these covariance matrices is estimated.</p><p>If the covariance matrices are actually <em>cross-covariance</em> matrices, no option is provided and the usual arithmetic mean is computed. If they are <code>Symmetric</code> or <code>Hermitian</code> covariance matrices, the <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#Statistics.mean">mean</a> function of package <a href="https://github.com/Marco-Congedo/PosDefManifold.jl">PosDefManifold.jl</a> is used, since those matrices may be positive definite by construction, hence a mean using a metric acting on the Riemannian manifold of positive definite matrices may be used.</p><p>The constructors using this feature employ the following optional keyword arguments for regulating the computation of the mean:</p><p><code>metric</code>: the metric used to compute the mean. <em>PosDefManifold.jl</em> supports 10 <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/introToRiemannianGeometry/#metrics-1">metrics</a>, nine of which can be used here (all but the <code>VonNeumann</code> metric). Of particular interest are the following</p><ol><li><code>Fisher</code>: the natural affine invariant metric, possessing all good properties of a mean.</li><li><code>logEuclidean</code>, <code>Jeffrey</code>: computationally cheaper alternatives to 1), but not possessing all good properties of a mean.</li><li><code>invEuclidean</code>: leading to the matrix harmonic mean.</li><li><code>Euclidean</code>: (default) leading to the usual matrix arithmetic mean, thus applying also if the input matrices are not positive-definite.</li><li><code>Wasserstein</code>: a metric widely adopted in statistics, optimal transport and quantum physics (also known as Bures-Hellinger), also applying if the input matrices are not positive-definite.</li></ol><p>Note that, since by default covariance matrices are computed along the larger dimension of data matrices, the covariance matrices will be positive definite as long as the number of observations is sufficiently larger then the number of variables.</p><p>See the documentation of the <a href="https://marco-congedo.github.io/PosDefManifold.jl/dev/riemannianGeometry/#Statistics.mean">mean</a> function for arguments <code>w</code>, <code>✓w</code>, <code>init</code>, <code>tol</code> and <code>verbose</code>. Note that the name of the <code>w</code> (and <code>init</code>) arguments may actually be <code>wCx</code>, <code>w₁</code> (<code>initCx</code>, <code>init₁</code>) and similar. This is to allow using them for different data matrices used to construct the filter.   </p><h2 id="subspace-dimension-1"><a class="docs-heading-anchor" href="#subspace-dimension-1">subspace dimension</a><a class="docs-heading-anchor-permalink" href="#subspace-dimension-1" title="Permalink"></a></h2><p>For a &#39;wide&#39; <span>$n⋅t$</span> data matrix <span>$X$</span>, where <span>$n$</span> is the number of variables and <span>$t&gt;n$</span> the number of samples, with <span>$n⋅n$</span> covariance matrix <span>$C$</span>, the transformed data is given by <span>$F^{H}X$</span> and the transformed covariance matrix by <span>$F^{H}CF$</span>.</p><p>In the above, <span>$F$</span> is the <span>$n⋅p$</span> filter matrix and <span>$p$</span> is named the <em>subspace dimension</em>. The data filtered in this subspace is given by</p><p><span>$\widetilde{X}=F^{-H}F^{H}X$</span></p><p>and the filtered covariance by</p><p><span>$\widetilde{C}=F^{-H}F^{H}CFF^{-1}$</span>.</p><p>If the matrix <span>$X$</span> is available in the &#39;tall&#39; form <span>$t⋅n$</span> (default), the tranformed data is given by <span>$XF$</span> and the data filtered in the subspace is given by</p><p><span>$\widetilde{X}=XFF^{-1}$</span>.</p><p>The expressions for the transformed and filtered covariance matrix are the same as before.</p><p>For all filters <em>Diagonalizations.jl</em> allows to set the subspace dimension <span>$p$</span> using the <code>eVar</code> and <code>eVarMeth</code> optional keyword arguments. Ultimately, <span>$p$</span> will be en integer <span>$∈[1, n]$</span> representing the subspace dimension. The user may set <span>$p$</span>:</p><ul><li><strong>manually</strong>,</li></ul><p>either setting <code>eVar</code> explicitly to an integer <span>$∈[1, n]$</span>, or  setting <code>eVar</code> to the desired explained variance in the subspace filtered  data, as a float <span>$∈(0, 1]$</span>, where <span>$1$</span> corresponds to the total variance.</p><ul><li><strong>automaticlly</strong> (default),</li></ul><p>according to the <code>.arev</code> (accumulated regularized eigenvalues) vector  that is computed by the filter constructors. This vector is  non-decreasing and the last element is always <span>$1.0$</span>. The way it is  computed depends on the filter. Please refer to the documentation of  each filter for details on how the <code>.arev</code> vector is defined.</p><p>When <code>eVar</code> is given as a float or when such float ia allowed to be chosen automatically (default), the function passed as the <code>eVarMeth</code> argument determines the subspace dimension <span>$p$</span> so as to explain an amount of variance as close as possible to the desierd <code>eVar</code>. In fact <code>.arev</code> holds only <span>$n$</span> discrete possible values of explained variance. Therefore, the <code>.arev</code> vector is passed to the <code>eVarMeth</code> function. By default, <code>eVarMeth</code> is set to the Julia standard <a href="https://docs.julialang.org/en/v1/base/sort/#Base.Sort.searchsortedfirst">searchsortedfirst</a> function, which will select the smallest <span>$p$</span> allowing at least <code>eVar</code> explained variance. This amounts to rounding up the desired <code>eVar</code> variance. Another useful choice is the Julia standard <a href="https://docs.julialang.org/en/v1/base/sort/#Base.Sort.searchsortedlast">searchsortedlast</a> function, which will select the largest <span>$p$</span> allowing at most <code>eVar</code> explained variance. This amounts to rounding down the desired <code>eVar</code> variance.  </p><p>You can pass a user-defined function as <code>eVarMeth</code>. The function you define will take the <code>.arev</code> vector computed by the filter constructor as input and will return an integer, which will be automatically clamped to be <span>$∈[1, n]$</span>.</p><p>Note that once the filter has been constructed, its <code>.eVar</code> field will hold the actual explained variance, not the desired one that has been passed to the constructor using the <code>eVar</code> argument.</p><p>Note also that for some filter constructors you will find the <code>eVar</code> optional keyword argument and also other arguments with simialr name, such as <code>eVarCx</code> and <code>eVarCy</code>. These arguments act in a similar way as the main <code>eVar</code> argument, but apply to determine the subspace dimension of intermediate diagonalization procedures, typically, pre-whitening procedures. See also <a href="#notation-and-nomenclature-1">notation &amp; nomenclature</a> and <a href="#covariance-matrix-estimations-1">covariance matrix estimations</a>.</p><h2 id="scale-and-permutation-1"><a class="docs-heading-anchor" href="#scale-and-permutation-1">scale and permutation</a><a class="docs-heading-anchor-permalink" href="#scale-and-permutation-1" title="Permalink"></a></h2><p>Let <span>$F$</span> be a diagonalizer of matrix <span>$C$</span>, i.e.,</p><p><span>$F^{H}CF=Λ$</span></p><p>with <span>$Λ$</span> a diagonal matrix. Let <span>$P$</span> a <a href="https://en.wikipedia.org/wiki/Permutation_matrix">permutation matrix</a> and <span>$O$</span> a diagonal matrix whose entries are either <span>$1$</span> or <span>$-1$</span>. It is easy to verify then that any matrix <span>$FPO$</span> is an <em>identical</em> diagonalizer of <span>$C$</span>, since <span>$OP^{H}F^{H}CFPO=Λ$</span>. This implies that the filter matrices found by an exact diagonalization procedures are arbitrary up to <strong>sign and permutation</strong> of their columns.</p><p>If <span>$O$</span> is a generic diagonal matrix, it is easy to verify then that any matrix <span>$FPO$</span> is an <strong>equivalent</strong> diagonalizer of <span>$C$</span> (Belouchrani et al., 1997 <a href="../#-1">🎓</a>), since <span>$OP^{T}F^{T}CFPO$</span> is also diagonal, albeit different from <span>$Λ$</span>. This implies that there exist infinite equivalent exact diagonalizers and that the solution is arbitrary up to <strong>scale and permutation</strong> of the columns. Of course, the scale ambiguity implies the sign ambiguity, but not vice versa.   All exact diagonalization procedures implicitly constraint the solution to find <span>$P$</span> and <span>$O$</span> such that <span>$Λ$</span> possesses a desired property. For example, in principal component analysis the elements of <span>$Λ$</span> are the maximum values that can be attained constraining  <span>$F$</span> to be orthogonal.</p><p>The same ambiguity applies to <strong>approximate joint diagonalization</strong>. Let <span>$F$</span> be an approximate joint diagonalizer of matrix set <span>${C_1,...,C_K}$</span>, i.e.,</p><p><span>$F^{H}C_lF≈Λ_k$</span> for <span>$l∈[1, k]$</span></p><p>and let <span>$D$</span> be a diagonal matrix, then it is easy to verify that any matrix <span>$FPD$</span> is an <em>equivalent</em> approximate joint diagonalizer of the set <span>$C$</span>. To check if two diagonalizers are equivaent, you can use the <a href="../tools/#Diagonalizations.spForm"><code>spForm</code></a> function.</p><h2 id="notation-and-nomenclature-1"><a class="docs-heading-anchor" href="#notation-and-nomenclature-1">notation &amp; nomenclature</a><a class="docs-heading-anchor-permalink" href="#notation-and-nomenclature-1" title="Permalink"></a></h2><p>Throughout the code and documentation of this package the following notation is followed:</p><ul><li><strong>scalars</strong> and <strong>vectors</strong> are denoted using lower-case letters, e.g., <code>y</code>,</li><li><strong>matrices</strong> using upper case letters, e.g., <code>X</code></li><li><strong>sets (vectors) of matrices</strong> using bold upper-case letters, e.g., <code>𝐗</code>.</li><li>superscripts <em>H</em> and <em>T</em> denote matrix complex conjugate-transpose and transpose.</li></ul><p>The following nomenclature is used consistently:</p><ul><li><span>$X$</span>, <span>$Y$</span>: <strong>data matrices</strong></li><li><span>$𝐗$</span>, <span>$𝐘$</span>: <strong>vectors of data matrices</strong></li><li><span>$C$</span>: a <strong>covariance matrix</strong></li><li><span>$𝐂$</span>: a <strong>vector of covariance matrices</strong></li><li><span>$Cx$</span>: the <strong>covariance matrix</strong> of data matrix <span>$X$</span></li><li><span>$Cxy$</span>: the <strong>cross-covariance matrix</strong> of <span>$X$</span> and <span>$Y$</span></li><li><span>$U$</span>, <span>$V$</span>: <strong>orthogonal matrices</strong> of eigenvectors or the left and right singular vectors</li><li><span>$λ$</span>: <strong>vector</strong> of eigenvalues, singular values or a function thereof</li><li><span>$Λ$</span>: <strong>diagonal matrix</strong> of eigenvalues, singular values or a function thereof</li><li><span>$B$</span>, <span>$F$</span>: <strong>non-singular matrices</strong></li></ul><p>In the examples, bold upper-case letters are replaced by upper case letters in order to allow reading in the REPL.</p><h2 id="acronyms-1"><a class="docs-heading-anchor" href="#acronyms-1">acronyms</a><a class="docs-heading-anchor-permalink" href="#acronyms-1" title="Permalink"></a></h2><ul><li>AJD: Approximate Joint Diagonalization (Cardoso &amp; Souloumiac, 1996; Flury &amp; Gautschi, 1986)</li><li>AJEVD: Approximate Joint Eigenvalue-Eigenvector Decomposition</li><li>AJSVD: Approximate Joint Singular Value Decomposition (Congedo et al., 2011)</li><li>AMUSE: Algorithm for Multiple Source Extraction (Molgedey &amp; Schuster, 1994; Tong et al., 1991)</li><li>BSS: Blind Source Separation</li><li>CCA: Canonical Correlation Analysis (Hotelling, 1936)</li><li>CSP: Common Spatial Pattern (Fukunaga, 1990)</li><li>CSTP: Common Spatio-Temporal Pattern (Congedo et al., 2016)</li><li>EEG: Electroencephalography</li><li>ERP: Event-Related Potentials</li><li>FOBI: Fourth-Order Blind Identification (Cardoso, 1989)</li><li>gCCA: generalized CCA</li><li>gMCA: generalized MCA</li><li>JADE: Joint Diagonalization of Eigenmatrices (Cardoso &amp; Souloumiac, 1993)</li><li>LShrLW: Linear Shrinkage of Ledoit and Wolf (2004)</li><li>NShrLW: Non-linear Shrinkage of Ledoit and Wolf (2018)</li><li>MCA: Maximum Covariance Analysis</li><li>NoJoB: Non-orthogonal Joint BSS (Congedo et al., 2012)</li><li>OJoB: Orthogonal Joint BSS (Congedo et al., 2012)</li><li>PCA: Principal Component Analysis (Pearson, 1901)</li><li>SCM: Sample Covariance Matrix</li><li>SOBI: Second-Order Blind Identification (Belouchrani et al., 1997)</li></ul><p>For the references see <a href="../#-1">🎓</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Documentation</a><a class="docs-footer-nextpage" href="../pca/">PCA »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 1 January 2020 22:05">Wednesday 1 January 2020</span>. Using Julia version 1.3.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>

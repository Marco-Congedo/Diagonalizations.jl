var documenterSearchIndex = {"docs":
[{"location":"majd/#mAJD","page":"mAJD","title":"mAJD","text":"","category":"section"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"Multiple Approximate Joint Diagonalization (MAJD) is the utmost general diagonalization prodedure implemented in Diagonalizations.jl. It generalizes the AJD to the case of multiple datasets (m1) and the gMCA/gCCA to the case of multiple observations (k1). Therefore, it suits the situation m2 (multiple datasets) and k2 (multiple observations) at once.","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"Let X_l1X_lm be k sets of m data matrices of dimension nt each, indexed by l1k.","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"From these data matrices let us estimate","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"C_lij=frac1tX_liX_lj^H, for all l1k and ij1m, hspace1cm [majd.1]","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"i.e., all covariance (i=j) and cross-covariance (ij) matrices for all l1k.","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"The MAJD seeks m matrices F_1F_m diagonalizing as much as possible all products","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"F_i^H C_lij F_j, for all l1k and ij1m hspace1cm [majd.2]","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"or all products","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"F_i^H C_lij F_j, for all l1k and ij1m, hspace1cm [majd.3]","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"depending on the chosen model (see argument fullModel below).","category":"page"},{"location":"majd/#pre-whitening-for-MAJD","page":"mAJD","title":"pre-whitening for MAJD","text":"","category":"section"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"Pre-whitening can be applied. In this case, first m whitening matrices W_1W_m are found such that","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"W_i^HBig(frac1ksum_l=1^kC_kiiBig)W_i=I, for all i1m hspace1cm","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"then the following transformed AJD problem if solved for U_1U_m:","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"U_i^H(W_i^HC_lijW_j)U_jÎ›_lij, for all l1k and ij1m.","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"Finally, F_1F_m are obtained as","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"F_i=W_iU_i, for i1m. hspace1cm","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"Notice that:","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"matrix W may be taken rectangular so as to engender a dimensionality reduction at this stage. This may improve the convergence behavior of AJD algorithms if the matrices C_lii are not well-conditioned.  \nif this two-step procedure is employed, the final matrices F_1F_m are never orthogonal, even if the solving AJD algorithm constrains the solutions within the orthogonal group.","category":"page"},{"location":"majd/#permutation-for-MAJD","page":"mAJD","title":"permutation for MAJD","text":"","category":"section"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"As usual, the approximate diagonalizers F_1F_m are arbitrary up to a scale and permutation. in MAJD scaling is fixed by appropriate constraints. For the remaining sign and permutation ambiguities, Diagonalizations.jl attempts to solve them by finding signed permutation matrices for F_1F_m so as to make all diagonal elements of [gmca.2] or [gmca.3] positive and sorted in descending order.","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"Let","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"Î»=Î»_1Î»_n  hspace1cm [majd.4]","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"be the diagonal elements of","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"frac1k(m^2-m)sum_j=1^ksum_ij=1^m(F_i^H C_lij F_j) hspace1cm [majd.5]","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"and Ïƒ_TOT=sum_i=1^nÎ»_i be the total variance.","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"We denote widetildeF_i=f_i1 ldots f_ip the matrix holding the first pn column vectors of F_i, where p is the subspace dimension. The explained variance is given by","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"Ïƒ_p=fracsum_i=1^pÎ»_iÏƒ_TOT hspace1cm [majd.6]","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"and the accumulated regularized eigenvalues (arev) by","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"Ïƒ_j=sum_i=1^jÏƒ_i, for j=1 ldots n, hspace1cm [majd.7]","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"where Ïƒ_i is given by Eq. [majd.6].","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"Solution","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"There is no closed-form solution to the AJD problem in general. See Algorithms.","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"Constructors","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":"One constructor is available (see here below). The constructed LinearFilter object holding the MAJD will have fields:","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":".F: vector of matrices widetildeF_1widetildeF_m with columns holding the first p eigenvectors in F_1F_m, or just F_1F_m if p=n","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":".iF: the vector of the left-inverses of the matrices in .F","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":".D: the leading pp block of Î›, i.e., the elements [majd.4] associated to the matrices in .F in diagonal form.","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":".eVar: the explained variance [majd.6] for the chosen value of p.","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":".ev: the vector Î» [majd.4].","category":"page"},{"location":"majd/","page":"mAJD","title":"mAJD","text":".arev: the accumulated regularized eigenvalues  in [majd.7].","category":"page"},{"location":"majd/#Diagonalizations.majd","page":"mAJD","title":"Diagonalizations.majd","text":"function majd(ğ‘¿::VecVecMat;\n              covEst     :: StatsBase.CovarianceEstimator = SCM,\n              dims       :: Into    = â—‹,\n              meanX      :: Into    = 0,\n          algorithm :: Symbol    = :NoJoB,\n          fullModel :: Bool      = false,\n          preWhite  :: Bool      = false,\n          sort      :: Bool      = true,\n          init      :: VecMato   = â—‹,\n          tol       :: Real      = 0.,\n          maxiter   :: Int       = _maxiter(algorithm, eltype(ğ‘¿[1][1])),\n          verbose   :: Bool      = false,\n          threaded  :: Bool      = true,\n        eVar     :: TeVaro   = _minDim(ğ‘¿),\n        eVarC    :: TeVaro   = â—‹,\n        eVarMeth :: Function = searchsortedfirst,\n        simple   :: Bool     = false)\n\n\nReturn a LinearFilter object.\n\nMultiple Approximate Joint Diagonalization of the k sets of m data matrices ğ— using the given solving algorithm (NoJoB by default).\n\nIf fullModel is true, the [gmca.3] problem here above is solved, otherwise (default), the [gmca.2] problem here above is solved.\n\nIf preWhite the two-step procedure explained here above in the section pre-whitening for MAJD is used. Dimensionality reduction can be obtained at this stage using arguments eVarC and eVarMeth.\n\nThe default values are:\n\neVarC is set to 0.999\neVarMeth=searchsortedfirst.\n\nIf sort is true (default), the column vectors of the matrices F_1F_m are signed and permuted as explained here above in permutation for MAJD, otherwise they will have arbitrary sign and will be in arbitrary order.\n\nIf verbose is true (false by default), the convergence attained at each iteration will be printed in the REPL.\n\neVar and eVarMeth are used to define a subspace dimension p using the accumulated regularized eigenvalues in Eq. [gmca.7]\n\nThe default values are:\n\neVar is set to the minimum dimension of the matrices in ğ—\neVarMeth=searchsortedfirst.\n\nIf simple is set to true, p is set equal to the dimension of the covariance matrices that are computed on the matrices in ğ—, which depends on the choice of dims, and only the fields .F and .iF are written in the constructed object. This corresponds to the typical output of approximate diagonalization algorithms.\n\nif threaded=true (default) and the number of threads Julia is instructed to use (the output of Threads.nthreads()), is higher than 1, solving algorithms supporting multi-threading run in multi-threaded mode. See Algorithms and these notes on multi-threading.\n\nSee also: gMCA, gCCA, AJD.\n\nExamples:\n\nusing Diagonalizations, LinearAlgebra, PosDefManifold, Test\n\n##  Create data for testing the case k>1, m>1 ##\n# `t` is the number of samples,\n# `m` is the number of datasets,\n# `k` is the number of observations,\n# `n` is the number of variables,\n# `noise` must be smaller than 1.0. The smaller the noise, the more data are correlated\n# Output k vectors of m data data matrices\nfunction getData(t, m, k, n, noise)\n    # create m identical data matrices and rotate them by different\n    # random orthogonal matrices V_1,...,V_m\n    ğ•=[randU(n) for i=1:m] # random orthogonal matrices\n    # variables common to all subjects with unique variance profile across k\n    X=[(abs2.(randn(n))).*randn(n, t) for s=1:k]\n    # each subject has this common part plus a random part\n    ğ—=[[ğ•[i]*((1-noise)*X[s] + noise*randn(n, t)) for i=1:m] for s=1:k]\n    return ğ—, ğ•\nend\n\nfunction getData(::Type{Complex{T}}, t, m, k, n, noise) where {T<:AbstractFloat}\n    # create m identical data matrices and rotate them by different\n    # random orthogonal matrices V_1,...,V_m\n    ğ•=[randU(ComplexF64, n) for i=1:m] # random orthogonal matrices\n    # variables common to all subjects with unique variance profile across k\n    X=[(abs2.(randn(n))).*randn(ComplexF64, n, t) for s=1:k]\n    # each subject has this common part plus a random part\n    ğ—=[[ğ•[i]*((1-noise)*X[s] + noise*randn(ComplexF64, n, t)) for i=1:m] for s=1:k]\n    return ğ—, ğ•\nend\n\n\n# REAL data\n# do joint blind source separation of non-stationary data\nt, m, n, k, noise = 200, 5, 4, 6, 0.1\nXset, Vset=getData(t, m, k, n, noise)\nğ’=Array{Matrix}(undef, k, m, m)\nfor s=1:k, i=1:m, j=1:m ğ’[s, i, j]=(Xset[s][i]*Xset[s][j]')/t end\n\naX=majd(Xset; fullModel=true, algorithm=:OJoB)\n# the spForm index of the estimated demixing matrices times the true\n# mixing matrix must be low\n@test mean(spForm(aX.F[i]'*Vset[i]) for i=1:m)<0.1\n\n# test the same using NoJoB algorithm\naX=majd(Xset; fullModel=true, algorithm=:NoJoB)\n@test mean(spForm(aX.F[i]'*Vset[i]) for i=1:m)<0.1\n\n# plot the original cross-covariance matrices and the rotated\n# cross-covariance matrices\n\n# Get all products ğ”[i]' * ğ’[l, i, j] * ğ”[j]\nfunction _rotate_crossCov(ğ”, ğ’, m, k)\n    ğ’®=Array{Matrix}(undef, k, m, m)\n    @inbounds for l=1:k, i=1:m, j=1:m ğ’®[l, i, j]=ğ”[i]'*ğ’[l, i, j]*ğ”[j] end\n    return ğ’®\nend\n\n# Put all `k` cross-covariances in a single matrix\n# of dimension m*n x m*n for visualization\nfunction ğ’2Mat(ğ’::AbstractArray, m, k)\n    n=size(ğ’[1, 1, 1], 1)\n    C=Matrix{Float64}(undef, m*n, m*n)\n    for i=1:m, j=1:m, x=1:n, y=1:n C[i*n-n+x, j*n-n+y]=ğ’[k, i, j][x, y] end\n    return C\nend\n\nusing Plots\n\nCset=[ğ’2Mat(ğ’, m, s) for s=1:k]\n Cmax=maximum(maximum(abs.(C)) for C âˆˆ Cset)\n h1 = heatmap(Cset[1], clim=(-Cmax, Cmax), yflip=true, c=:bluesreds, title=\"all cross-cov, k=1\")\n h2 = heatmap(Cset[2], clim=(-Cmax, Cmax), yflip=true, c=:bluesreds, title=\"all cross-cov, k=2\")\n h3 = heatmap(Cset[3], clim=(-Cmax, Cmax), yflip=true, c=:bluesreds, title=\"all cross-cov, k=3\")\n h4 = heatmap(Cset[4], clim=(-Cmax, Cmax), yflip=true, c=:bluesreds, title=\"all cross-cov, k=4\")\n h5 = heatmap(Cset[5], clim=(-Cmax, Cmax), yflip=true, c=:bluesreds, title=\"all cross-cov, k=5\")\n h6 = heatmap(Cset[6], clim=(-Cmax, Cmax), yflip=true, c=:bluesreds, title=\"all cross-cov, k=6\")\n ğŸ“ˆ=plot(h1, h2, h3, h4, h5, h6, size=(1200,550))\n# savefig(ğŸ“ˆ, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FigmAJD1.png\")\n\nğ’®=_rotate_crossCov(aX.F, ğ’, m, k)\n Sset=[ğ’2Mat(ğ’®, m, s) for s=1:k]\n Smax=maximum(maximum(abs.(S)) for S âˆˆ Sset)\n h11 = heatmap(Sset[1], clim=(-Smax, Smax), yflip=true, c=:bluesreds, title=\"rotated cross-cov, k=1\")\n h12 = heatmap(Sset[2], clim=(-Smax, Smax), yflip=true, c=:bluesreds, title=\"rotated cross-cov, k=2\")\n h13 = heatmap(Sset[3], clim=(-Smax, Smax), yflip=true, c=:bluesreds, title=\"rotated cross-cov, k=3\")\n h14 = heatmap(Sset[4], clim=(-Smax, Smax), yflip=true, c=:bluesreds, title=\"rotated cross-cov, k=4\")\n h15 = heatmap(Sset[5], clim=(-Smax, Smax), yflip=true, c=:bluesreds, title=\"rotated cross-cov, k=5\")\n h16 = heatmap(Sset[6], clim=(-Smax, Smax), yflip=true, c=:bluesreds, title=\"rotated cross-cov, k=6\")\n ğŸ“‰=plot(h11, h12, h13, h14, h15, h16, size=(1200,550))\n# savefig(ğŸ“‰, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FigmAJD2.png\")\n\n\n(Image: Figure mAJD1)\n\n(Image: Figure mAJD2)\n\nIn the bottom figures here above, the rotated cross-covariance matrices have the expected strip-diagonal form, that is, each block F_i^Tfrac1t(X_liX_lj^T)F_j, for l1k, ij1m, is approximately diagonal.\n\n# COMPLEX data\n# do joint blind source separation of non-stationary data\nt, m, n, k, noise = 200, 5, 4, 6, 0.1\nXcset, Vcset=getData(ComplexF64, t, m, k, n, noise)\nğ’=Array{Matrix}(undef, k, m, m)\nfor s=1:k, i=1:m, j=1:m ğ’[s, i, j]=(Xcset[s][i]*Xcset[s][j]')/t end\n\naXc=majd(Xcset; fullModel=true, algorithm=:OJoB)\n# the spForm index of the estimated demixing matrices times the true\n# mixing matrix must be low\n@test mean(spForm(aXc.F[i]'*Vcset[i]) for i=1:m)<0.1\n\n# test the same using NoJoB algorithm\naXc=majd(Xcset; fullModel=true, algorithm=:NoJoB)\n@test mean(spForm(aXc.F[i]'*Vcset[i]) for i=1:m)<0.1\n\n\n\n\n\n","category":"function"},{"location":"Diagonalizations/#Diagonalizations.jl","page":"Diagonalizations","title":"Diagonalizations.jl","text":"","category":"section"},{"location":"Diagonalizations/#dependencies","page":"Diagonalizations","title":"dependencies","text":"","category":"section"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"standard Julia packages external packages\nLinearAlgebra CovarianceEstimation\nStatistics PosDefManifold\nStatsBase ","category":"page"},{"location":"Diagonalizations/#types","page":"Diagonalizations","title":"types","text":"","category":"section"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"abstract type LinearFilters end","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"is the abstract type for all filters.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"All filters are instances of the following immutable structure:","category":"page"},{"location":"Diagonalizations/#LinearFilter","page":"Diagonalizations","title":"LinearFilter","text":"","category":"section"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"struct LinearFilter <: LinearFilters\n   F     :: AbstractArray\n   iF    :: AbstractArray\n   D     :: Diagonal\n   eVar  :: Float64\n   ev    :: Vec\n   arev  :: Vec\n   name  :: String","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Fields:","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":".F: for simple filters (PCA, Whitening, CSP, AJD), this is an np matrix, where n is the number of variables in the data the filter has been derived from and p is the subspace dimension. For composite filters this is a vector of m of such np matrices, where","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"m=2 for MCA, CCA and CSTP filters\nm2 for gMCA, gCCA and mAJD filters.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":".iF: the pn left-inverse of the filter(s) in .F, that is, multiplying on the right all matrices in .iF by the corresponding matrices in .F yields the pp identity matrix.   ","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"The following fields are populated by default, but may be set altogether to nothing by all constructors using the simple optional keyword argument:","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":".D: a pp Diagonal matrix holding the (generalized) eigenvalues or singular values, depending on the filter, of the last diagonalization that has been used to derive the filter. Since .D is in diagonal form, this matrix can be used directly in algebraic computations. For example, if a is a PCA filter computed from covariance matric C and p=n, then Câ‰ˆa.F*a.D*a.iF is true. In a similar way, if b is a MCA filter computed from cross-covariance matric C_xy and p=n, then C_xyâ‰ˆb.F[1]*b.D*b.F[2]' is true.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":".eVar: the actual explained variance or variance ratio of the filter(s) in .F. This depends on the filter, see the documentation of each filter for details.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":".ev: a vector holding all the n (generalized) eigenvalues or singular values, depending on the filter, of the last diagonalization that has been used to derive the filter. If p=n, this is a vector holding the diagonal of .D, otherwise .D holds in diagonal form only the first p elements of .ev.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"arev.: a vector holding an accumulated regularized function of .ev used to find the subspace dimension. This depends on the filter, see the documentation of each filter for details.","category":"page"},{"location":"Diagonalizations/#LinearFilter-methods","page":"Diagonalizations","title":"LinearFilter methods","text":"","category":"section"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"The LinearFilter structure supports the following methods:","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"size(f::LF)","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Return the size of f.F if it is a matrix, an iterator over the sizes of all matrices in f.F if it is a vector of matrices.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"length(f::LF)","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Return 1 if f.F is a matrix, the number of matrices in f.F if it is a vector of matrices. Referring to Table 1 and Fig. 1 (see Overview), this is the number of datasets m.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"eltype(f::LF)","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Return the element type of the matrix(ces) in f.F.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"==(f::LF, g::LF), â‰ˆ(f::LF, g::LF)","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Return true if all fields of LinearFilter f and g are equivalent, false otherwise. All but the .F and .iF fields are requested to be equal, where for vector fields approximate equality is ascetrained using the â‰ˆ function. For the equivalence of the matrices in fields .F and .iF, it is requested that the mean spForm index of the matrices in f.iF times the corresponding matrices in g.F and of the matrices in g.iF times the corresponding matrices in f.F is smaller then 0.05.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"â‰ (f::LF, g::LF), â‰‰ (f::LF, g::LF)","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"The negation of ==.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"function cut(f::LinearFilter, p::Int64)","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Create another LinearFilter object with a smaller subspace dimension given by argument p. This applies to the matrix(ces) in fields .F, .iF and .D. All other fields remain the same.","category":"page"},{"location":"Diagonalizations/#data-input","page":"Diagonalizations","title":"data input","text":"","category":"section"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"All filter constructors take as input either data matrices or covariance matrices. Covariance matrices must be flagged by Julia as either Symmetric, if they are real, or Hermitian, if they are real or complex, e.g.,","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"X=randn(100, 30)\nC=(X'*X)/100\np=pca(Symmetric(C))\n# p=pca(C) will throw an ArgumentError","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"the above call to the pca constructor is equivalent to","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"X=randn(100, 30)\np=pca(X)","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Some methods take as input a vector of Hermitian matrices, of type â„Vector, see typecasting matrices.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Using real data matrices as input, shrinked covariance matrix estimators can be used for several filters (e.g., PCA, Whitening, CSP, CSTP). See here below.","category":"page"},{"location":"Diagonalizations/#covariance-matrix-estimations","page":"Diagonalizations","title":"covariance matrix estimations","text":"","category":"section"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"By default, when data matrices are used as data input, Diagonalizations.jl computes covariance matrices along the larger dimension of the data matrices. That is, for rc data matrix X, if rc frac1rX^TX is computed, otherwise frac1cXX^T is computed. Hence, the default behavior assumes that the number of observations is larger than the number of variables, as it is usually appropriate. Covariance matrices can be computed along a specific dimension using optional keyword argument dims, as in StatsBase.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Many filter constructors allow to use shrinked covariance matrix estimations (only for real data) by means of the CovarianceEstimation package.    The following constants are provided to allow quick access to the most popular choices among the many estimators implemented therein:","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"SCM=SimpleCovariance()","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"This is the default for all constructors and corresponds to the standard sample covariance matrix (SCM) estimation.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"LShrLW=LShr(ConstantCorrelation())","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"This corresponds to the SCM estimator shrinked by the linear method of Ledoit and Wolf (2004) ğŸ“.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"LShr=LinearShrinkage","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"This is a shortcut for requesting other types of linear Shrinkage. See the CovarianceEstimation package for details.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"NShrLW=AnalyticalNonlinearShrinkage()","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"This corresponds to the SCM estimator shrinked by the analytical non-linear method of Ledoit and Wolf (2018) ğŸ“.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Also, several filter constructors allow to use a mean and w keyword arguments:","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"mean can be used to subtract the mean from the variables of data matrices (e.g., data matrix X).","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"if mean=0, the mean will not be subtracted (default);\nif mean=nothing, the mean will be computed and subtracted;\nmean can be a vector of means to be subtracted:\nit must have length=size(X, 2) if dims=1, length=size(X, 2) if dims=2;\nmean can also be a matrix of means to be subtracted:\nit must have size=(1, size(X, 2)) if dims=1, size=(size(X, 1), 1) if dims=2.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"note: Nota Bene\nFor filter constructors taking as input sets of data matrices, the mean argument can be set only to 0 (default) or nothing.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"w can be nothing (default) or a StatsBase.AbstractWeights object to weights the samples of data matrices (e.g., data matrix X). It must have length=size(X, dims), where dims by default is set to the larger dimesnion of X. For some constructors w can also be a function. This is documented in the concerned constructors.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Note that if several data matrices can be given as input to filter constructors, for example X, Y,..., then you will find arguments named such as meanX, meanY,... and wX, wY,... to differentiate the mean ad weights of the several input data matrices.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Examples:","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"using Diagonalizations\n\nX=randn(100, 30) # X is 'tall'\np=pca(X)","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"The call here above uses the default SCM estimator and computes the PCA from the 3030 covariance matrix frac1100X^TX. The 'filter' p.F is 30p, where p is the subspace dimension. For complex data the call is the same:","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Xc=randn(ComplexF64, 100, 30)\npc=pca(Xc)","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"This call","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"p=pca(X; covEst=LShrLW)","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"uses the linear shrinked estimator of Ledoit and Wolf (2004).","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"The call","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"p=pca(X; dims=2)","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"uses the default SCM estimator and computes the PCA from the 100100 (rank-deficient) covariance matrix frac130XX^T. The 'filter' p.F is in this case 100p.","category":"page"},{"location":"Diagonalizations/#mean-covariance-matrix-estimations","page":"Diagonalizations","title":"mean covariance matrix estimations","text":"","category":"section"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Some filters can take as input data a set of data matrices (a vector of matrices). In this case a covariance matrix is estimated for each data matrix in the set and then a mean of these covariance matrices is estimated.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"If the covariance matrices are actually cross-covariance matrices, no option is provided and the usual arithmetic mean is computed. If they are Symmetric or Hermitian covariance matrices, the mean function of package PosDefManifold.jl is used, since those matrices may be positive definite by construction, hence a mean using a metric acting on the Riemannian manifold of positive definite matrices may be used.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"The constructors using this feature employ the following optional keyword arguments for regulating the computation of the mean:","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"metric: the metric used to compute the mean. PosDefManifold.jl supports 10 metrics, nine of which can be used here (all but the VonNeumann metric). Of particular interest are the following","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Fisher: the natural affine invariant metric, possessing all good properties of a mean.\nlogEuclidean, Jeffrey: computationally cheaper alternatives to 1), but not possessing all good properties of a mean.\ninvEuclidean: leading to the matrix harmonic mean.\nEuclidean: (default) leading to the usual matrix arithmetic mean, thus applying also if the input matrices are not positive-definite.\nWasserstein: a metric widely adopted in statistics, optimal transport and quantum physics (also known as Bures-Hellinger), also applying if the input matrices are not positive-definite.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Note that, since by default covariance matrices are computed along the larger dimension of data matrices, the covariance matrices will be positive definite as long as the number of observations is sufficiently larger then the number of variables.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"See the documentation of the mean function for arguments w, âœ“w, init, tol and verbose. Note that the name of the w (and init) arguments may actually be wCx, wâ‚ (initCx, initâ‚) and similar. This is to allow using them for different data matrices used to construct the filter.   ","category":"page"},{"location":"Diagonalizations/#subspace-dimension","page":"Diagonalizations","title":"subspace dimension","text":"","category":"section"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"For a 'wide' nt data matrix X, where n is the number of variables and tn the number of samples, with nn covariance matrix C, the transformed data is given by F^HX and the transformed covariance matrix by F^HCF.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"In the above, F is the np filter matrix and p is named the subspace dimension. The data filtered in this subspace is given by","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"widetildeX=F^-HF^HX","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"and the filtered covariance by","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"widetildeC=F^-HF^HCFF^-1.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"If the matrix X is available in the 'tall' form tn (default), the tranformed data is given by XF and the data filtered in the subspace is given by","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"widetildeX=XFF^-1.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"The expressions for the transformed and filtered covariance matrix are the same as before.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"For all filters Diagonalizations.jl allows to set the subspace dimension p using the eVar and eVarMeth optional keyword arguments. Ultimately, p will be en integer 1 n representing the subspace dimension. The user may set p:","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"manually,","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"either setting eVar explicitly to an integer 1 n, or  setting eVar to the desired explained variance in the subspace filtered  data, as a float (0 1, where 1 corresponds to the total variance.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"automatically (default),","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"according to the .arev (accumulated regularized eigenvalues) vector  that is computed by the filter constructors. This vector is  non-decreasing and the last element is always 10. The way it is  computed depends on the filter. Please refer to the documentation of  each filter for details on how the .arev vector is defined.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"When eVar is given as a float or when such float ia allowed to be chosen automatically (default), the function passed as the eVarMeth argument determines the subspace dimension p so as to explain an amount of variance as close as possible to the desierd eVar. In fact .arev holds only n discrete possible values of explained variance. Therefore, the .arev vector is passed to the eVarMeth function. By default, eVarMeth is set to the Julia standard searchsortedfirst function, which will select the smallest p allowing at least eVar explained variance. This amounts to rounding up the desired eVar variance. Another useful choice is the Julia standard searchsortedlast function, which will select the largest p allowing at most eVar explained variance. This amounts to rounding down the desired eVar variance.  ","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"You can pass a user-defined function as eVarMeth. The function you define will take the .arev vector computed by the filter constructor as input and will return an integer, which will be automatically clamped to be 1 n.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Note that once the filter has been constructed, its .eVar field will hold the actual explained variance, not the desired one that has been passed to the constructor using the eVar argument.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Note also that for some filter constructors you will find the eVar optional keyword argument and also other arguments with simialr name, such as eVarCx and eVarCy. These arguments act in a similar way as the main eVar argument, but apply to determine the subspace dimension of intermediate diagonalization procedures, typically, pre-whitening procedures. See also notation & nomenclature and covariance matrix estimations.","category":"page"},{"location":"Diagonalizations/#scale-and-permutation","page":"Diagonalizations","title":"scale and permutation","text":"","category":"section"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Let F be a diagonalizer of matrix C, i.e.,","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"F^HCF=Î›","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"with Î› a diagonal matrix. Let P a permutation matrix and O a diagonal matrix whose entries are either 1 or -1. It is easy to verify then that any matrix FPO is an identical diagonalizer of C, since OP^HF^HCFPO=Î›. This implies that the filter matrices found by an exact diagonalization procedures are arbitrary up to sign and permutation of their columns.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"If D is a generic diagonal matrix, it is easy to verify then that any matrix FPD is an equivalent diagonalizer of C (Belouchrani et al., 1997 ğŸ“), since DP^HF^HCFPD is also diagonal, albeit different from Î›. This implies that there exist infinite equivalent exact diagonalizers and that the solution is arbitrary up to scale and permutation of the columns. Of course, the scale ambiguity implies the sign ambiguity, but not vice versa.   All exact diagonalization procedures implicitly constraint the solution to find P and D such that Î› possesses a desired property. For example, in principal component analysis the elements of Î› are the maximum values that can be attained constraining  F to be orthogonal.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"The same ambiguity applies to approximate joint diagonalization. Let F be an approximate joint diagonalizer of matrix set C_1C_K, i.e.,","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"F^HC_lFÎ›_k for l1 k","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"and let D be a diagonal matrix, then it is easy to verify that any matrix FPD is an equivalent approximate joint diagonalizer of the set C. To check if two diagonalizers are equivalent, you can use the spForm function.","category":"page"},{"location":"Diagonalizations/#notation-and-nomenclature","page":"Diagonalizations","title":"notation & nomenclature","text":"","category":"section"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"Throughout the code and documentation of this package the following notation is followed:","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"scalars and vectors are denoted using lower-case letters, e.g., x, y,\nmatrices using upper case letters, e.g., X, Y,\nsets (vectors) of matrices using bold upper-case letters, e.g., ğ—, ğ˜.\nsuperscripts H and T denote matrix complex conjugate-transpose and transpose.","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"The following nomenclature is used consistently:","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"X, Y: data matrices\nğ—, ğ˜: vectors of data matrices\nC: a covariance matrix\nğ‚: a vector of covariance matrices\nC_x: the covariance matrix of data matrix X\nC_xy: the cross-covariance matrix of X and Y\nU, V: orthogonal matrices of eigenvectors or the left and right singular vectors\nÎ»: vector of eigenvalues, singular values or a function thereof\nÎ›: diagonal matrix of eigenvalues, singular values or a function thereof\nB, F: non-singular matrices","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"In the examples, bold upper-case letters are replaced by upper case letters in order to allow reading in the REPL.","category":"page"},{"location":"Diagonalizations/#acronyms","page":"Diagonalizations","title":"acronyms","text":"","category":"section"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"AJD: Approximate Joint Diagonalization (Cardoso & Souloumiac, 1996; Flury & Gautschi, 1986)\nAJEVD: Approximate Joint Eigenvalue-Eigenvector Decomposition\nAJSVD: Approximate Joint Singular Value Decomposition (Congedo et al., 2011)\nAMUSE: Algorithm for Multiple Source Extraction (Molgedey & Schuster, 1994; Tong et al., 1991)\nBSS: Blind Source Separation\nCCA: Canonical Correlation Analysis (Hotelling, 1936)\nCSP: Common Spatial Pattern (Fukunaga, 1990)\nCSTP: Common Spatio-Temporal Pattern (Congedo et al., 2016)\nEEG: Electroencephalography\nERP: Event-Related Potentials\nFOBI: Fourth-Order Blind Identification (Cardoso, 1989)\nGAJD: Gauss AJD algorithm (Frobenius cost function), unpublished from the author of this package.\nGLogLike: Gauss AJD algorithm (log-likelihood cost function), still experimental.\ngCCA: generalized CCA\ngMCA: generalized MCA\nJADE: Joint Diagonalization of Eigenmatrices AJD algorithm (Cardoso & Souloumiac, 1993)\nJADEmax: JADE using the Riemannian gradient to order the rotations (Usevich et al., 2020)\nLogLike: AJD algorithm optimizing the Log-Likelihood cost function (Pham, 2000)\nLogLikeR: another implementation of LogLike for real data only.\nLShrLW: Linear Shrinkage of Ledoit and Wolf (2004)\nNShrLW: Non-linear Shrinkage of Ledoit and Wolf (2018)\nmAJD: multiple AJD (jointly on several data sets)\nMCA: Maximum Covariance Analysis\nNoJoB: Non-orthogonal mAJD algorithm (Congedo et al., 2012)\nOJoB: Orthogonal mAJD algorithm (Congedo et al., 2012)\nPCA: Principal Component Analysis (Pearson, 1901)\nQNLogLike: Quasi-Newton LogLike (Ablin et al., 2019)\nSCM: Sample Covariance Matrix\nSOBI: Second-Order Blind Identification (Belouchrani et al., 1997)","category":"page"},{"location":"Diagonalizations/","page":"Diagonalizations","title":"Diagonalizations","text":"For the references see ğŸ“.","category":"page"},{"location":"gmca/#gMCA","page":"gMCA","title":"gMCA","text":"","category":"section"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"Generalized Maximum Covariance Analysis (gMCA) is a multiple approximate joint diagonalization prodedure generalizing the maximum covariance analysis (MCA) to the situation m2 (number of datasets), as for MCA with k=1 (one observation).","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"Let X_1X_m be a set of m data matrices of dimension nt, where n is the number of variables and t the number of samples, both common to all datasets. From these data matrices let us estimate","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"C_ij=frac1tX_iX_j^H, for all ij1m, hspace1cm [gmca.1]","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"i.e., all covariance (i=j) and cross-covariance (ij) matrices.","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"The gMCA seeks m matrices F_1F_m diagonalizing as much as possible all products","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"F_i^H C_ij F_j, for all ij1m. hspace1cm [gmca.2]","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"If the MCA (m=2) diagonalizes the cross-covariance, this generalized model (m2) diagonalizes all cross-covariance matrices.","category":"page"},{"location":"gmca/#alternative-model-for-gMCA","page":"gMCA","title":"alternative model for gMCA","text":"","category":"section"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"The gMCA constructors also allow to seeks m matrices F_1F_m diagonalizing as much as possible all products","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"F_i^H C_ij F_j, for all ij1m. hspace1cm [gmca.3]","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"As compared to model [gmca.2], this model diagonalizes the covariance matrices in addition to the cross-covariance matrices.","category":"page"},{"location":"gmca/#permutation-for-gMCA","page":"gMCA","title":"permutation for gMCA","text":"","category":"section"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"As usual, the approximate diagonalizers F_1F_m are arbitrary up to a scale and permutation. In gMCA scaling is fixed by appropriate constraints. For the remaining sign and permutation ambiguities, Diagonalizations.jl attempts to solve them by finding signed permutation matrices for F_1F_m so as to make all diagonal elements of [gmca.2] or [gmca.3] positive and sorted in descending order.","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"Let","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"Î»=Î»_1Î»_n  hspace1cm [gmca.4]","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"be the diagonal elements of","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"frac1m^2-msum_ij=1^m(F_i^H C_ij F_j) hspace1cm [gmca.5]","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"and Ïƒ_TOT=sum_i=1^nÎ»_i be the total covariance.","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"We denote widetildeF_i=f_i1 ldots f_ip the matrix holding the first pn column vectors of F_i, for i1m, where p is the subspace dimension. The explained variance is given by","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"Ïƒ_p=fracsum_i=1^pÎ»_iÏƒ_TOT, hspace1cm [gmca.6]","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"and the accumulated regularized eigenvalues (arev) by","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"Ïƒ_j=sum_i=1^jÏƒ_i, for j=1 ldots n, hspace1cm [gmca.7]","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"where Ïƒ_i is given by Eq. [gmca.6].","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"For setting the subspace dimension p manually, set the eVar optional keyword argument of the gMCA constructors either to an integer or to a real number, this latter establishing p in conjunction with argument eVarMeth using the arev vector (see subspace dimension). By default, eVar is set to 0.999.","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"Solution","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"There is no closed-form solution to the AJD problem in general. See Algorithms.","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"Note that the solution of the MCA are orthogonal matrices. In order to mimic this in gMCA use OJoB. Using NoJoB will constraint the solution only in the general linear group.","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"Constructors","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":"One constructor is available (see here below). The constructed LinearFilter object holding the gMCA will have fields:","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":".F: vector of matrices widetildeF_1widetildeF_m with columns holding the first p eigenvectors in F_1F_m, or just F_1F_m if p=n","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":".iF: the vector of the left-inverses of the matrices in .F","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":".D: the leading pp block of Î›, i.e., the elements [gmca.4] associated to the matrices in .F in diagonal form.","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":".eVar: the explained variance [gmca.6] for the chosen value of p.","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":".ev: the vector Î» [gmca.4].","category":"page"},{"location":"gmca/","page":"gMCA","title":"gMCA","text":".arev: the accumulated regularized eigenvalues, defined by Eq. [gmca.7].","category":"page"},{"location":"gmca/#Diagonalizations.gmca","page":"gMCA","title":"Diagonalizations.gmca","text":"function gmca(ğ—::VecMat;\n              covEst     :: StatsBase.CovarianceEstimator = SCM,\n              dims       :: Into    = â—‹,\n              meanX      :: Into    = 0,\n          algorithm :: Symbol    = :OJoB,\n          fullModel :: Bool      = false,\n          sort      :: Bool      = true,\n          init      :: VecMato   = â—‹,\n          tol       :: Real      = 0.,\n          maxiter   :: Int       = _maxiter(algorithm, eltype(ğ—[1])),\n          verbose   :: Bool      = false,\n          threaded  :: Bool      = true,\n        eVar     :: TeVaro   = _minDim(ğ—),\n        eVarMeth :: Function = searchsortedfirst,\n        simple   :: Bool     = false)\n\n\nReturn a LinearFilter object.\n\nGeneralized Maximum Covariance Analysis of the set of m data matrices ğ— using the given solving algorithm (OJoB by default).\n\nIf fullModel is true, the [gmca.3] problem here above is solved, otherwise (default), the [gmca.2] problem here above is solved.\n\nIf sort is true (default), the column vectors of the matrices F_1F_m are signed and permuted as explained here above in permutation for gMCA, otherwise they will have arbitrary sign and will be in arbitrary order.\n\nRegarding arguments init, tol and maxiter, see Algorithms.\n\nIf verbose is true (false by default), the convergence attained at each iteration will be printed in the REPL.\n\neVar and eVarMeth are used to define a subspace dimension p using the accumulated regularized eigenvalues of Eq. [gmca.7].\n\nThe default values are:\n\neVar is set to the minimum dimension of the matrices in ğ—\neVarMeth=searchsortedfirst\n\nIf simple is set to true, p is set equal to the dimension of the covariance matrices that are computed on the matrices in ğ—, which depends on the choice of dims, and only the fields .F and .iF are written in the constructed object. This corresponds to the typical output of approximate diagonalization algorithms.\n\nif threaded=true (default) and the number of threads Julia is instructed to use (the output of Threads.nthreads()), is higher than 1, solving algorithms supporting multi-threading run in multi-threaded mode. See Algorithms and these notes on multi-threading.\n\nSee also: MCA, gCCA, mAJD.\n\nExamples:\n\nusing Diagonalizations, LinearAlgebra, PosDefManifold, Test\n\n\n####  Create data for testing the case k=1, m>1\n# `t` is the number of samples,\n# `m` is the number of datasets,\n# `n` is the number of variables,\n# `noise` must be smaller than 1.0. The smaller the noise,\n#  the more data are correlated.\nfunction getData(t, m, n, noise)\n    # create m identical data matrices and rotate them by different\n    # random orthogonal matrices V_1,...,V_m\n    ğ•=[randU(n) for i=1:m] # random orthogonal matrices\n    X=randn(n, t)  # data common to all subjects\n    # each subject has this common part plus a random part\n    ğ—=[ğ•[i]'*((1-noise)*X + noise*randn(n, t)) for i=1:m]\n    return ğ—\nend\n\nfunction getData(::Type{Complex{T}}, t, m, n, noise) where {T<:AbstractFloat}\n    # create m identical data matrices and rotate them by different\n    # random orthogonal matrices V_1,...,V_m\n    ğ•=[randU(ComplexF64, n) for i=1:m] # random orthogonal matrices\n    X=randn(ComplexF64, n, t)  # data common to all subjects\n    # each subject has this common part plus a random part\n    ğ—=[ğ•[i]'*((1-noise)*X + noise*randn(ComplexF64, n, t)) for i=1:m]\n    return ğ—\nend\n\n\n# REAL data: check that for the case m=2 gMCA gives the same result as MCA\nt, m, n, noise = 20, 2, 6, 0.1\nXset=getData(t, m, n, noise)\nCx=(Xset[1]*Xset[1]')/t\nCy=(Xset[2]*Xset[2]')/t\nCxy=(Xset[1]*Xset[2]')/t\n\ngm=gmca(Xset; simple=true)\nm=mca(Cxy; simple=true)\n\n@test (m.F[1]'*Cxy*m.F[2]) â‰ˆ (gm.F[1]'*Cxy*gm.F[2])\n# the following must be the identity matrix out of a possible sign ambiguity\n@test abs.(m.F[1]'*gm.F[1]) â‰ˆ I\n@test abs.(m.F[2]'*gm.F[2]) â‰ˆ I\n\n# COMPLEX data: check that for the case m=2 gMCA gives the same result as MCA\nt, m, n, noise = 20, 2, 6, 0.1\nXcset=getData(ComplexF64, t, m, n, noise)\nCcx=(Xcset[1]*Xcset[1]')/t\nCcy=(Xcset[2]*Xcset[2]')/t\nCcxy=(Xcset[1]*Xcset[2]')/t\n\ngmc=gmca(Xcset; simple=true)\nmc=mca(Ccxy; simple=true)\n\n# for complex data just do a sanity check as the order of vectors\n# is arbitrary\n@test spForm(mc.F[1]'gmc.F[1])<0.01\n@test spForm(mc.F[2]'gmc.F[2])<0.01\n\n\n# REAL data: m>2 case\nt, m, n, noise = 20, 4, 6, 0.1\nXset=getData(t, m, n, noise)\n\n# ... selecting subspace dimension allowing an explained variance = 0.9\ngm=gmca(Xset, eVar=0.9)\n\n# name of the filter\ngm.name\n\nğ’=Array{Matrix}(undef, 1, m, m)\nfor i=1:m, j=1:m ğ’[1, i, j]=(Xset[i]*Xset[j]')/t end\n\nusing Plots\n# plot regularized accumulated eigenvalues\nplot(gm.arev)\n\n\n# plot the original cross-covariance matrices and the rotated\n# cross-covariance matrices\n\n# Get all products ğ”[i]' * ğ’[l, i, j] * ğ”[j]\nfunction _rotate_crossCov(ğ”, ğ’, m, k)\n    ğ’®=Array{Matrix}(undef, k, m, m)\n    @inbounds for l=1:k, i=1:m, j=1:m ğ’®[l, i, j]=ğ”[i]'*ğ’[l, i, j]*ğ”[j] end\n    return ğ’®\nend\n\n\n# Put all cross-covariances in a single matrix of dimension m*n x m*n for visualization\nfunction ğ’2Mat(ğ’::AbstractArray, m, k)\n    n=size(ğ’[1, 1, 1], 1)\n    C=Matrix{Float64}(undef, m*n, m*n)\n    for i=1:m, j=1:m, x=1:n, y=1:n C[i*n-n+x, j*n-n+y]=ğ’[k, i, j][x, y] end\n    return C\nend\n\n C=ğ’2Mat(ğ’, m, 1)\n Cmax=maximum(abs.(C));\n h1 = heatmap(C, clim=(-Cmax, Cmax), yflip=true, c=:bluesreds, title=\"all cross-covariances\")\n ğ’®=_rotate_crossCov(gm.F, ğ’, m, 1)\n S=ğ’2Mat(ğ’®, m, 1)\n Smax=maximum(abs.(S));\n h2 = heatmap(S, clim=(0, Smax), yflip=true, c=:amp, title=\"all rotated cross-covariances\")\n ğŸ“ˆ=plot(h1, h2, size=(700,300))\n# savefig(ğŸ“ˆ, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FiggMCA.png\")\n\n\n(Image: Figure gMCA)\n\nIn the figure here above, the rotated cross-covariance matrices have the expected  strip-diagonal form, that is, each block F_i^Tfrac1t(X_iX_j^T)F_j,  for ij1m, is approximately diagonal. Each block is 55 because  setting eVar=0.9 the subspace dimension has been set to 5.\n\n# COMPLEX data: m>2 case\nt, m, n, noise = 20, 4, 6, 0.1\nXcset=getData(ComplexF64, t, m, n, noise)\n\n# ... selecting subspace dimension allowing an explained variance = 0.9\ngmc=gmca(Xcset, eVar=0.9)\n\n\n\n\n\n","category":"function"},{"location":"cca/#CCA","page":"CCA","title":"CCA","text":"","category":"section"},{"location":"cca/","page":"CCA","title":"CCA","text":"Canonical Correlation Analysis was first proposed by Hotelling (1936) ğŸ“. It can be conceived as the multivariate extension of Pearson's product-moment correlation, as the bilinear version of Whitening or as the standardized version of MCA; if MCA maximizes the cross-covariance of two data set, CCA maximizes their correlation. Like the MCA, the CCA corresponds to the situation m=2 (two datasets) and k=1 (one observation).","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"As per MCA, Let X and Y be two n_xt and n_yt data matrices, where n_x and n_y are the number of variables in X and Y, respectively and t the number of samples. We assume here too that the samples in X and Y are synchronized. Let C_x be the n_xn_x covariance matrix of X and C_y be the n_yn_y covariance matrix of Y. Let also C_xy=frac1tX^HY be the n_xn_y cross-covariance matrix. CCA seeks two matrices F_x and F_y such that","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"left  beginarrayrlF_x^HC_xF_x=IF_y^HC_yF_y=IF_x^HC_xyF_y=Î› endarray right, hspace1cm [cca.1]","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"where Î› is a nn diagonal matrix, with n=min(n_x n_y). The first components (rows) of F_x^HX and F_y^HY hold the linear combination of X and Y with maximal correlation, the second the linear combination with maximal residual correlation and so on, subject to constraint [cca.1].","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"In matrix form this can be written such as","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"beginpmatrix F_x^H  00  F_y^Hendpmatrix beginpmatrix C_x  C_xyC_xy^H  C_yendpmatrix beginpmatrix F_x  00  F_yendpmatrix = beginpmatrix I  Î›Î›  Iendpmatrix. hspace1cm [cca.2]","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"If n_x=n_y, from [cca.1] it follows","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"left  beginarrayrlF_x^-HF_x^-1=C_xF_y^-HF_y^-1=C_yF_x^-HF_y^-1=C_xy endarray right, hspace1cm [cca.3]","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"that is, CCA is a special kind of full-rank factorization of C_x and C_y.","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"Equating the left hand sides of the first two expressions in [cca.1] and setting E_x=B_xB_y^-H, E_y=B_yB_x^-H, it follows","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"left  beginarrayrlC_x=E_xC_yE_x^HC_y=E_yC_xE_y^H endarray right, hspace1cm [cca.4]","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"that is, CCA defines a linear transormation linking the covariance matrices of two data sets.","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"Since it maximizes the correlation, differently from MCA, CCA is not sensitive to the amplitude of the two processes. This should be kept in mind, as the correlation of two signals may be high even if the amplitude of one of the two signal is very low, say, at the noise level. In such a case the resulting correlation is meaningless.","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"The accumulated regularized eigenvalues (arev) for the CCA are defined as","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"Ïƒ_j=sum_i=1^jÏƒ_i, for j=1 ldots n, hspace1cm [cca.5]","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"where Ïƒ_i is given by","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"Ïƒ_p=fracsum_i=1^pÎ»_iÏƒ_TOT  hspace1cm [cca.6]","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"and Î»_i are the singular values in Î› of [cca.1].","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"For setting the subspace dimension p manually, set the eVar optional keyword argument of the CCA constructors either to an integer or to a real number, this latter establishing p in conjunction with argument eVarMeth using the arev vector (see subspace dimension). By default, eVar is set to 0.999.","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"Solution","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"If n_x=n_y the solutions to the CCA B_x and B_y are given by the eigenvector matrix of (non-symmetric) matrices C_x^-1C_xyC_y^-1C_yx and C_y^-1C_yxC_x^-1C_xy, respecively.","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"A numerically preferable solution, accommodating also the case n_xn_y, is the following two-step procedure, which is the one here implemented:","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"get some whitening matrices hspace01cm(W_x W_y)hspace01cm such that hspace01cmW_x^HC_xW_x=Ihspace01cm and hspace01cmW_y^HC_yW_y=I\ndo hspace01cmtextrmSVD(W_x^HC_xyW_y)=U_xÎ›U_y^H","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"The solutions are hspace01cmB_x=W_xU_xhspace01cm and hspace01cmB_y=W_yU_yhspace01cm.","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"Constructors","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":"Three constructors are available (see here below). The constructed LinearFilter object holding the CCA will have fields:","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":".F[1]: matrix widetildeB_x=b_x1 ldots b_xp with the columns holding the first p vectors in B_x.","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":".F[2]: matrix widetildeB_y=b_y1 ldots b_yp with the columns holding the first p vectors in B_y.","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":".iF[1]: the left-inverse of .F[1]","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":".iF[2]: the left-inverse of .F[2]","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":".D: the leading pp block of Î›, i.e., the correlation values associated to .F in diagonal form.","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":".eVar: the explained variance for the chosen value of p, defined in [cca.6].","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":".ev: the vector diag(Î›) holding all n correlation values.","category":"page"},{"location":"cca/","page":"CCA","title":"CCA","text":".arev: the accumulated regularized eigenvalues, defined in [cca.5].","category":"page"},{"location":"cca/#Diagonalizations.cca","page":"CCA","title":"Diagonalizations.cca","text":"(1)\nfunction cca(Cx :: SorH, Cy :: SorH, Cxy :: Mat;\n             eVarCx   :: TeVaro=â—‹,\n             eVarCy   :: TeVaro=â—‹,\n             eVar     :: TeVaro=â—‹,\n             eVarMeth :: Function=searchsortedfirst,\n             simple   :: Bool=false)\n\n(2)\nfunction cca(X::Mat, Y::Mat;\n             covEst   :: StatsBase.CovarianceEstimator=SCM,\n             dims     :: Into = â—‹,\n             meanX    :: Tmean = 0,\n             meanY    :: Tmean = 0,\n             wX       :: Tw = â—‹,\n             wY       :: Tw = â—‹,\n             wXY      :: Tw = â—‹,\n          eVarCx   :: TeVaro = â—‹,\n          eVarCy   :: TeVaro = â—‹,\n          eVar     :: TeVaro = â—‹,\n          eVarMeth :: Function = searchsortedfirst,\n          simple   :: Bool = false)\n\n(3)\nfunction cca(ğ—::VecMat, ğ˜::VecMat;\n             covEst   :: StatsBase.CovarianceEstimator=SCM,\n             dims     :: Into = â—‹,\n             meanX    :: Into = 0,\n             meanY    :: Into = 0,\n          eVarCx   :: TeVaro = â—‹,\n          eVarCy   :: TeVaro = â—‹,\n          eVar     :: TeVaro = â—‹,\n          eVarMeth :: Function = searchsortedfirst,\n          simple   :: Bool = false,\n       metric   :: Metric = Euclidean,\n       wCx      :: Vector = [],\n       wCy      :: Vector = [],\n       âœ“w       :: Bool = true,\n       initCx   :: SorHo = nothing,\n       initCy   :: SorHo = nothing,\n       tol      :: Real = 0.,\n       verbose  :: Bool = false)\n\nReturn a LinearFilter object:\n\n(1) Canonical correlation analysis with, as input, real or complex:\n\ncovariance matrix Cx of dimension n_xn_x,\ncovariance matrix Cy of dimension n_yn_y and\ncross-covariance matrix Cxy of dimension n_xn_y.\n\nCx and Cy must be flagged as Symmetric, if real or Hermitian, if real or complex, see data input. Instead Cxy is a generic Matrix object since it is not symmetric/Hermitian, left alone square.\n\neVarCx, eVarCy, eVar and eVarMeth are keyword optional arguments for defining the subspace dimension p. Particularly:\n\neVarCx and eVarCy are used to determine a subspace dimension in the whitening of Cx and Cy, i.e., in the first step of the two-step procedure descrived above in the solution section.\neVar is used to determine the final subspace dimension using the .arev vector given by Eq. [cca.5] here above.\neVarMeth applies to eVarCx, eVarCy and eVar.\n\nThe default values are:\n\neVarCx=0.999\neVarCy=0.999\neVar=0.999\neVarMeth=searchsortedfirst\n\nIf simple is set to true, p is set equal to min(n_x n_y) and only the fields .F and .iF are written in the constructed object. This option is provided for low-level work when you don't need to define a subspace dimension or you want to define it by your own methods.\n\n(2) Canonical correlation analysis with real or complex data matrices X and Y as input.\n\ncovEst, dims, meanX, meanY,  wX, wY and wXY are optional keyword arguments to regulate the estimation of the covariance matrices of X and Y and their cross-covariance matrix C_xy. Particularly (See covariance matrix estimations),\n\nmeanX is the mean argument for data matrix X.\nmeanY is the mean argument for data matrix Y.\nwX is the w argument for estimating a weighted covariance matrix C_x.\nwY is the w argument for estimating a weighted covariance matrix C_y.\nwXY is the w argument for estimating a weighted  cross-covariance matrix C_XY.\ncovEst applies only to the estimation of C_x and C_y.\n\nOnce C_x, C_y and C_xy estimated, method (1) is invoked with optional keyword arguments eVar, eVarCx, eVarCy, eVarMeth and simple. See method (1) for details.\n\n(3) Canonical correlation analysis with two vectors of real or complex data matrices ğ— and ğ˜ as input. ğ— and ğ˜ must hold the same number of matrices and corresponding pairs of matrices therein must comprise the same number of samples.\n\ncovEst, dims, meanX and meanY are optional keyword arguments to regulate the estimation of the covariance matrices of all matrices in ğ— and ğ˜ and the cross-covariance matrices for all pairs of their corresponding data matrices. See method (2) and covariance matrix estimations.\n\nA mean of these covariance matrices is computed. For the cross-covariance matrices the arithmetic mean is used. For the covariance matrices of ğ— and ğ˜, optional keywords arguments metric, wCx, wCy, âœ“w, initCx, initCy, tol and verbose are used to allow non-Euclidean mean estimations. Particularly (see mean covariance matrix estimations),\n\nwCx are the weights for the covariance matrices of ğ—,\nwCy are the weights for the covariance matrices of ğ˜,\ninitCx is the initialization for the mean of the covariance matrices of ğ—,\ninitCy is the initialization for the mean of the covariance matrices of ğ˜.\n\nBy default, the arithmetic mean is computed.\n\nOnce the mean covariance and cross-covariance matrices are estimated, method (1) is invoked with optional keyword arguments eVarCx, eVarCy, eVar, eVarMeth and simple. See method (1) for details.\n\nSee also: Whitening, MCA, gCCA, mAJD.\n\nExamples:\n\nusing Diagonalizations, LinearAlgebra, PosDefManifold, Test\n\n# Method (1) real\nn, t=10, 100\nX=genDataMatrix(n, t)\nY=genDataMatrix(n, t)\nCx=Symmetric((X*X')/t)\nCy=Symmetric((Y*Y')/t)\nCxy=(X*Y')/t\ncC=cca(Cx, Cy, Cxy, simple=true)\n@test cC.F[1]'*Cx*cC.F[1]â‰ˆI\n@test cC.F[2]'*Cy*cC.F[2]â‰ˆI\nD=cC.F[1]'*Cxy*cC.F[2]\n@test norm(D-Diagonal(D))+1. â‰ˆ 1.\n\n# Method (1) complex\nXc=genDataMatrix(ComplexF64, n, t)\nYc=genDataMatrix(ComplexF64, n, t)\nCxc=Hermitian((Xc*Xc')/t)\nCyc=Hermitian((Yc*Yc')/t)\nCxyc=(Xc*Yc')/t\ncCc=cca(Cxc, Cyc, Cxyc, simple=true)\n@test cCc.F[1]'*Cxc*cCc.F[1]â‰ˆI\n@test cCc.F[2]'*Cyc*cCc.F[2]â‰ˆI\nDc=cCc.F[1]'*Cxyc*cCc.F[2]\n@test norm(Dc-Diagonal(Dc))+1. â‰ˆ 1.\n\n\n# Method (2) real\ncXY=cca(X, Y, simple=true)\n@test cXY.F[1]'*Cx*cXY.F[1]â‰ˆI\n@test cXY.F[2]'*Cy*cXY.F[2]â‰ˆI\nD=cXY.F[1]'*Cxy*cXY.F[2]\n@test norm(D-Diagonal(D))+1. â‰ˆ 1.\n@test cXY==cC\n\n# Method (2) complex\ncXYc=cca(Xc, Yc, simple=true)\n@test cXYc.F[1]'*Cxc*cXYc.F[1]â‰ˆI\n@test cXYc.F[2]'*Cyc*cXYc.F[2]â‰ˆI\nDc=cXYc.F[1]'*Cxyc*cXYc.F[2]\n@test norm(Dc-Diagonal(Dc))+1. â‰ˆ 1.\n@test cXYc==cCc\n\n\n# Method (3) real\n# canonical correlation analysis of the average covariance and cross-covariance\nk=10\nXset=[genDataMatrix(n, t) for i=1:k]\nYset=[genDataMatrix(n, t) for i=1:k]\n\nc=cca(Xset, Yset)\n\n# ... selecting subspace dimension allowing an explained variance = 0.9\nc=cca(Xset, Yset; eVar=0.9)\n\n# ... subtracting the mean from the matrices in Xset and Yset\nc=cca(Xset, Yset; meanX=nothing, meanY=nothing, eVar=0.9)\n\n# cca on the average of the covariance and cross-covariance matrices\n# computed along dims 1\nc=cca(Xset, Yset; dims=1, eVar=0.9)\n\n# name of the filter\nc.name\n\nusing Plots\n# plot regularized accumulated eigenvalues\nplot(c.arev)\n\n# plot the original covariance and cross-covariance matrices\n# and their transformed counterpart\n CxyMax=maximum(abs.(Cxy));\n h1 = heatmap(Cxy, clim=(-CxyMax, CxyMax), title=\"Cxy\", yflip=true, c=:bluesreds);\n D=cC.F[1]'*Cxy*cC.F[2];\n Dmax=maximum(abs.(D));\n h2 = heatmap(D, clim=(0, Dmax), title=\"F1'CxyF2\", yflip=true, c=:amp);\n CxMax=maximum(abs.(Cx));\n h3 = heatmap(Cx, clim=(-CxMax, CxMax), title=\"Cx\", yflip=true, c=:bluesreds);\n h4 = heatmap(cC.F[1]'*Cx*cC.F[1], clim=(0, 1), title=\"F1'CxF1\", yflip=true, c=:amp);\n CyMax=maximum(abs.(Cy));\n h5 = heatmap(Cy, clim=(-CyMax, CyMax), title=\"Cy\", yflip=true, c=:bluesreds);\n h6 = heatmap(cC.F[2]'*Cy*cC.F[2], clim=(0, 1), title=\"F2'CyF2\", yflip=true, c=:amp);\n ğŸ“ˆ=plot(h3, h5, h1, h4, h6, h2, size=(800,400))\n# savefig(ğŸ“ˆ, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FigCCA.png\")\n\n\n(Image: Figure CCA)\n\n# Method (3) complex\n# canonical correlation analysis of the average covariance and cross-covariance\nk=10\nXcset=[genDataMatrix(ComplexF64, n, t) for i=1:k]\nYcset=[genDataMatrix(ComplexF64, n, t) for i=1:k]\n\ncc=cca(Xcset, Ycset)\n\n# ... selecting subspace dimension allowing an explained variance = 0.9\ncc=cca(Xcset, Ycset; eVar=0.9)\n\n# ... subtracting the mean from the matrices in Xset and Yset\ncc=cca(Xcset, Ycset; meanX=nothing, meanY=nothing, eVar=0.9)\n\n# cca on the average of the covariance and cross-covariance matrices\n# computed along dims 1\ncc=cca(Xcset, Ycset; dims=1, eVar=0.9)\n\n# name of the filter\ncc.name\n\n\n\n\n\n","category":"function"},{"location":"mca/#MCA","page":"MCA","title":"MCA","text":"","category":"section"},{"location":"mca/","page":"MCA","title":"MCA","text":"Maximum Covariance Analysis is obtained by singular-value decomposition. It can be conceived as the multivariate extension of covariance and as the bilinear version of PCA; if PCA diagonalizes the covariance matrix of a data set, MCA diagonalized the cross-covariance matrix of two data sets. It corresponds to the situation m=2 (two datasets) and k=1 (one observation).","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"Let X and Y be two n_xt and n_yt data matrices, where n_x and n_y are the number of variables in X and Y, respectively and t the number of samples. We assume that the samples in X and Y are synchronized. Let C_xy=frac1tX^HY be the n_xn_y cross-covariance matrix. MCA seeks two orthogonal matrices U and V such that","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"U_x^HC_xyU_y=Î›, hspace1cm [mca.1]","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"where Î› is a nn diagonal matrix, with n=min(n_x n_y). The first components (rows) of U_x^HX and U_y^HY hold the linear combination of X and Y with maximal covariance, the second the linear combination with maximal residual covariance and so on, subject to constraint U_x^HU_x=I and U_y^HU_x=I. If n_x=n_y, U_x and U_y are both square, hence it holds also U_xU_x^H=I and U_yU_y^H=I, otherwise this holds only for one of them.","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"It should be kept in mind that MCA is sensitive to the amplitude of each process, since the covariance is maximized and not the correlation as in CCA. Threfore, if the amplitude of the two processes is not homegeneous, the covariance will be driven by the process with highest amplitude.","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"The accumulated regularized eigenvalues (arev) for the MCA are defined as","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"Ïƒ_j=sum_i=1^jÏƒ_i, for j=1 ldots n, hspace1cm [mca.2]","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"where Ïƒ_i is given by","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"Ïƒ_p=fracsum_i=1^pÎ»_iÏƒ_TOT  hspace1cm [mca.3]","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"and Î»_i are the singular values in Î› of [mca.1].","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"For setting the subspace dimension p manually, set the eVar optional keyword argument of the MCA constructors either to an integer or to a real number, this latter establishing p in conjunction with argument eVarMeth using the arev vector (see subspace dimension). By default, eVar is set to 0.999.","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"Solution","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"The MCA solution is given by the singular value decoposition of C_xy","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"textrmSVD(C_xy)=U_xÎ›U_y^H.","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"It is worth mentioning that","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"widetildeU_xwidetildeÎ›widetildeU_y^H,","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"where widetildeU_x=u_x1 ldots u_xp is the matrix holding the first pn_x left singular vectors, widetildeU_y=u_y1 ldots u_yp is the matrix holding the first pn_y right singular vectors and widetildeÎ› is the leading pp block of Î›, is the best approximant to C_xy with rank p in the least-squares sense (Good, 1969)ğŸ“.","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"Constructors","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":"Three constructors are available (see here below). The constructed LinearFilter object holding the MCA will have fields:","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":".F[1]: matrix widetildeU_x with orthonormal columns holding the first p left singular vectors in U_x.","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":".F[2]: matrix widetildeU_y with orthonormal columns holding the first p right singular vectors in U_y.","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":".iF[1]: the (conjugate) transpose of .F[1]","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":".iF[2]: the (conjugate) transpose of .F[2]","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":".D: the leading pp block of Î›, i.e., the singular values associated to .F in diagonal form.","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":".eVar: the explained variance for the chosen value of p, defined in [mca.3].","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":".ev: the vector diag(Î›) holding all n singular values.","category":"page"},{"location":"mca/","page":"MCA","title":"MCA","text":".arev: the accumulated regularized eigenvalues, defined in [mca.2].","category":"page"},{"location":"mca/#Diagonalizations.mca","page":"MCA","title":"Diagonalizations.mca","text":"(1)\nfunction mca(Cxy :: Mat;\n             eVar      :: TeVaro = â—‹,\n             eVarMeth  :: Function = searchsortedfirst,\n             simple    :: Bool = false)\n\n(2)\nfunction mca(X::Mat, Y::Mat;\n             dims     :: Into = â—‹,\n             meanX    :: Tmean = 0,\n             meanY    :: Tmean = 0,\n             wXY      :: Tw = â—‹,\n          eVar     :: TeVaro = â—‹,\n          eVarMeth :: Function = searchsortedfirst,\n          simple   :: Bool = false)\n\n(3)\nfunction mca(ğ—::VecMat, ğ˜::VecMat;\n             dims     :: Into = â—‹,\n             meanX    :: Into = 0,\n             meanY    :: Into = 0,\n          eVar     :: TeVaro = â—‹,\n          eVarMeth :: Function = searchsortedfirst,\n          simple   :: Bool = false)\n\nReturn a LinearFilter object:\n\n(1) Maximum covariance analysis with real or complex covariance matrix Cxy of dimension n_xn_y as input.\n\nDifferently from PCA and Whitening, Cxy is a generic Matrix object since it is not symmetric/Hermitian, left alone square.\n\neVar and eVarMeth are keyword optional arguments for defining the subspace dimension p using the .arev vector given by Eq. [mca.2].\n\nThe default values are:\n\neVar=0.999\neVarMeth=searchsortedfirst\n\nIf simple is set to true, p is set equal to min(n_x n_y) and only the fields .F and .iF are written in the constructed object. This option is provided for low-level work when you don't need to define a subspace dimension or you want to define it by your own methods.\n\n(2) Maximum covariance analysis with real or complex data matrices X and Y as input.\n\ndims, meanX, meanY and wXY are optional keyword arguments to regulate the estimation of the cross-covariance matrix C_xy. Particularly (see covariance matrix estimations),\n\nmeanX is the mean argument for data matrix X.\nmeanY is the mean argument for data matrix Y.\nwXY is the weight argument for estimating a weighted cross-covariance matrix C_XY.\n\nOnce the cross-covariance matrix estimated, method (1) is invoked with optional keyword arguments eVar, eVarMeth and simple. See method (1) for details.\n\n(3) Maximum covariance analysis with two vectors of real or complex data matrices ğ— and ğ˜ as input. ğ— and ğ˜ must hold the same number of matrices and corresponding pairs of matrices therein must comprise the same number of samples.\n\ndims, meanX and meanY are optional keyword arguments to regulate the estimation of the cross-covariance matrices for all pairs of corresponding data matrices in ğ— and ğ˜. See method (2) and covariance matrix estimations.\n\nThe arithmetic mean of these cross-covariance matrices is computed and method (1) is invoked with optional keyword arguments eVar, eVarMeth and simple. See method (1) for details.\n\nSee also: PCA, CCA, gMCA, mAJD.\n\nExamples:\n\nusing Diagonalizations, LinearAlgebra, PosDefManifold, Test\n\n# Method (1) real\nn, t=10, 100\nX=genDataMatrix(n, t)\nY=genDataMatrix(n, t)\nCx=Symmetric((X*X')/t)\nCy=Symmetric((Y*Y')/t)\nCxy=(X*Y')/t\nmC=mca(Cxy, simple=true)\n@test Cxyâ‰ˆmC.F[1]*mC.D*mC.F[2]'\nD=mC.F[1]'Cxy*mC.F[2]\n@test norm(D-Diagonal(D))+1. â‰ˆ 1.\n\n# Method (1) complex\nXc=genDataMatrix(ComplexF64, n, t)\nYc=genDataMatrix(ComplexF64, n, t)\nCxc=Symmetric((Xc*Xc')/t)\nCyc=Symmetric((Yc*Yc')/t)\nCxyc=(Xc*Yc')/t\nmCc=mca(Cxyc, simple=true)\n@test Cxycâ‰ˆmCc.F[1]*mCc.D*mCc.F[2]'\nDc=mCc.F[1]'Cxyc*mCc.F[2]\n@test norm(Dc-Diagonal(Dc))+1. â‰ˆ 1.\n\n\n# Method (2) real\nmXY=mca(X, Y, simple=true)\nD=mXY.F[1]'*Cxy*mXY.F[2]\n@test norm(D-Diagonal(D))+1â‰ˆ1.\n@test mXY==mC\n\n# Method (2) complex\nmXYc=mca(Xc, Yc, simple=true)\nDc=mXYc.F[1]'*Cxyc*mXYc.F[2]\n@test norm(Dc-Diagonal(Dc))+1. â‰ˆ 1.\n@test mXYc==mCc\n\n\n# Method (3) real\n# maximum covariance analysis of the average covariance and cross-covariance\nk=10\nXset=[genDataMatrix(n, t) for i=1:k]\nYset=[genDataMatrix(n, t) for i=1:k]\n\nm=mca(Xset, Yset)\n\n# ... selecting subspace dimension allowing an explained variance = 0.5\nm=mca(Xset, Yset; eVar=0.5)\n\n# ... subtracting the mean from the matrices in Xset and Yset\nm=mca(Xset, Yset; meanX=nothing, meanY=nothing, eVar=0.5)\n\n# mca on the average of the covariance and cross-covariance matrices\n# computed along dims 1\nm=mca(Xset, Yset; dims=1, eVar=0.5)\n\n# name of the filter\nm.name\n\nusing Plots\n# plot regularized accumulated eigenvalues\nplot(m.arev)\n\n# plot the original cross-covariance matrix and the rotated\n# cross-covariance matrix\n Cmax=maximum(abs.(Cxy));\n h1 = heatmap(Cxy, clim=(-Cmax, Cmax), yflip=true, c=:bluesreds, title=\"Cxy\");\n D=mC.F[1]'*Cxy*mC.F[2];\n Dmax=maximum(abs.(D));\n h2 = heatmap(D, clim=(0, Dmax), yflip=true, c=:amp, title=\"F[1]'*Cxy*F[2]\");\n ğŸ“ˆ=plot(h1, h2, size=(700,300))\n# savefig(ğŸ“ˆ, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FigMCA.png\")\n\n(Image: Figure MCA)\n\n# Method (3) complex\n# maximum covariance analysis of the average covariance and cross-covariance\n\nk=10\nXcset=[genDataMatrix(ComplexF64, n, t) for i=1:k]\nYcset=[genDataMatrix(ComplexF64, n, t) for i=1:k]\n\nmc=mca(Xcset, Ycset)\n\n# ... selecting subspace dimension allowing an explained variance = 0.5\nmc=mca(Xcset, Ycset; eVar=0.5)\n\n# ... subtracting the mean from the matrices in Xset and Yset\nmc=mca(Xcset, Ycset; meanX=nothing, meanY=nothing, eVar=0.5)\n\n# mca on the average of the covariance and cross-covariance matrices\n# computed along dims 1\nmc=mca(Xcset, Ycset; dims=1, eVar=0.5)\n\n# name of the filter\nmc.name\n\n\n\n\n\n","category":"function"},{"location":"csp/#CSP","page":"CSP","title":"CSP","text":"","category":"section"},{"location":"csp/","page":"CSP","title":"CSP","text":"The Common Spatial Pattern (CSP) are filters obtained by generalized eigenvalue-eigenvector decomposition. They corresponds to the situation m=1 (one dataset) and k=2 (two observation). The goal of a CSP filter is to maximize a variance ratio. Let (X_1 X_2) be (nt_1 nt_2) data matrices, where n is their number of variables and (t_1 t_2) their number of samples. Let (C_1 C_2) be the nn covariance matrices of data matrices (X_1 X_2) and C=C_1+C_2. The CSP consists in the joint diagonalization of C and C_1 or, equivalently, of C and C_2 (Fukunaga, 1990, p. 31-33 ğŸ“). The joint diagonalizer B is scaled such as to verify (see scale and permutation)","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"left  beginarrayrlB^TCB=IB^TC_1B=Î›B^TC_2B=I-Î› endarray right, hspace1cm [csp.1]","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"That is to say, Î»_1ldotsÎ»_n are the diagonal elements of B^TC_1B and 1-Î»_1ldots1-Î»_n the diagonal elements of B^TC_2B. The CSP maximizes the ratio between the variance of the corresponding components of transformed processes B^TX_1 and B^TX_2, constraining their sum to unity. The ratio is ordered by descending order such as","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"Ïƒ=Î»_1(1-Î»_1)ldotsÎ»_n(1-Î»_n), hspace1cm [csp.2]","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"The CSP has two use cases, which can be selected using the selMeth optional keyword argument of its constructors:","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"a) Separating two classes.   In this case C_1 and C_2 are   covariance matrices of two distinct classes. The constructors will   retain a filter with form   widetildeB=B_1 B_2, where we have defined partitions   B=B_1 B_0 B_2. B_1 and B_2 are the first p_1 and last p_2 vectors of B corresponding to high and low values of   the variance ratio [csp.2], that is, explaining variance   that is useful for separating the classes, while B_0 corresponds   to components corresponding to variance ratios close to 1,   meaning that are not useful for separating the classes.   The subspace dimension in this use case will be given by p=p_1+p_2.   Notice that in Diagonalizations.jl we allow p_1p_2.\nb) Enhance the signal-to-noise ratio.   In this case C_1 and C_2 are   covariance matrices of the 'signal' and of the 'noise', respectively.   The constructors will retain a filter widetildeB=b_1 ldots b_p holding the first p vectors of B corresponding to the highest   values of the variance ratio [csp.2]. The discared n-p vectors   correspond to components that explains progressively more and more   variance related to the noise process. In this case we have   a natural landmark for selecting automatically the   subspace dimension p, as the larger dimension   whose variance ratio is greater then 1.","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"In order to retrive the appropriate partitions of B to construct a filter given a subspace dimension p, we need a way to measure the distance of the ratios [csp.2] from 1. For this purpose we will make use of the Fisher distance adopted on the Riemannian manifold of positive definite matrices, in its scalar form, yielding","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"Î´_i=textrmlog^2(Ïƒ_i), for i=1 ldots n. hspace1cm [csp.3]","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"After this transformation, extreme values of the ratio becomes high, that is, the function Î´_i assumes the shape of a (non-symmetric) wine cup and has a minimum close to zero.","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"Now let Î´_TOT=sum_i=1^nÎ´_i be the total distance and define the explained variance of the CSP for dimension p such as","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"v_p=fracsum_j=1^pÎ´_jÎ´_TOT, hspace1cm [csp.4]","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"where the Î´_j's are given by [csp.3]. Note that for use case a) described here above the accumulated sums in [csp.4] are computed after sorting the Î´_i values in descending order.","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"The arev field of CSP filter is defined as the accumulated variance ratio given by","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"v_1ldotsv_n, hspace1cm [csp.5]","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"where the v_i's' are defined in [csp.4].","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"For setting the subspace dimension p manually, set the eVar optional keyword argument of the CSP constructors either to an integer or to a real number (see subspace dimension). If you don't, by default p is chosen (see Fig. 2)","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"in use case a) as the larger dimension p whose sorted Î´_p value [csp.3]  is larger then the geometric mean of Î´ 1. n2 is taken as upper bound.\nin use case b) as the larger dimension p whose Ïƒ_p value [csp.2] is larger then 1.","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"In both cases the dimension corresponding to the munimum of [csp.3] is taken  as an upper bound and p can be equal to at most n-1.","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"(Image: Figure 2)  Figure 2 Illustration of the way CSP constrcuctors determines  automatically the subspace dimension p under use case a) and b).  On the left, we are interested in the p_1 and p_2 vectors of B  associated with extremal values of the variance ratios [csp.2],  which are the high values of Î´ [csp.3] enclosed in the shaded boxes  of the figure. The threshold is set to the geometric mean  of Î´. Since Diagonalizations.jl always sorts the explained variance  in descending order, widetildeB is defined as  b_1 b_10 b_9 b_2 b_8 b_3.  On the right, we are interested in the first p vectors of  B associated with positive values of the variance ratios [csp.2],  which are the high values on the left of the minimum of Î´ enclosed  in the shaded box of the figure.  The threshold is set in this case to the first value of Ïƒ [csp.2]  smaller then 1, which roughly coincides with the minimum of Î´  reported in the figure. Assuming in this example they do coincide,  widetildeB would be defined as b_1 ldots b_5.","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"Solution","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"The CSP solution B is given by the generalized eigenvalue-eigenvector decomposition of the pair (C C_1).","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"A numerically preferable solution is the following two-step procedure:","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"get a whitening matrix hspace01cmWhspace01cm such that hspace01cmW^TCW=Ihspace01cm.\ndo hspace01cmtextrmEVD(W^TC_1W)=UÎ›U^T","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"The solution is hspace01cmB=WU.","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"Constructors","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"Three constructors are available (see here below). The constructed LinearFilter object holding the CSP will have fields:","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":".F: matrix widetildeB as defined above. This is just B of [csp.1] if optional keyword argument simple=true is passed to the constructors (see below).","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":".iF: the left-inverse of .F","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":".D: the pp diagonal matrix with the elements of Î› [csp.1] corresponding to the vectors of widetildeB.","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":".eVar: the explained variance for the chosen value of p, given by the p^th value of [csp.5]","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":".ev: the vector holding all n generalized eigenvalues, i.e., the diagonal elements of matrix Î› [csp.1]. Notice that if selMeth=:extremal these elements are sorted differently than in .D and their position do not correspond to the vectors of .F.","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":".arev: the accumulated regularized eigenvalues, defined in [csp.5].","category":"page"},{"location":"csp/","page":"CSP","title":"CSP","text":"note: Nota Bene\n.eVar and .arev are computed on sorted Î´_i values in use case a), see [csp.5], [csp.4] and Fig. 2.","category":"page"},{"location":"csp/#Diagonalizations.csp","page":"CSP","title":"Diagonalizations.csp","text":"(1)\nfunction csp(Câ‚ :: SorH, Câ‚‚ :: SorH;\n             eVar     :: TeVaro = â—‹,\n             eVarC    :: TeVaro = â—‹,\n             eVarMeth :: Function = searchsortedfirst,\n             selMeth  :: Symbol = :extremal,\n             simple   :: Bool = false)\n\n(2)\nfunction csp(Xâ‚ :: Mat, Xâ‚‚ :: Mat;\n             covEst   :: StatsBase.CovarianceEstimator = SCM,\n             dims     :: Into = â—‹,\n             meanXâ‚   :: Tmean = 0,\n             meanXâ‚‚   :: Tmean = 0,\n             wXâ‚      :: Tw = â—‹,\n             wXâ‚‚      :: Tw = â—‹,\n          eVar     :: TeVaro = â—‹,\n          eVarC    :: TeVaro = â—‹,\n          eVarMeth :: Function = searchsortedfirst,\n          selMeth  :: Symbol = :extremal,\n          simple   :: Bool = false)\n\n(3)\nfunction csp(ğ—â‚::VecMat, ğ—â‚‚::VecMat;\n             covEst   :: StatsBase.CovarianceEstimator = SCM,\n             dims     :: Into = â—‹,\n             meanXâ‚   :: Into = 0,\n             meanXâ‚‚   :: Into = 0,\n          eVar     :: TeVaro = â—‹,\n          eVarC    :: TeVaro = â—‹,\n          eVarMeth :: Function = searchsortedfirst,\n          selMeth  :: Symbol = :extremal,\n          simple   :: Bool = false,\n       metric   :: Metric = Euclidean,\n       wâ‚       :: Vector = [],\n       wâ‚‚       :: Vector = [],\n       âœ“w       :: Bool = true,\n       initâ‚    :: SorHo = nothing,\n       initâ‚‚    :: SorHo = nothing,\n       tol      :: Real = 0.,\n       verbose  :: Bool = false)\n\nReturn a LinearFilter object:\n\n(1) Common spatial pattern with covariance matrices C_1 and C_2 of dimension nn as input. The subscript of the covariance matrices refers to the dims used to compute it (see above).\n\neVar, eVarC and eVarMeth are keyword optional arguments for defining the subspace dimension p. Particularly:\n\nBy default, the two-step procedure described above is used to find the  solution. In this case eVarC is used for defining the subspace dimension of  the whitening step. If eVarC=0.0 is passed (not to be confused with  eVarC=0 ), the solution will be find by the generalized  eigenvalue-eigenvector procedure.\neVar is the keyword optional argument for defining the  subspace dimension p using the .arev vector  given by [csp.5].\neVarMeth applies to both eVarC and eVar. The default value is  evarMeth=searchsortedfirst.\n\nif selMeth=:extremal (default) use case a) Separating two classes described above is considered. Any other symbol for selMeth will instruct to consider instead the use case b) Enhance the signal-to-noise ratio.\n\nIf simple is set to true, p is set equal to n and only the fields .F and .iF are written in the constructed object. This option is provided for low-level work when you don't need to define a subspace dimension or you want to define it by your own methods.\n\n(2) Common spatial pattern with data matrices Xâ‚ and Xâ‚‚ as input.\n\nXâ‚ and Xâ‚‚ are real or complex data matrices.\n\ncovEst, dims, meanXâ‚, meanXâ‚‚,  wXâ‚ and wXâ‚‚ are optional keyword arguments to regulate the estimation of the covariance matrices (C_1 C_2) of (Xâ‚, Xâ‚‚). Particularly (See covariance matrix estimations),\n\nmeanXâ‚ is the mean argument for data matrix Xâ‚.\nmeanXâ‚‚ is the mean argument for data matrix Xâ‚‚.\nwXâ‚ is the w argument for estimating a weighted covariance matrix for Xâ‚.\nwXâ‚‚ is the w argument for estimating a weighted covariance matrix for Xâ‚‚.\ncovEst applies to the estimations of both covariance matrices.\n\nOnce the two covariance matrices C_1 and C_2 estimated, method (1) is invoked with optional keyword arguments eVar, eVarC, eVarMeth, selMeth and simple. See method (1) for details.\n\n(3) Common spatial pattern with two vectors of data matrices ğ—â‚ and ğ—â‚‚ as input.\n\nğ—â‚ and ğ—â‚‚ do not need to hold the same number of matrices and the number of samples in the matrices they contain is arbitrary.\n\ncovEst, dims, meanXâ‚ and meanXâ‚‚ are optional keyword arguments to regulate the estimation of the covariance matrices for all matrices in ğ—â‚ and ğ—â‚‚. See method (2) and covariance matrix estimations.\n\nA mean covariance matrix is computed separatedly from the covariance matrices computed from the data matrices in ğ—â‚ and ğ—â‚‚, using optional keywords arguments metric, wâ‚, wâ‚‚, âœ“w, initâ‚, initâ‚‚, tol and verbose. Particularly (see mean covariance matrix estimations),\n\nwâ‚ are the weights for the covariance matrices computed from ğ—â‚,\nwâ‚‚ are the weights for the covariance matrices computed from ğ—â‚‚,\ninitâ‚ is the initialization for the mean of the covariance matrices computed from ğ—â‚,\ninitâ‚‚ is the initialization for the mean of the covariance matrices computed from ğ—â‚‚.\n\nBy default, the arithmetic mean is computed.\n\nSee also: CSTP, PCA, AJD, mAJD.\n\nExamples:\n\nusing Diagonalizations, LinearAlgebra, PosDefManifold, Test\n\n# Method (1) real\nt, n=50, 10\nX1=genDataMatrix(n, t)\nX2=genDataMatrix(n, t)\nCx1=Symmetric((X1*X1')/t)\nCx2=Symmetric((X2*X2')/t)\nC=Cx1+Cx2\ncC=csp(Cx1, Cx2; simple=true)\nDx1=cC.F'*Cx1*cC.F\n@test norm(Dx1-Diagonal(Dx1))+1â‰ˆ1.\nDx2=cC.F'*Cx2*cC.F\n@test norm(Dx2-Diagonal(Dx2))+1â‰ˆ1.\n@test cC.F'*C*cC.Fâ‰ˆI\n@test norm(Dx1-(I-Dx2))+1â‰ˆ1.\n\n# Method (1) complex\nt, n=50, 10\nX1c=genDataMatrix(ComplexF64, n, t)\nX2c=genDataMatrix(ComplexF64, n, t)\nCx1c=Hermitian((X1c*X1c')/t)\nCx2c=Hermitian((X2c*X2c')/t)\nCc=Cx1c+Cx2c\ncCc=csp(Cx1c, Cx2c; simple=true)\nDx1c=cCc.F'*Cx1c*cCc.F\n@test norm(Dx1c-Diagonal(Dx1c))+1. â‰ˆ 1.\nDx2c=cCc.F'*Cx2c*cCc.F\n@test norm(Dx2c-Diagonal(Dx2c))+1. â‰ˆ 1.\n@test cCc.F'*Cc*cCc.Fâ‰ˆI\n@test norm(Dx1c-(I-Dx2c))+1. â‰ˆ 1.\n\n\n# Method (2) real\nc12=csp(X1, X2, simple=true)\nDx1=c12.F'*Cx1*c12.F\n@test norm(Dx1-Diagonal(Dx1))+1â‰ˆ1.\nDx2=c12.F'*Cx2*c12.F\n@test norm(Dx2-Diagonal(Dx2))+1â‰ˆ1.\n@test c12.F'*C*c12.Fâ‰ˆI\n@test norm(Dx1-(I-Dx2))+1â‰ˆ1.\n@test cC==c12\n\n# Method (2) complex\nc12c=csp(X1c, X2c, simple=true)\nDx1c=c12c.F'*Cx1c*c12c.F\n@test norm(Dx1c-Diagonal(Dx1c))+1. â‰ˆ 1.\nDx2c=c12c.F'*Cx2c*c12c.F\n@test norm(Dx2c-Diagonal(Dx2c))+1. â‰ˆ 1.\n@test c12c.F'*Cc*c12c.Fâ‰ˆI\n@test norm(Dx1c-(I-Dx2c))+1. â‰ˆ 1.\n@test cCc==c12c\n\n\n# Method (3) real\n# CSP of the average covariance matrices\nk=10\nXset=[genDataMatrix(n, t) for i=1:k]\nYset=[genDataMatrix(n, t) for i=1:k]\n\nc=csp(Xset, Yset)\n\n# ... selecting subspace dimension allowing an explained variance = 0.9\nc=csp(Xset, Yset; eVar=0.9)\n\n# ... subtracting the mean from the matrices in Xset and Yset\nc=csp(Xset, Yset; meanXâ‚=nothing, meanXâ‚‚=nothing, eVar=0.9)\n\n# csp on the average of the covariance and cross-covariance matrices\n# computed along dims 1\nc=csp(Xset, Yset; dims=1, eVar=0.9)\n\n# name of the filter\nc.name\n\nusing Plots\n# plot regularized accumulated eigenvalues\nplot(c.arev)\n\n\n# plot the original covariance matrices and the transformed counterpart\n# example when argument `selMeth` is `extremal` (default): 2-class separation\n cC=csp(Cx1, Cx2)\n Cx1Max=maximum(abs.(Cx1));\n h1 = heatmap(Cx1, clim=(-Cx1Max, Cx1Max), title=\"Cx1\", yflip=true, c=:bluesreds);\n h2 = heatmap(cC.F'*Cx1*cC.F, clim=(0, 1), title=\"F'*Cx1*F\", yflip=true, c=:amp);\n Cx2Max=maximum(abs.(Cx2));\n h3 = heatmap(Cx2, clim=(-Cx2Max, Cx2Max), title=\"Cx2\", yflip=true, c=:bluesreds);\n h4 = heatmap(cC.F'*Cx2*cC.F, clim=(0, 1), title=\"F'*Cx2*F\", yflip=true, c=:amp);\n CMax=maximum(abs.(C));\n h5 = heatmap(C, clim=(-CMax, CMax), title=\"Cx1+Cx2\", yflip=true, c=:bluesreds);\n h6 = heatmap(cC.F'*C*cC.F, clim=(0, 1), title=\"F'*(Cx1+Cx2)*F\", yflip=true, c=:amp);\n ğŸ“ˆ=plot(h1, h3, h5, h2, h4, h6, size=(800,400))\n# savefig(ğŸ“ˆ, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FigCSP1.png\")\n\n(Image: Figure CSP1)\n\n# example when argument `selMeth` is different from `extremal`: enhance snr\n cC=csp(Cx1, Cx2; selMeth=:enhaceSNR)\n Cx1Max=maximum(abs.(Cx1));\n h1 = heatmap(Cx1, clim=(-Cx1Max, Cx1Max), title=\"Cx1\", yflip=true, c=:bluesreds);\n h2 = heatmap(cC.F'*Cx1*cC.F, clim=(0, 1), title=\"F'*Cx1*F\", yflip=true, c=:amp);\n Cx2Max=maximum(abs.(Cx2));\n h3 = heatmap(Cx2, clim=(-Cx2Max, Cx2Max), title=\"Cx2\", yflip=true, c=:bluesreds);\n h4 = heatmap(cC.F'*Cx2*cC.F, clim=(0, 1), title=\"F'*Cx2*F\", yflip=true, c=:amp);\n CMax=maximum(abs.(C));\n h5 = heatmap(C, clim=(-CMax, CMax), title=\"Cx1+Cx2\", yflip=true, c=:bluesreds);\n h6 = heatmap(cC.F'*C*cC.F, clim=(0, 1), title=\"F'*(Cx1+Cx2)*F\", yflip=true, c=:amp);\n ğŸ“‰=plot(h1, h3, h5, h2, h4, h6, size=(800,400))\n# savefig(ğŸ“‰, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FigCSP2.png\")\n\n\n(Image: Figure CSP2)\n\n# Method (3) complex\n# CSP of the average covariance matrices\nk=10\nXsetc=[genDataMatrix(ComplexF64, n, t) for i=1:k]\nYsetc=[genDataMatrix(ComplexF64, n, t) for i=1:k]\n\ncc=csp(Xsetc, Ysetc)\n\n# ... selecting subspace dimension allowing an explained variance = 0.9\ncc=csp(Xsetc, Ysetc; eVar=0.9)\n\n# ... subtracting the mean from the matrices in Xset and Yset\ncc=csp(Xsetc, Ysetc; meanXâ‚=nothing, meanXâ‚‚=nothing, eVar=0.9)\n\n# csp on the average of the covariance and cross-covariance matrices\n# computed along dims 1\ncc=csp(Xsetc, Ysetc; dims=1, eVar=0.9)\n\n# name of the filter\ncc.name\n\n\n\n\n\n","category":"function"},{"location":"ajd/#AJD","page":"AJD","title":"AJD","text":"","category":"section"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Approximate Joint Diagonalization (AJD) is a diagonalization prodedure generalizing the eigenvalue-eigenvector decomposition to more then two matrices. This corresponds to the situation m=1 (one dataset) and k2 (number of observations). As such, is a very general procedure with a myriad of potential applications. It was first proposed by Flury and Gautschi (1986) in statistics and by Cardoso and Souloumiac(1996) in signal processing ğŸ“. Since, it has become a fundamental tool for solving the blind source separation(BSS) problem.","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Let C_1C_k be a set of nn symmetric or Hermitian matrices. In BSS typically those are covariance matrices, Fourier cross-spectral matrices, lagged covariance matrices or slices of 4th order cumulants, where n is the number of variables.","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"An AJD algorithm seeks a matrix F diagonalizing all matrices in the set as much as possible, according to some diagonalization criterion, that is, we want to achieve","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"F^HC_lFÎ›_l, for all l1k. hspace1cm [ajd.1]","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"In some algorithm, such as OJoB, F is constrained to be orthogonal, in others, like NoJoB only to be non-singular.","category":"page"},{"location":"ajd/#pre-whitening-for-AJD","page":"AJD","title":"pre-whitening for AJD","text":"","category":"section"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Similarly to the two-step procedures encountered in other filters, e.g., for the CCA, for solving the AJD problem often pre-whitening is applied: first a whitening matrix W if found such that","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"W^HBig(frac1ksum_l=1^kC_kBig)W_k=I, hspace1cm [ajd.2]","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"then the following transformed AJD problem if solved for U:","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"U^H(W^HC_lW)UÎ›_l, for all l1k.","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Finally, F is obtained as","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"F=WU. hspace1cm [ajd.3]","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Notice that:","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"matrix W may be taken rectangular so as to engender a dimensionality reduction at this stage. This may improve the convergence behavior of AJD algorithms if the matrices C_1C_k are not well-conditioned.  \nif this two-step procedure is employed, the final solution F is never orthogonal, even if the solving AJD algorithm constrains the solution within the orthogonal group.","category":"page"},{"location":"ajd/#permutation-for-AJD","page":"AJD","title":"permutation for AJD","text":"","category":"section"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Approximate joint diagonalizers are arbitrary up to a scale and permutation. Diagonalizations.jl attempts to solve the permutation ambiguity by reordering the columns of F so as to sort in descending order the diagonal elements of","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"frac1ksum_l=1^kF^HC_kF. hspace1cm [ajd.4]","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"This sorting mimics the sorting of exact diagonalization procedures such as the PCA, of which the AJD is a generalization, however it is meaningful only if the input matrices C_1C_k are positive definite.","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"In analogy with PCA, let","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Î»=Î»_1Î»_n  hspace1cm [ajd.5]","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"be the diagonal elements of [ajd.4] and let","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Ïƒ_TOT=sum_i=1^nÎ»_i be the total variance.","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"We denote widetildeF=f_1 ldots f_p the matrix holding the first pn column vectors of F, where p is the subspace dimension. The explained variance is given by","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Ïƒ_p=fracsum_i=1^pÎ»_iÏƒ_TOT hspace1cm [ajd.6]","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"and the accumulated regularized eigenvalues (arev) by","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Ïƒ_j=sum_i=1^jÏƒ_i, for j=1 ldots n. hspace1cm [ajd.7]","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"For setting the subspace dimension p manually, set the eVar optional keyword argument of the MCA constructors either to an integer or to a real number, this latter establishing p in conjunction with argument eVarMeth using the arev vector (see subspace dimension). By default, eVar is set to 0.999.","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Solution","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"There is no closed-form solution to the AJD problem in general. See Algorithms.","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Constructors","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":"Two constructors are available (see here below). The constructed LinearFilter object holding the AJD will have fields:","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":".F: matrix widetildeF with columns holding the first p eigenvectors in F, or just F if p=n","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":".iF: the left-inverse of .F","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":".D: the leading pp block of Î›, i.e., the elements [ajd.5] associated to .F in diagonal form.","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":".eVar: the explained variance [ajd.6] for the chosen value of p.","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":".ev: the vector Î» [ajd.5].","category":"page"},{"location":"ajd/","page":"AJD","title":"AJD","text":".arev: the accumulated regularized eigenvalues, defined in [ajd.7].","category":"page"},{"location":"ajd/#Diagonalizations.ajd","page":"AJD","title":"Diagonalizations.ajd","text":"(1)\nfunction ajd(ğ‚::â„Vector;\n             trace1    :: Bool   = false,\n             w         :: Union{Tw, Function} = â—‹,\n          algorithm :: Symbol = :NoJoB,\n          preWhite  :: Bool   = false,\n          sort      :: Bool   = true,\n          init      :: Mato   = â—‹,\n          tol       :: Real   = 1e-6,\n          maxiter   :: Int    = _maxiter(algorithm, eltype(ğ‚[1])),\n          verbose   :: Bool   = false,\n          threaded  :: Bool   = true,\n        eVar     :: TeVaro    = _minDim(ğ‚),\n        eVarC    :: TeVaro    = â—‹,\n        eVarMeth :: Function  = searchsortedfirst,\n        simple   :: Bool      = false)\n\n(2)\nfunction ajd(ğ—::VecMat;\n             covEst     :: StatsBase.CovarianceEstimator = SCM,\n             dims       :: Into = â—‹,\n             meanX      :: Into = 0,\n          trace1     :: Bool = false,\n          w          :: Twf  = â—‹,\n       algorithm :: Symbol = :NoJoB,\n       preWhite  :: Bool = false,\n       sort      :: Bool = true,\n       init      :: Mato = â—‹,\n       tol       :: Real = 1e-6,\n       maxiter   :: Int  = _maxiter(algorithm, eltype(ğ—[1])),\n       verbose   :: Bool = false,\n       threaded  :: Bool = true,\n     eVar     :: TeVaro    = _minDim(ğ—),\n     eVarC    :: TeVaro    = â—‹,\n     eVarMeth :: Function  = searchsortedfirst,\n     simple   :: Bool      = false)\n\n\nReturn a LinearFilter object:\n\n(1) Approximate joint diagonalization of the set of k symmetric or Hermitian matrices ğ‚, of type â„Vector using the given solving algorithm (NoJoB by default).\n\nIf trace1 is true, all matrices in the set ğ‚ are normalized so as to have trace equal to 1. It is false by default. This option applies only for solving algorithms that are not invariante by scaling, that is, those based on the least-squares (Frebenius) criterion. See Algorithms.\n\nif w is a StatsBase.AbstractWeights, the weights are applied to the set ğ‚. If w is a Function, the weights are found passing each matrix in the set to such function. An appropriate choice for AJD algorithms minimizing a least-squares criterion, like OJoB and NoJoB, is the nonDiagonality function (Congedo et al.(2008)ğŸ“). By default, no weights are applied.\n\nIf preWhite is true the solution is found by the two-step procedure described here above in section pre-whitening for AJD. By default, it is false. Dimensionality reduction can be obtained at this stage using arguments eVarC and eVarMeth, in the same way they are used to find the subspace dimension p, but using the accumulated regularized eigenvalues of\n\nfrac1ksum_l=1^kC_k.\n\nThe default values are:\n\neVarC is set to 0.999\neVarMeth=searchsortedfirst.\n\nIf sort is true (default), the vectors in .F are permuted as explained here above in permutation for AJD, otherwise they will be in arbitrary order.\n\nRegarding arguments init, tol and maxiter, see Algorithms.\n\nIf verbose is true (false by default), the convergence attained at each iteration will be printed in the REPL.\n\neVar and eVarMeth are used to define a subspace dimension p using the accumulated regularized eigenvalues in Eq. [ajd.7].\n\nThe default values are:\n\neVar is set to the dimension of the matrices in ğ‚\neVarMeth=searchsortedfirst.\n\nNote that passing nothing or a real nummber as eVar (see subspace dimension) is meningful only if sort is set to true (default) and if the input matrices C_1C_k are positive definite.\n\nIf simple is set to true, p is set equal to the dimension of the matrices C_1C_k and only the fields .F and .iF are written in the constructed object. This corresponds to the typical output of AJD algorithms.\n\nif threaded=true (default) and the number of threads Julia is instructed to use (the output of Threads.nthreads()), is higher than 1, AJD algorithms supporting multi-threading run in multi-threaded mode. See Algorithms and these notes on multi-threading.\n\n(2) Approximate joint diagonalization with a set of k data matrices ğ— as input; the covariance matrices of the set are estimated using arguments covEst, dims and meanX (see covariance matrix estimations) and passed to method (1) with the remaining arguments of method (2).\n\nSee also: PCA, CSP, mAJD.\n\nExamples:\n\nusing Diagonalizations, LinearAlgebra, PosDefManifold, Test\n\nconst err=1e-6\n\n# method (1) real\nt, n, k=50, 10, 10\nA=randn(n, n) # mixing matrix in model x=As\nXset = [genDataMatrix(t, n) for i = 1:k]\nXfixed=randn(t, n)./1\nfor i=1:length(Xset) Xset[i]+=Xfixed end\nCset = â„Vector([â„((Xset[s]'*Xset[s])/t) for s=1:k])\naC=ajd(Cset; algorithm=:OJoB, simple=true)\naC2=ajd(Cset; algorithm=:NoJoB, simple=true)\naC3=ajd(Cset; algorithm=:LogLike, simple=true)\naC4=ajd(Cset; algorithm=:LogLikeR, simple=true)\naC5=ajd(Cset; algorithm=:JADE, simple=true)\naC6=ajd(Cset; algorithm=:JADEmax, simple=true)\naC7=ajd(Cset; algorithm=:GAJD, simple=true)\naC8=ajd(Cset; algorithm=:QNLogLike, simple=true)\n\n# a=ajd(Cset; algorithm=:GAJD2, simple=true, verbose=true)\n\n# method (2) real\naX=ajd(Xset; algorithm=:OJoB, simple=true)\naX2=ajd(Xset; algorithm=:NoJoB, simple=true)\naX3=ajd(Xset; algorithm=:LogLike, simple=true)\naX4=ajd(Xset; algorithm=:LogLikeR, simple=true)\naX5=ajd(Xset; algorithm=:JADE, simple=true)\naX6=ajd(Xset; algorithm=:JADEmax, simple=true)\naX7=ajd(Xset; algorithm=:GAJD, simple=true)\naX8=ajd(Xset; algorithm=:QNLogLike, simple=true)\n\n@test aXâ‰ˆaC\n@test aX2â‰ˆaC2\n@test aX3â‰ˆaC3\n@test aX4â‰ˆaC4\n@test aX5â‰ˆaC5\n@test aX6â‰ˆaC6\n@test aX7â‰ˆaC7\n@test aX8â‰ˆaC8\n\n# method (1) complex\nt, n, k=50, 10, 10\nAc=randn(ComplexF64, n, n) # mixing matrix in model x=As\nXcset = [genDataMatrix(ComplexF64, t, n) for i = 1:k]\nXcfixed=randn(ComplexF64, t, n)./1\nfor i=1:length(Xcset) Xcset[i]+=Xcfixed end\nCcset = â„Vector([â„((Xcset[s]'*Xcset[s])/t) for s=1:k])\naCc=ajd(Ccset; algorithm=:OJoB, simple=true)\naCc2=ajd(Ccset; algorithm=:NoJoB, simple=true)\naCc3=ajd(Ccset; algorithm=:LogLike, simple=true)\naCc4=ajd(Ccset; algorithm=:JADE, simple=true)\naCc5=ajd(Ccset; algorithm=:JADEmax, simple=true)\n\n# method (2) complex\naXc=ajd(Xcset; algorithm=:OJoB, simple=true)\naXc2=ajd(Xcset; algorithm=:NoJoB, simple=true)\naXc3=ajd(Xcset; algorithm=:LogLike, simple=true)\naXc4=ajd(Xcset; algorithm=:JADE, simple=true)\naXc5=ajd(Xcset; algorithm=:JADEmax, simple=true)\n\n@test aXcâ‰ˆaCc\n@test aXc2â‰ˆaCc2\n@test aXc3â‰ˆaCc3\n@test aXc4â‰ˆaCc4\n@test aXc5â‰ˆaCc5\n\n# create 20 REAL random commuting matrices\n# they all have the same eigenvectors\nCset2=PosDefManifold.randP(3, 20; eigvalsSNR=Inf, commuting=true)\n# estimate the approximate joint diagonalizer (AJD)\na=ajd(Cset2; algorithm=:OJoB)\n# the orthogonal AJD must be equivalent to the eigenvector matrix\n# of any of the matrices in Cset\n@test norm([spForm(a.F'*eigvecs(C)) for C âˆˆ Cset2])/20 < err\n# do the same for JADE algorithm\na=ajd(Cset2; algorithm=:JADE)\n@test norm([spForm(a.F'*eigvecs(C)) for C âˆˆ Cset2])/20 < err\n# do the same for JADEmax algorithm\na=ajd(Cset2; algorithm=:JADEmax)\n@test norm([spForm(a.F'*eigvecs(C)) for C âˆˆ Cset2])/20 < err\n\n\n# generate positive definite matrices with model A*D_Îº*D, where\n# A is the mixing matrix and D_Îº, for all Îº=1:k, are diagonal matrices.\n# The estimated AJD matrix must be the inverse of A\n# and all transformed matrices bust be diagonal\nn, k=3, 10\nDest=PosDefManifold.randÎ›(eigvalsSNR=10, n, k)\n# make the problem identifiable\nfor i=1:k Dest[k][1, 1]*=i/(k/2) end\nfor i=1:k Dest[k][3, 3]/=i/(k/2) end\nA=randn(n, n) # non-singular mixing matrix\nCset3=Vector{Hermitian}([Hermitian(A*D*A') for D âˆˆ Dest])\na=ajd(Cset3; algorithm=:NoJoB, eVarC=n)\n@test spForm(a.F'*A)<âˆšerr\n@test mean(nonD(a.F'*Cset3[i]*a.F) for i=1:k)<err\na=ajd(Cset3; algorithm=:LogLike, eVarC=n)\n@test spForm(a.F'*A)<âˆšerr\n@test mean(nonD(a.F'*Cset3[i]*a.F) for i=1:k)<err\na=ajd(Cset3; algorithm=:LogLikeR, eVarC=n)\n@test spForm(a.F'*A)<âˆšerr\n@test mean(nonD(a.F'*Cset3[i]*a.F) for i=1:k)<err\na=ajd(Cset3; algorithm=:GAJD, eVarC=n)\n@test spForm(a.F'*A)<âˆšerr\n@test mean(nonD(a.F'*Cset3[i]*a.F) for i=1:k)<err\na=ajd(Cset3; algorithm=:QNLogLike, eVarC=n)\n@test spForm(a.F'*A)<âˆšerr\n@test mean(nonD(a.F'*Cset3[i]*a.F) for i=1:k)<err\n\n\n\n# Do the same thing for orthogonal diagonalizers:\n# now A will be orthogonal\nO=randU(n) # orthogonal mixing matrix\nCset4=Vector{Hermitian}([Hermitian(O*D*O') for D âˆˆ Dest])\na=ajd(Cset4; algorithm=:OJoB, eVarC=n)\n@test spForm(a.F'*O)<âˆšerr\n@test mean(nonD(a.F'*Cset4[i]*a.F) for i=1:k)<err\na=ajd(Cset4; algorithm=:JADE, eVarC=n)\n@test spForm(a.F'*O)<âˆšerr\n@test mean(nonD(a.F'*Cset4[i]*a.F) for i=1:k)<âˆšerr\na=ajd(Cset4; algorithm=:JADEmax, eVarC=n)\n@test spForm(a.F'*O)<âˆšerr\n@test mean(nonD(a.F'*Cset4[i]*a.F) for i=1:k)<âˆšerr\n\n\n# repeat the test adding noise; now the model is no more exactly identifiable\nfor k=1:length(Cset3) Cset3[k]+=randP(n)/1000 end\na=ajd(Cset3; algorithm=:NoJoB, eVarC=n)\n@test spForm(a.F'*A)<err^(1/6)\n@test mean(nonD(a.F'*Cset3[i]*a.F) for i=1:k)<âˆšerr\na=ajd(Cset3; algorithm=:LogLike, eVarC=n)\n@test spForm(a.F'*A)<err^(1/6)\n@test mean(nonD(a.F'*Cset3[i]*a.F) for i=1:k)<âˆšerr\na=ajd(Cset3; algorithm=:LogLikeR, eVarC=n)\n@test spForm(a.F'*A)<err^(1/6)\n@test mean(nonD(a.F'*Cset3[i]*a.F) for i=1:k)<âˆšerr\na=ajd(Cset3; algorithm=:GAJD, eVarC=n)\n@test spForm(a.F'*A)<err^(1/6)\n@test mean(nonD(a.F'*Cset3[i]*a.F) for i=1:k)<âˆšerr\na=ajd(Cset3; algorithm=:QNLogLike, eVarC=n)\n@test spForm(a.F'*A)<err^(1/6)\n@test mean(nonD(a.F'*Cset3[i]*a.F) for i=1:k)<âˆšerr\n\n\n# the same thing for orthogonal diagonalizers\nfor k=1:length(Cset4) Cset4[k]+=randP(n)/1000 end\na=ajd(Cset4; algorithm=:OJoB, eVarC=n)\n@test spForm(a.F'*O)<err^(1/6)\n@test mean(nonD(a.F'*Cset4[i]*a.F) for i=1:k)<âˆšerr\na=ajd(Cset4; algorithm=:JADE, eVarC=n)\n@test spForm(a.F'*O)<err^(1/6)\n@test mean(nonD(a.F'*Cset4[i]*a.F) for i=1:k)<âˆšerr\na=ajd(Cset4; algorithm=:JADEmax, eVarC=n)\n@test spForm(a.F'*O)<err^(1/6)\n@test mean(nonD(a.F'*Cset4[i]*a.F) for i=1:k)<âˆšerr\n\n\n# create 20 COMPLEX random commuting matrices\n# they all have the same eigenvectors\nCcset2=PosDefManifold.randP(ComplexF64, 3, 20; eigvalsSNR=Inf, commuting=true)\n# estimate the approximate joint diagonalizer (AJD)\nac=ajd(Ccset2; algorithm=:OJoB)\n# he AJD must be equivalent to the eigenvector matrix of any of the matrices in Cset\n# just a sanity check as rounding errors appears for complex data\n@test norm([spForm(ac.F'*eigvecs(C)) for C âˆˆ Ccset2])/20<âˆšerr\n# do the same for JADE algorithm\nac=ajd(Ccset2; algorithm=:JADE)\n@test norm([spForm(ac.F'*eigvecs(C)) for C âˆˆ Ccset2])/20<âˆšerr\n# do the same for JADEmax algorithm\nac=ajd(Ccset2; algorithm=:JADEmax)\n@test norm([spForm(ac.F'*eigvecs(C)) for C âˆˆ Ccset2])/3<âˆšerr\n\n\n# the same thing using the NoJoB and LogLike algorithms. Require less precision\n# as the NoJoB solution is not constrained in the orthogonal group\nac=ajd(Ccset2; algorithm=:NoJoB)\n@test norm([spForm(ac.F'*eigvecs(C)) for C âˆˆ Ccset2])/20<âˆšerr\nac=ajd(Ccset2; algorithm=:LogLike)\n@test norm([spForm(ac.F'*eigvecs(C)) for C âˆˆ Ccset2])/20<âˆšerr\n\n# REAL data:\n# normalize the trace of input matrices,\n# give them weights according to the `nonDiagonality` function\n# apply pre-whitening and limit the explained variance both\n# at the pre-whitening level and at the level of final vector selection\nCset=PosDefManifold.randP(20, 80; eigvalsSNR=10, SNR=10, commuting=false)\n\na=ajd(Cset; algorithm=:OJoB, trace1=true, w=nonD, preWhite=true, eVarC=4, eVar=0.99)\na=ajd(Cset; algorithm=:NoJoB, trace1=true, w=nonD, preWhite=true, eVarC=4, eVar=0.99)\na=ajd(Cset; algorithm=:LogLike, w=nonD, preWhite=true, eVarC=4, eVar=0.99)\na=ajd(Cset; algorithm=:LogLikeR, w=nonD, preWhite=true, eVarC=4, eVar=0.99)\na=ajd(Cset; algorithm=:JADE, w=nonD, preWhite=true, eVarC=4, eVar=0.99)\na=ajd(Cset; algorithm=:JADEmax, w=nonD, preWhite=true, eVarC=4, eVar=0.99)\na=ajd(Cset; algorithm=:GAJD, w=nonD, preWhite=true, eVarC=4, eVar=0.99)\na=ajd(Cset; algorithm=:QNLogLike, w=nonD, preWhite=true, eVarC=4, eVar=0.99)\n\n\n# AJD for plots below\na=ajd(Cset; algorithm=:QNLogLike, verbose=true, preWhite=true)\n\nusing Plots\n# plot the original covariance matrices\n# and their transformed counterpart\nCMax=maximum(maximum(abs.(C)) for C âˆˆ Cset);\n h1 = heatmap(Cset[1], clim=(-CMax, CMax), title=\"C1\", yflip=true, c=:bluesreds);\n h2 = heatmap(Cset[2], clim=(-CMax, CMax), title=\"C2\", yflip=true, c=:bluesreds);\n h3 = heatmap(Cset[3], clim=(-CMax, CMax), title=\"C3\", yflip=true, c=:bluesreds);\n h4 = heatmap(Cset[4], clim=(-CMax, CMax), title=\"C4\", yflip=true, c=:bluesreds);\n ğŸ“ˆ=plot(h1, h2, h3, h4, size=(700,400))\n# savefig(ğŸ“ˆ, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FigAJD1.png\")\n\nDset=[a.F'*C*a.F for C âˆˆ Cset];\n DMax=maximum(maximum(abs.(D)) for D âˆˆ Dset);\n h5 = heatmap(Dset[1], clim=(-DMax, DMax), title=\"F'*C1*F\", yflip=true, c=:bluesreds);\n h6 = heatmap(Dset[2], clim=(-DMax, DMax), title=\"F'*C2*F\", yflip=true, c=:bluesreds);\n h7 = heatmap(Dset[3], clim=(-DMax, DMax), title=\"F'*C3*F\", yflip=true, c=:bluesreds);\n h8 = heatmap(Dset[4], clim=(-DMax, DMax), title=\"F'*C4*F\", yflip=true, c=:bluesreds);\n ğŸ“‰=plot(h5, h6, h7, h8, size=(700,400))\n# savefig(ğŸ“‰, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FigAJD2.png\")\n\n\n(Image: Figure AJD1)\n\n(Image: Figure AJD2)\n\n\n# COMPLEX data:\n# normalize the trace of input matrices,\n# give them weights according to the `nonDiagonality` function\n# apply pre-whitening and limit the explained variance both\n# at the pre-whitening level and at the level of final vector selection\nCcset=PosDefManifold.randP(3, 20; eigvalsSNR=10, SNR=2, commuting=false)\n\nac=ajd(Ccset; trace1=true, w=nonD, preWhite=true,\n       algorithm=:OJoB, eVarC=8, eVar=0.99)\nac=ajd(Ccset; eVarC=8, eVar=0.99)\nac=ajd(Ccset; algorithm=:LogLike, eVarC=8, eVar=0.99)\nac=ajd(Ccset; algorithm=:JADE, eVarC=8, eVar=0.99)\nac=ajd(Ccset; algorithm=:JADEmax, eVarC=8, eVar=0.99)\n\n\n\n\n\n\n","category":"function"},{"location":"pca/#PCA","page":"PCA","title":"PCA","text":"","category":"section"},{"location":"pca/","page":"PCA","title":"PCA","text":"Principal Component Analysis (PCA) is obtained by eigenvalue-eigenvector decomposition. It was first conceived by Karl Pearson (1901)ğŸ“ as a way to fit straight lines to a multidimensional cloud of points. It corresponds to the situation m=1 (one dataset) and k=1 (one observation).","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"Let X be a nt data matrix, where n is the number of variables and t the number of samples and let C be its nn covariance matrix. Being C a positive semi-definite matrix, its eigenvector matrix U diagonalizes C by rotation, as","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"U^HCU=Î›. hspace1cm [pca.1]","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"The eigenvalues in the diagonal matrix Î› are all non-negative. They are all real and positive if C is positive definite, which is assumed in the remaining of this exposition. The linear transformation U^HX yields uncorrelated data with variance of the n^th component equal to the corresponding eigenvalue Î»_n, that is,","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"frac1TU^HXX^HU=Î›. hspace1cm [pca.2]","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"In Diagonalizations.jl the diagonal elements of diagonalized matrices are always arranged by descending order, such as","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"Î»_1ldotsÎ»_n. hspace1cm [pca.3]","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"Then, because of the extremal properties of eigenvalues (Congedo, 2013, p. 66; Schott, 1997, p. 104-128)ğŸ“, the first component (row) of U^HX holds the linear combination of X with maximal variance, the second the linear combination with maximal residual variance and so on, subject to constraint U^HU=UU^H=I.","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"Let Ïƒ_TOT=sum_i=1^nÎ»_i=tr(C) be the total variance and let widetildeU=u_1 ldots u_p be the matrix holding the first pn eigenvectors, where p is the subspace dimension, then","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"Ïƒ_p=fracsum_i=1^pÎ»_iÏƒ_TOT=fractr(widetildeU^HCwidetildeU)tr(C)  hspace1cm [pca.4]","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"is named the explained variance and","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"Îµ_p=Ïƒ_TOT-Ïƒ_p hspace1cm [pca.5]","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"is named the representation error. These quantities are expressed in proportions, that is, it holds Ïƒ_p+Îµ_p=1.","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"The accumulated regularized eigenvalues (arev) are defined as","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"Ïƒ_j=sum_i=1^jÏƒ_i, for j=1 ldots n, hspace1cm [pca.6]","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"where Ïƒ_i is given by Eq. [pca.4].","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"For setting the subspace dimension p manually, set the eVar optional keyword argument of the PCA constructors either to an integer or to a real number, this latter establishing p in conjunction with argument eVarMeth using the arev vector (see subspace dimension). By default, eVar is set to 0.999.","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"Solution","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"The PCA solution is given by the eigenvalue-eigenvector decoposition of C","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"textrmEVD(C)=UÎ›U^H.","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"It is worth mentioning that","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"widetildeUwidetildeÎ›widetildeU^H,","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"where widetildeÎ› is the leading pp block of Î›, is the best approximant to C with rank p in the least-squares sense (Good, 1969)ğŸ“.","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"Constructors","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":"Three constructors are available (see here below). The constructed LinearFilter object holding the PCA will have fields:","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":".F: matrix widetildeU with orthonormal columns holding the first p eigenvectors in U, or just U if p=n","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":".iF: the (conjugate) transpose of .F","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":".D: the leading pp block of Î›, i.e., the eigenvalues associated to .F in diagonal form.","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":".eVar: the explained variance [pca.4] for the chosen value of p.","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":".ev: the vector diag(Î›) holding all n eigenvalues.","category":"page"},{"location":"pca/","page":"PCA","title":"PCA","text":".arev: the accumulated regularized eigenvalues in [pca.6].","category":"page"},{"location":"pca/#Diagonalizations.pca","page":"PCA","title":"Diagonalizations.pca","text":"(1)\nfunction pca(C :: SorH;\n             eVar     :: TeVaro = nothing,\n             eVarMeth :: Function = searchsortedfirst,\n             simple   :: Bool = false)\n\n(2)\nfunction pca(X::Mat;\n             covEst   :: StatsBase.CovarianceEstimator = SCM,\n             dims     :: Into = â—‹,\n             meanX    :: Tmean = 0,\n             wX       :: Tw = â—‹,\n          eVar     :: TeVaro = â—‹,\n          eVarMeth :: Function = searchsortedfirst,\n          simple   :: Bool = false)\n\n(3)\nfunction pca(ğ—::VecMat;\n             covEst   :: StatsBase.CovarianceEstimator = SCM,\n             dims     :: Into = â—‹,\n             meanX    :: Into = 0,\n          eVar     :: TeVaro = â—‹,\n          eVarMeth :: Function = searchsortedfirst,\n          simple   :: Bool = false,\n       metric   :: Metric = Euclidean,\n       w        :: Vector = [],\n       âœ“w       :: Bool = true,\n       init     :: SorHo = nothing,\n       tol      :: Real = 0.,\n       verbose  :: Bool = false)\n\nReturn a LinearFilter object:\n\n(1) Principal component analysis with real or complex covariance matrix C as input.\n\nC must be flagged as Symmetric, if real, or Hermitian, if either real or complex, see data input.\n\neVar and evarMeth are keyword optional arguments for defining the subspace dimension p using the .arev vector given by Eq. [pca.6]. The default values are:\n\neVar=0.999\nevarMeth=searchsortedfirst\n\nIf simple is set to true, p is set equal to n and only the fields .F and .iF are written in the constructed object. This option is provided for low-level work when you don't need to define a subspace dimension or you want to define it by your own methods.\n\n(2) Principal component analysis with a real or complex data matrix X as input.\n\nCovEst, dims, meanX, wX are optional keyword arguments to regulate the estimation of the covariance matrix of X. See covariance matrix estimations.\n\nOnce the covariance matrix estimated, method (1) is invoked with optional keyword arguments eVar, eVarMeth and simple.\n\n(3) Principal component analysis with a vector of real or complex data matrices ğ— as input.\n\nCovEst, dims and meanX are optional keyword arguments to regulate the estimation of the covariance matrices for all data matrices in ğ—. See covariance matrix estimations.\n\nA mean of these covariance matrices is computed using optional keywords arguments metric, w, âœ“w, init, tol and verbose. See mean covariance matrix estimations. By default, the arithmetic mean is computed.\n\nOnce the mean covariance matrix estimated, method (1) is invoked with optional keyword arguments eVar, eVarMeth and simple.\n\nSee also: Whitening, CSP, MCA, AJD.\n\nExamples:\n\nusing Diagonalizations, LinearAlgebra, PosDefManifold, Test\n\n# Method (1) real\nn, t=10, 100\nX=genDataMatrix(n, t)\nC=(X*X')/t\npC=pca(Hermitian(C); simple=true)\n# or, shortly\npC=pca(â„(C); simple=true)\n\n# Method (1) complex\nXc=genDataMatrix(ComplexF64, n, t)\nCc=(Xc*Xc')/t\npCc=pca(Hermitian(Cc); simple=true)\n\n\n# Method (2) real\npX=pca(X; simple=true)\n@test Câ‰ˆpC.F*pC.D*pC.iF\n@test Câ‰ˆpC.F*pC.D*pC.F'\n@test pXâ‰ˆpC\n\n# Method (2) complex\npXc=pca(Xc; simple=true)\n@test Ccâ‰ˆpCc.F*pCc.D*pCc.iF\n@test Ccâ‰ˆpCc.F*pCc.D*pCc.F'\n@test pXcâ‰ˆpCc\n\n\n# Method (3) real\nk=10\nXset=[genDataMatrix(n, t) for i=1:k]\n\n# pca on the average covariance matrix\np=pca(Xset)\n\n# ... selecting subspace dimension allowing an explained variance = 0.5\np=pca(Xset; eVar=0.5)\n\n# ... averaging the covariance matrices using the logEuclidean metric\np=pca(Xset; metric=logEuclidean, eVar=0.5)\n\n# ... giving weights `w` to the covariance matrices\np=pca(Xset; metric=logEuclidean, w=abs2.(randn(k)), eVar=0.5)\n\n# ... subtracting the mean\np=pca(Xset; meanX=nothing, metric=logEuclidean, w=abs2.(randn(k)), eVar=0.5)\n\n# pca on the average of the covariance matrices computed along dims 1\np=pca(Xset; dims=1)\n\n# explained variance\np.eVar\n\n# name of the filter\np.name\n\nusing Plots\n# plot regularized accumulated eigenvalues\nplot(p.arev)\n\n# plot the original covariance matrix and the rotated covariance matrix\n Cmax=maximum(abs.(C));\n h1 = heatmap(C, clim=(-Cmax, Cmax), yflip=true, c=:bluesreds, title=\"C\");\n D=pC.F'*C*pC.F;\n Dmax=maximum(abs.(D));\n h2 = heatmap(D, clim=(0, Dmax), yflip=true, c=:amp, title=\"F'*C*F\");\n ğŸ“ˆ=plot(h1, h2, size=(700, 300))\n# savefig(ğŸ“ˆ, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FigPCA.png\")\n\n(Image: Figure PCA)\n\n# Method (3) complex\nk=10\nXcset=[genDataMatrix(ComplexF64, n, t) for i=1:k]\n\n# pca on the average covariance matrix\npc=pca(Xcset)\n\n# ... selecting subspace dimension allowing an explained variance = 0.5\npc=pca(Xcset; eVar=0.5)\n\n# ... averaging the covariance matrices using the logEuclidean metric\npc=pca(Xcset; metric=logEuclidean, eVar=0.5)\n\n# ... giving weights `w` to the covariance matrices\npc=pca(Xcset; metric=logEuclidean, w=abs2.(randn(k)), eVar=0.5)\n\n# ... subtracting the mean\npc=pca(Xcset; meanX=nothing, metric=logEuclidean, w=abs2.(randn(k)), eVar=0.5)\n\n# pca on the average of the covariance matrices computed along dims 1\npc=pca(Xcset; dims=1)\n\n# explained variance\npc.eVar\n\n# name of the filter\npc.name\n\n\n\n\n\n","category":"function"},{"location":"whitening/#Whitening","page":"Whitening","title":"Whitening","text":"","category":"section"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"Whitening (also named Sphering) is a standardized version of PCA. Possibly it is the most used diagonalization procedure among all. Like PCA, it corresponds to the situation m=1 (one dataset) and k=1 (one observation).","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"Let X be a nt data matrix, where n is the number of variables and t the number of samples and let C be its nn covariance matrix. Being C a positive semi-definite matrix, its eigenvector matrix U diagonalizes C by rotation, as","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"U^HCU=Î›. hspace1cm [whitening.1]","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"The eigenvalues in the diagonal matrix Î› are all non-negative. They are all real and positive if C is positive definite, which is assumed in the remaining of this exposition. The linear transformation Î›^-12U^HX yields uncorrelated data with unit variance at all n components, that is,","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"frac1TÎ›^-12U^HXX^HUÎ›^-12=I. hspace1cm [whitening.2]","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"Whitened data remains whitened after whatever further rotation. That is, for any orthogonal matrix V, it holds","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"V^Hbig(frac1TÎ›^-12U^HXX^HUÎ›^-12big)V=I. hspace1cm [whitening.3]","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"Hence there exist an infinite number of possible whitening matrices with general form V^HÎ›^-12U^H. Because of this property whitening plays a fundamental role as a first step in many two-steps diagonalization procedures (e.g., for the CSP, CSTP and CCA). Particularly important among the infinite family of whitening matrices is the only symmetric one (or Hermitian, if complex), which is the inverse of the principal square root of C and is found repeatedly in computations on the manifold of positive definite matrices (see for example intro to Riemannian geometry).","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"For setting the subspace dimension p manually, set the eVar optional keyword argument of the Whitening constructors either to an integer or to a real number, this latter establishing p in conjunction with argument eVarMeth using the arev vector, which is defined as for the PCA filter, see Eq. [pca.6] therein and subspace dimension for details. By default, eVar is set to 0.999.","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"Solution","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"As for PCA, the solution is given by the eigenvalue-eigenvector decoposition of C","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"textrmEVD(C)=UÎ›U^H.","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"Constructors","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":"Three constructors are available (see here below), which use exactly the same syntax as for PCA. The constructed LinearFilter object holding the Whitening will have fields:","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":".F: matrix widetildeUwidetildeÎ›^-12 with scaled orthonormal columns, where widetildeÎ› is the leading pp block of Î› and  widetildeU=u_1 ldots u_p holds the first p eigenvectors in U. If p=n, .F is just UÎ›^-12.","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":".iF: widetildeÎ›^12widetildeU^H, the left-inverse of .F.","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":".D: widetildeÎ›, i.e., the eigenvalues associated to .F in diagonal form.","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":".eVar: the explained variance for the chosen value of p. This is the same as for the PCA, see Eq. [pca.4] therein.","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":".ev: the vector diag(Î›) holding all n eigenvalues.","category":"page"},{"location":"whitening/","page":"Whitening","title":"Whitening","text":".arev: the accumulated regularized eigenvalues. This is the same as for the PCA, see Eq. [pca.6] therein.","category":"page"},{"location":"whitening/#Diagonalizations.whitening","page":"Whitening","title":"Diagonalizations.whitening","text":"(1)\nfunction whitening(C :: SorH;\n                   eVar     :: TeVaro=â—‹,\n                   eVarMeth :: Function=searchsortedfirst,\n                   simple   :: Bool=false)\n\n(2)\nfunction whitening(X::Mat;\n                   covEst   :: StatsBase.CovarianceEstimator = SCM,\n                   dims     :: Into = â—‹,\n                   meanX    :: Tmean = 0,\n                   wX       :: Tw = â—‹,\n                eVar     :: TeVaro = â—‹,\n                eVarMeth :: Function = searchsortedfirst,\n                simple   :: Bool = false)\n\n(3)\nfunction whitening(ğ—::VecMat;\n                   covEst   :: StatsBase.CovarianceEstimator = SCM,\n                   dims     :: Into = â—‹,\n                   meanX    :: Into = 0,\n                eVar     :: TeVaro = â—‹,\n                eVarMeth :: Function = searchsortedfirst,\n                simple   :: Bool = false,\n             metric   :: Metric = Euclidean,\n             w        :: Vector = [],\n             âœ“w       :: Bool = true,\n             init     :: SorHo = nothing,\n             tol      :: Real = 0.,\n             verbose  :: Bool = false)\n\n\nReturn a LinearFilter object:\n\n(1) Whitening with real or complex covariance matrix C as input.\n\nC must be flagged as Symmetric, if real or Hermitian, if real or complex, see data input.\n\neVar and evarMeth are keyword optional arguments for defining the subspace dimension p using the .arev vector given by Eq. [pca.6], see PCA. The default values are:\n\neVar=0.999\nevarMeth=searchsortedfirst\n\nIf simple is set to true, p is set equal to n and only the fields .F and .iF are written in the constructed object. This option is provided for low-level work when you don't need to define a subspace dimension or you want to define it by your own methods.\n\n(2) Whitening with a real or complex data matrix X as input.\n\nCovEst, dims, meanX, wX are optional keyword arguments to regulate the estimation of the covariance matrix of X. See covariance matrix estimations.\n\nOnce the covariance matrix estimated, method (1) is invoked with optional keyword arguments eVar, eVarMeth and simple.\n\n(3) Whitening with a vector of real or complex data matrices ğ— as input.\n\nCovEst, dims and meanX are optional keyword arguments to regulate the estimation of the covariance matrices for all data matrices in ğ—. See covariance matrix estimations.\n\nA mean of these covariance matrices is computed using optional keywords arguments metric, w, âœ“w, init, tol and verbose. See mean covariance matrix estimations. By default, the arithmetic mean is computed.\n\nOnce the mean covariance matrix estimated, method (1) is invoked with optional keyword arguments eVar, eVarMeth and simple.\n\nSee also: PCA, CCA.\n\nExamples:\n\nusing Diagonalizations, LinearAlgebra, PosDefManifold, Test\n\n# Method (1) real\nn, t=10, 100\nX=genDataMatrix(n, t)\nC=(X*X')/t\nwC=whitening(Hermitian(C); simple=true)\n# or, shortly\nwC=whitening(â„(C); simple=true)\n\n# Method (1) complex\nXc=genDataMatrix(ComplexF64, n, t)\nCc=(Xc*Xc')/t\nwCc=whitening(Hermitian(Cc); simple=true)\n\n\n# Method (2) real\nwX=whitening(X; simple=true)\n@test wC.F'*C*wC.Fâ‰ˆI\n@test wX.F'*C*wX.Fâ‰ˆI\n@test wXâ‰ˆwC\n\n# Method (2) complex\nwXc=whitening(Xc; simple=true)\n@test wCc.F'*Cc*wCc.Fâ‰ˆI\n@test wXc.F'*Cc*wXc.Fâ‰ˆI\n@test wXcâ‰ˆwCc\n\n\n# Method (3) real\nk=10\nXset=[genDataMatrix(n, t) for i=1:k]\n\n# whitening on the average covariance matrix\nw=whitening(Xset)\n\n# ... selecting subspace dimension allowing an explained variance = 0.5\nw=whitening(Xset; eVar=0.5)\n\n# ... averaging the covariance matrices using the logEuclidean metric\nw=whitening(Xset; metric=logEuclidean, eVar=0.5)\n\n# ... giving weights `w` to the covariance matrices\nw=whitening(Xset; metric=logEuclidean, w=abs2.(randn(k)), eVar=0.5)\n\n# ... subtracting the mean\nw=whitening(Xset; meanX=nothing, metric=logEuclidean, w=abs2.(randn(k)), eVar=0.5)\n\n# whitening on the average of the covariance matrices computed along dims 1\nw=whitening(Xset; dims=1)\n\n# explained variance\nw.eVar\n\n# name of the filter\nw.name\n\nusing Plots\n# plot regularized accumulated eigenvalues\nplot(w.arev)\n\n# plot the original covariance matrix and the whitened covariance matrix\n Cmax=maximum(abs.(C));\n h1 = heatmap(C, clim=(-Cmax, Cmax), yflip=true, c=:bluesreds, title=\"C\");\n D=wC.F'*C*wC.F;\n h2 = heatmap(D, clim=(0, 1), yflip=true, c=:amp, title=\"F'*C*F\");\n ğŸ“ˆ=plot(h1, h2, size=(700, 300))\n# savefig(ğŸ“ˆ, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FigWhitening.png\")\n\n(Image: Figure Whitening)\n\n\n# Method (3) complex\nk=10\nXcset=[genDataMatrix(ComplexF64, n, t) for i=1:k]\n\n# whitening on the average covariance matrix\nwc=whitening(Xcset)\n\n# ... selecting subspace dimension allowing an explained variance = 0.5\nwc=whitening(Xcset; eVar=0.5)\n\n# ... averaging the covariance matrices using the logEuclidean metric\nwc=whitening(Xcset; metric=logEuclidean, eVar=0.5)\n\n# ... giving weights `w` to the covariance matrices\nwc=whitening(Xset; metric=logEuclidean, w=abs2.(randn(k)), eVar=0.5)\n\n# ... subtracting the mean\nwc=whitening(Xcset; meanX=nothing, metric=logEuclidean, w=abs2.(randn(k)), eVar=0.5)\n\n# whitening on the average of the covariance matrices computed along dims 1\nwc=whitening(Xcset; dims=1)\n\n# explained variance\nwc.eVar\n\n# name of the filter\nwc.name\n\n\n\n\n\n","category":"function"},{"location":"gcca/#gCCA","page":"gCCA","title":"gCCA","text":"","category":"section"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"Generalized Canonical Correlation Analysis (gCCA) is a mutiple approximate joint diagonalization prodedure generalizing the canonical correlation analysis (CCA) to the situation m2 (number of datasets), as for CCA with k=1 (one observation). As the CCA is an MCA carried out on whitened data, so the gCCA is a gMCA carried out on whitened data.","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"Let X_1X_m be a set of m data matrices of dimension nt, where n is the number of variables and t the number of samples, both common to all datasets. From these data matrices let us estimate","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"C_ij=frac1tX_iX_j^H, for all ij1m, hspace1cm [gcca.1]","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"i.e., all covariance (i=j) and cross-covariance (ij) matrices.","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"The gMCA seeks m matrices F_1F_m diagonalizing as much as possible all products","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"F_i^H C_ij F_j, for all ij1m. hspace1cm [gcca.2]","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"under costraint","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"F_i^H C_ii F_i=I, for all i1m. hspace1cm [gcca.3]","category":"page"},{"location":"gcca/#permutation-for-gCCA","page":"gCCA","title":"permutation for gCCA","text":"","category":"section"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"Given constraint [gcca.3], the scaling of approximate diagonalizers F_1F_m are fixed, however there is still a sign and permutation ambiguity (see scale and permutation). Diagonalizations.jl attempts to solve them by finding signed permutation matrices for F_1F_m so as to make all diagonal elements of [gcca.2] positive and sorted in descending order.","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"Let","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"Î»=Î»_1Î»_n  hspace1cm [gcca.4]","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"be the diagonal elements of","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"frac1m^2-msum_ij=1^m(F_i^H C_ij F_j) hspace1cm [gcca.5]","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"and Ïƒ_TOT=sum_i=1^nÎ»_i be the total correlation.","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"We denote widetildeF_i=f_i1 ldots f_ip the matrix holding the first pn column vectors of F_i, where p is the subspace dimension. The explained variance is given by","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"Ïƒ_p=fracsum_i=1^pÎ»_iÏƒ_TOT hspace1cm [gcca.6]","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"and the accumulated regularized eigenvalues (arev) by","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"Ïƒ_j=sum_i=1^jÏƒ_i, for j=1 ldots n, hspace1cm [gcca.7]","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"where Ïƒ_i is given by Eq. [gcca.6].","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"For setting the subspace dimension p manually, set the eVar optional keyword argument of the gCCA constructors either to an integer or to a real number, this latter establishing p in conjunction with argument eVarMeth using the arev vector (see subspace dimension). By default, eVar is set to 0.999.","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"Solution","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"There is no closed-form solution to the AJD problem in general. See Algorithms.","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"Note that solving algorithms constraining the solution to the general linear group, like NoJoB, do not suit gCCA as they do not ensure constraint [gcca.2].","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"Constructors","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":"One constructor is available (see here below). The constructed LinearFilter object holding the gCCA will have fields:","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":".F: vector of matrices widetildeF_1widetildeF_m with columns holding the first p eigenvectors in F_1F_m, or just F_1F_m if p=n","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":".iF: the vector of the left-inverses of the matrices in .F","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":".D: the leading pp block of Î›, i.e., the elements [gcca.4] associated to the matrices in .F in diagonal form.","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":".eVar: the explained variance [gcca.6] for the chosen value of p.","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":".ev: the vector Î» [gcca.4].","category":"page"},{"location":"gcca/","page":"gCCA","title":"gCCA","text":".arev: the accumulated regularized eigenvalues, defined by [gcca.7]","category":"page"},{"location":"gcca/#Diagonalizations.gcca","page":"gCCA","title":"Diagonalizations.gcca","text":"function gcca(ğ—::VecMat;\n              covEst     :: StatsBase.CovarianceEstimator = SCM,\n              dims       :: Into    = â—‹,\n              meanX      :: Into    = 0,\n          algorithm :: Symbol    = :OJoB,\n          sort      :: Bool      = true,\n          init      :: VecMato   = â—‹,\n          tol       :: Real      = 0.,\n          maxiter   :: Int       = _maxiter(algorithm, eltype(ğ—[1])),\n          verbose   :: Bool      = false,\n          threaded  :: Bool      = true,\n        eVar     :: TeVaro   = _minDim(ğ—),\n        eVarMeth :: Function = searchsortedfirst,\n        simple   :: Bool     = false)\n\n\nReturn a LinearFilter object.\n\nGeneralized Canonical Correlation Analysis of the set of m data matrices ğ— using the given solving algorithm (OJoB by default).\n\nIf sort is true (default), the column vectors of the matrices F_1F_m are signed and permuted as explained here above in permutation for gCCA, otherwise they will have arbitrary sign and will be in arbitrary order.\n\nRegarding arguments init, tol and maxiter, see Algorithms.\n\nIf verbose is true (false by default), the convergence attained at each iteration will be printed in the REPL.\n\neVar and eVarMeth are used to define a subspace dimension p using the accumulated regularized eigenvalues in Eq. [gcca.7]\n\nThe default values are:\n\neVar is set to the minimum dimension of the matrices in ğ—\neVarMeth=searchsortedfirst\n\nIf simple is set to true, p is set equal to the dimension of the covariance matrices that are computed on the matrices in ğ—, which depends on the choice of dims, and only the fields .F and .iF are written in the constructed object. This corresponds to the typical output of approximate diagonalization algorithms.\n\nif threaded=true (default) and the number of threads Julia is instructed to use (the output of Threads.nthreads()), is higher than 1, solving algorithms supporting multi-threading run in multi-threaded mode. See Algorithms and these notes on multi-threading.\n\nSee also: MCA, gCCA, mAJD.\n\nExamples:\n\nusing Diagonalizations, LinearAlgebra, PosDefManifold, Test\n\n\n####  Create data for testing the case k=1, m>1\n# `t` is the number of samples,\n# `m` is the number of datasets,\n# `n` is the number of variables,\n# `noise` must be smaller than 1.0. The smaller the noise,\n#  the more data are correlated.\nfunction getData(t, m, n, noise)\n    # create m identical data matrices and rotate them by different\n    # random orthogonal matrices V_1,...,V_m\n    ğ•=[randU(n) for i=1:m] # random orthogonal matrices\n    X=randn(n, t)  # data common to all subjects\n    # each subject has this common part plus a random part\n    ğ—=[ğ•[i]'*((1-noise)*X + noise*randn(n, t)) for i=1:m]\n    return ğ—\nend\n\nfunction getData(::Type{Complex{T}}, t, m, n, noise) where {T<:AbstractFloat}\n    # create m identical data matrices and rotate them by different\n    # random orthogonal matrices V_1,...,V_m\n    ğ•=[randU(ComplexF64, n) for i=1:m] # random orthogonal matrices\n    X=randn(ComplexF64, n, t)  # data common to all subjects\n    # each subject has this common part plus a random part\n    ğ—=[ğ•[i]'*((1-noise)*X + noise*randn(ComplexF64, n, t)) for i=1:m]\n    return ğ—\nend\n\n# REAL data: check that for the case m=2 gCCA gives the same result as CCA\nt, m, n, noise = 20, 2, 6, 0.1\nXset=getData(t, m, n, noise)\nCx=(Xset[1]*Xset[1]')/t\nCy=(Xset[2]*Xset[2]')/t\nCxy=(Xset[1]*Xset[2]')/t\n\ngc=gcca(Xset; simple=true)\n\nc=cca(Hermitian(Cx), Hermitian(Cy), Cxy; simple=true)\n\n@test (c.F[1]'*Cxy*c.F[2]) â‰ˆ (gc.F[1]'*Cxy*gc.F[2])\n@test gc.F[1]'*Cx*gc.F[1]â‰ˆI\n@test gc.F[2]'*Cy*gc.F[2]â‰ˆI\nD=gc.F[1]'*Cxy*gc.F[2]\n@test norm(D-Diagonal(D))+1â‰ˆ1.\n\n\n# COMPLEX data: check that for the case m=2 gCCA gives the same result as CCA\nt, m, n, noise = 20, 2, 6, 0.1\nXcset=getData(ComplexF64, t, m, n, noise)\nCcx=(Xcset[1]*Xcset[1]')/t\nCcy=(Xcset[2]*Xcset[2]')/t\nCcxy=(Xcset[1]*Xcset[2]')/t\n\ngcc=gcca(Xcset; simple=true)\ncc=cca(Hermitian(Ccx), Hermitian(Ccy), Ccxy; simple=true)\n\n# for complex data just do a sanity check as the order of vectors\n# is arbitrary. The following two tests currently fail\n# @test spForm(cc.F[1]'gcc.F[1])<0.001\n# @test spForm(cc.F[2]'gcc.F[2])<0.001\n\n@test gcc.F[1]'*Ccx*gcc.F[1]â‰ˆI\n@test gcc.F[2]'*Ccy*gcc.F[2]â‰ˆI\n# sanity check only as there is noise in the complex case\nD=gcc.F[1]'*Ccxy*gcc.F[2]\n@test norm(D-Diagonal(D))/(n^2-n)<0.001\n\n\n# REAL data: m>2 case\nt, m, n, noise = 20, 4, 6, 0.1\nXset=getData(t, m, n, noise)\n\n# ... selecting subspace dimension allowing an explained variance = 0.9\ngc=gcca(Xset, eVar=0.9)\n\n# name of the filter\ngc.name\n\nğ’=Array{Matrix}(undef, 1, m, m)\nfor i=1:m, j=1:m ğ’[1, i, j]=(Xset[i]*Xset[j]')/t end\n\nusing Plots\n# plot regularized accumulated eigenvalues\nplot(gc.arev)\n\n\n# plot the original cross-covariance matrices and the rotated\n# cross-covariance matrices\n\n# Get all products ğ”[i]' * ğ’[l, i, j] * ğ”[j]\nfunction _rotate_crossCov(ğ”, ğ’, m, k)\n    ğ’®=Array{Matrix}(undef, k, m, m)\n    @inbounds for l=1:k, i=1:m, j=1:m ğ’®[l, i, j]=ğ”[i]'*ğ’[l, i, j]*ğ”[j] end\n    return ğ’®\nend\n\n\n# Put all cross-covariances in a single matrix of dimension m*n x m*n for visualization\nfunction ğ’2Mat(ğ’::AbstractArray, m, k)\n    n=size(ğ’[1, 1, 1], 1)\n    C=Matrix{Float64}(undef, m*n, m*n)\n    for i=1:m, j=1:m, x=1:n, y=1:n C[i*n-n+x, j*n-n+y]=ğ’[k, i, j][x, y] end\n    return C\nend\n\n C=ğ’2Mat(ğ’, m, 1)\n Cmax=maximum(abs.(C));\n h1 = heatmap(C, clim=(-Cmax, Cmax), yflip=true, c=:bluesreds, title=\"all cross-covariances\")\n ğ’®=_rotate_crossCov(gc.F, ğ’, m, 1)\n S=ğ’2Mat(ğ’®, m, 1)\n h2 = heatmap(S, clim=(0, 1), yflip=true, c=:amp, title=\"all rotated cross-covariances\")\n ğŸ“ˆ=plot(h1, h2, size=(700,300))\n# savefig(ğŸ“ˆ, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FiggCCA.png\")\n\n\n(Image: Figure gCCA)\n\nIn the figure here above, the rotated cross-covariance matrices have the expected   strip-diagonal form, that is, each block F_i^Tfrac1t(X_iX_j^T)F_j,   for ij1m, is approximately diagonal. Each block is 66 because   setting eVar=0.9 has not reduced the original dimension.   The solution is similar to the gMCA, but here the diagonal   of the rotated block matrix is the identity.\n\n# COMPLEX data: m>2 case\nt, m, n, noise = 20, 4, 6, 0.1\nXcset=getData(ComplexF64, t, m, n, noise)\n\n# ... selecting subspace dimension allowing an explained variance = 0.9\ngcc=gcca(Xcset, eVar=0.9)\n\n\n\n\n\n","category":"function"},{"location":"tools/#Tools","page":"Tools","title":"Tools","text":"","category":"section"},{"location":"tools/","page":"Tools","title":"Tools","text":"Unit tools.jl contains miscellaneous functions and internal functions.","category":"page"},{"location":"tools/#Diagonalizations.eig","page":"Tools","title":"Diagonalizations.eig","text":"function eig(A)\n\nfunction eig(A, B)\n\nCall Julia function eigen and return its output sorted by descending order of eigenvalues.\n\n\n\n\n\n","category":"function"},{"location":"tools/#Diagonalizations.nonDiagonality","page":"Tools","title":"Diagonalizations.nonDiagonality","text":"function nonDiagonality(C::Union{Matrix, Diagonal, SorH})\n\nMeasure of deviancy from diagonality of nn square matrix C, defined as (Congedo et al., 2008)ğŸ“.\n\nfracsum_ijc_ij^2(n-1)sum_ic_ii^2\n\nIt is equal to 0 if C is diagonal, equal to 1 if C is perfectly uniform.\n\nExamples:\n\nusing Diagonalizations\nC=ones(10, 10)                   # uniform matrix\nnd=nonDiagonality(C)             # must be 1\nD=Diagonal(abs.(randn(10, 10)))  # diagonal matrix\nnd=nonDiagonality(D)             # must be 0\n\n\n\n\n\n","category":"function"},{"location":"tools/#Diagonalizations.spForm","page":"Tools","title":"Diagonalizations.spForm","text":"function spForm(P::Union{Mat, Real, Complex})\n\nMeasure of deviancy from scaled permutation form of nn square matrix P, defined as\n\nfrac12(n-1)bigg(sum_row1-beta(row)+sum_col1-beta(col)bigg),\n\nwhere for each row and column of P, Î² is the maximum of the absolute values divided by the sum of the absolute values.\n\nThis index is equal to 0 if in each row and column P has only one non-zero element, that is, if P is a scaled permutation matrix. The larger the index, the farther away P is from this form.\n\nThis measure and several existing variants are well-known in the blind source separation / independent component analysis community, where it is used to compare approximate joint diagonalization algorithms on simulated data. In fact, if A is the inverse of the approximate joint diagonalizer that is used to generate the data and B the approximate joint diagonalizer estimated by an algorithm, P=BA must be as close as possible to a scaled permutation matrix (see scale and permutation).\n\nReturn 0.0 (zero) if P is a real of complex number.\n\nExamples:\n\nusing Diagonalizations, PosDefManifold\n# create 20 random commuting matrices\n# they all have the same eigenvectors\nCset=randP(3, 20; eigvalsSNR=Inf, commuting=true)\n# estimate the approximate joint diagonalizer (ajd)\na=ajd(Cset)\n# the ajd must be equivalent to the eigenvector matrix of\n# any of the matrices in Cset\nspForm(a.F'*eigvecs(Cset[1]))+1.0â‰ˆ1.0 ? println(\" â­ \") : println(\" â›” \")\n\n\n\n\n\n","category":"function"},{"location":"tools/#Diagonalizations.genDataMatrix","page":"Tools","title":"Diagonalizations.genDataMatrix","text":"(1)\nfunction genDataMatrix(t::Int, n::Int, A=nothing)\n\n(2)\nfunction genDataMatrix(::Type{Complex{T}},\n                       t::Int, n::Int, A=â—‹) where {T<:AbstractFloat}\n\n(1) Generate a tn random data matrix as XA, where X is a tn matrix with entries randomly drawn from a Gaussian distribution and A a nn symmetric matrix, which, if not provided as argument A, will be generated with entries randomly drawn from a uniform distribution âˆˆ[-1, 1].\n\n(2) as (1), but X is generated randomly from a complex Gaussian distribution and Ais Hermitian (complex) which, if not provided as argument A, will be generated with entries randomly drawn from a uniform distribution âˆˆ[-1-i1, 1+i1].\n\nExamples:\n\nA=genDataMatrix(100, 20) # (1), real\nA=genDataMatrix(ComplexF64, 100, 20) # (2), complex\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The advanced diagonalization methods implemented in Diagonalizations.jl, namely, gMCA, gCCA, AJD and mAJD are solved by iterative algorithms, here also called solvers.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Solvers differ from one another in several fashions:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"the restriction they impose on the solution(s) (orthogonal/unitary or non-singular)\nwhether they support complex data or only real data\nwhether they take as input only positive-definite (PD) matrices or all symmetric/Hermitian matrices\nthe diagonalization criterion they minimize\nthe optimization method they employ\nthe diagonalization methods they support\ntheir initialization\nwhether they support multi-threading or not","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"To date nine solvers are implemented:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Orthogonal joint blind source separation (OJOB: Congedo et al., 2012ğŸ“)\nNon-orthogonal joint blind source separation (NoJOB: Congedo et al., 2012ğŸ“)\nLog-likelihood (LogLike: Pham, 2001ğŸ“)\nLog-likelihood Real (LogLikeR: Pham, 2001ğŸ“)\nQuasi-Newton Log-likelihood (QNLogLike: Ablin et al., 2019ğŸ“)\nJoint Approximate Diagonalization of Eigenmatrices (JADE: Cardoso and Souloumiac, 1996ğŸ“)\nJoint Approximate Diagonalization of Eigenmatrices max (JADEmax: Usevich, Li and Comon, 2020ğŸ“)\nGauss Approximate Joint Diagonalization (GAJD, unpublished, from the author)\nGauss Log-Likelihood (GLogLike, unpublished, from the author, still experimental)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Their main characteristics and domain of application are summarized in the following table:","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Algorithm Solution Complex data Only PD Criterion Multi-threaded Supported Methods\nOJoB Orthogonal/Unitary yes no least-squares yes gMCA, gCCA, AJD, mAJD\nNoJoB non-singular yes no least-squares yes gMCA, AJD, mAJD\nLogLike non-singular yes yes log-likelihood no AJD\nLogLikeR non-singular no yes log-likelihood no AJD\nQNLogLike non-singular no yes log-likelihood no AJD\nJADE Orthogonal/Unitary yes no least-squares no AJD\nJADEmax Orthogonal/Unitary yes no least-squares no AJD\nGAJD non-singular no no least-squares no AJD\nGLogLike non-singular no yes log-likelihood no AJD","category":"page"},{"location":"algorithms/#using-solvers","page":"Algorithms","title":"using solvers","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A solver is to find m matrices, where m depends on the sought diagonalization method (see Overview). All algorithms by default are initialized by m identity matrices, with the exception of OJoB, for which, following Congedo et al. (2011)ğŸ“, the m^th solution matrix is initalized by the eigenvectors of","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"frac1mksum_j=1^msum_l=1^k C_mjlC_mjl^H,","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"where C_mjl is the cross-covariance matrix between dataset m (fixed) and dataset j for observation l. When m=1, for instance in the case of AJD, those are just the k covariance matrices to be jointly diagonalized.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A matrix (if m=1) or a vector of matrices (if m1) can be passed with the init argument in order to initialize the solver differently. Note that for LogLike and LogLikeR algorithms the actual approximate joint diagonalizer will be given by init*B, where B is the output of the algorithms.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"tol is the tolerance for convergence of the solver. By default it is set to the square root of Base.eps of the nearest real type of the data input. If the solver encounters difficulties in converging, try setting tol in between 1e-6 and 1e-3.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Argument maxiter is the maximum number of iterations allowed to the solver. For all algorithms, by default maxiter is set to 1000 for real data and to 3000 for complex data.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"If the maximum number of iteration is attained, a warning is printed in the REPL. In this case, try increasing maxiter and/or tol.","category":"page"},{"location":"algorithms/#Multi-threading","page":"Algorithms","title":"Multi-threading","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The OJoB, NoJOB and QNLogLike algorithms supports multi-threading. The methods' constructors feature the threaded optional keyword argument, which is true by default. For OJoB and NoJoB, the algorithms run in multi-threaded mode paralellising several comptations over the dimension of the input matrices n, if threaded is true, 2nx and x1, where x is the number of threads Julia is instructed to use. For QNLogLike, the algorithms run in multi-threaded mode paralellising several computations over the number of matrices k, if threaded is true, 2kx and x1.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Before running these methods you may want to set the number of threades Julia is instructed to use to the number of logical CPUs of your machine. Besides being optionally multi-threaded, OJoB and NoJoB algorithms heavely use BLAS. Before using these methods you may want to set BLAS.set_num_threads(Sys.CPU_THREADS) to the number of logical CPUs of your machine. If you are using any of the package written by the author, all this is done automatically. See these notes.","category":"page"},{"location":"cstp/#CSTP","page":"CSTP","title":"CSTP","text":"","category":"section"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"In the CSP one assumes that the multiplicity of data have a common structure along one dimension of the input data matrices. For example, in electroencephalography (EEG) a data matrix X, which is comprised of n variables corresponding to the spatial locations for the electrodes on the scalp and t temporal samples, this is the spatial dimension. The assumption holds because the same brain source engenders a fixed spatial pattern on the scalp, whereas, in general, the temporal pattern is arbitrary.","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"The common spatio-temporal pattern (CSTP) extends the CSP to situations when the multiplicity of data have a common structure along both dimensions. In EEG, for example, this is the case of event-related potentials (ERPs). The assumption holds because, again, the same brain source engenders a fixed spatial pattern on the scalp and furthermore ERPs have a quasi-fixed temporal pattern. As the CSP, the CSTP corresponds to the situation m=1 (one dataset) and k=2 (two observation).","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"Given a set of k data matrices X_1 ldots X_k of dimension nt, with mean barX=frac1ksum_i=1^kX_i, the goal of the CSTP is to find two matrices B_(1) and B_(2) verifying","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"left  beginarrayrlB_(1)^TC_(2)B_(1)=IB_(2)^TC_(1)B_(2)=IB_(1)^TbarXB_(2)=Î› endarray right, hspace1cm [cstp.1],","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"where","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"left  beginarrayrlC_(1)=sum_i=1^kfrac1t(X_i^TX_i)C_(2)=sum_i=1^kfrac1n(X_iX_i^T) endarray right, hspace1cm [cstp.2]","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"are the mean covariance matrices along the first and second dimension of the X_i matrices and Î› is a diagonal matrix.","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"In words, the CSTP maximizes the ratio of the variance of the transformed barX over the transformed mean covariance matrices C_(1) and C_(2). The CSTP can threfore be used to enhance the signal-to-noise ratio of data matrices mean estimation. For doing so, we retain the filters widetildeB_(1)=b_(1)1 ldots b_(1)p and widetildeB_(2)=b_(2)1 ldots b_(2)p holding the first p vectors of B_(1) and B_(2) corresponding to the highest values of the variance ratio Î›.","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"For the CSTP we define the total variance ratio as","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"Î»_TOT=sum_i=1^nÎ»_i,","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"where the Î»_i are the diagonal elements of Î› [cstp.1] and we define the explained variance for dimension p such as","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"Ïƒ_p=fracsum_i=1^pÎ»_iÎ»_TOT. hspace1cm [cstp.3]","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"The .arev field of the CSTP filter is defined as the vector of accumulated variance ratios","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"Ïƒ_1ldotsÏƒ_n, hspace1cm [cstp.4]","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"where Ïƒ_j is defined in [cstp.3].","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"For setting the subspace dimension p manually, set the eVar optional keyword argument of the CSTP constructors either to an integer or to a real number, this latter establishing p in conjunction with argument eVarMeth using the arev vector (see subspace dimension). By default, eVar is set to 0.999.","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"Solution","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"The CSTP solutions B_(1) and B_(2) can be found by a two-step procedure (Congedo et al., 2016)ğŸ“:","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"get two whitening matrices hspace01cmW_(1)hspace01cm and hspace01cmW_(2)hspace01cm such that left  beginarrayrlW_(1)^TC_(1)W_(1)=IW_(2)^TC_(2)W_(2)=I endarray right\ndo hspace01cmtextrmSVD(W_(2)^TbarXW_(1))=UÎ›V^T","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"The solutions are hspace01cmB_(1)=W_(2)Uhspace01cm and hspace01cmB_(2)=W_(1)V.","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"Constructors","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":"Two constructors are available (see here below). The constructed LinearFilter object holding the CSTP will have fields:","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":".F[1]: matrix widetildeB_(1)=b_(1)1 ldots b_(1)p. This is the whole matrix B_(1) if p=n.","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":".F[2]: matrix widetildeB_(2)=b_(2)1 ldots b_(2)p. This is the whole matrix B_(2) if p=n","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":".iF[1]: the left-inverse of .F[1]","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":".iF[2]: the left-inverse of .F[2]","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":".D: the leading pp block of Î› in [cstp.1].","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":".eVar: the explained variance for the chosen value of p, given by the p^th value of [cstp.4].","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":".ev: the vector diag(Î›) holding all n diagonal elements of matrix Î› in [cstp.1].","category":"page"},{"location":"cstp/","page":"CSTP","title":"CSTP","text":".arev: the accumulated regularized eigenvalues, defined in [cstp.4].","category":"page"},{"location":"cstp/#Diagonalizations.cstp","page":"CSTP","title":"Diagonalizations.cstp","text":"(1)\nfunction cstp( X :: Mat, Câ‚â‚â‚ :: SorH, Câ‚â‚‚â‚ :: SorH;\n               eVar     :: TeVaro = â—‹,\n               eVarC    :: TeVaro = â—‹,\n               eVarMeth :: Function = searchsortedfirst,\n               simple   :: Bool = false)\n\n(2)\nfunction cstp( ğ—::VecMat;\n               covEst   :: StatsBase.CovarianceEstimator = SCM,\n               meanXdâ‚  :: Into = 0,\n               meanXdâ‚‚  :: Into = 0,\n            eVar     :: TeVaro = â—‹,\n            eVarC    :: TeVaro = â—‹,\n            eVarMeth :: Function = searchsortedfirst,\n            simple   :: Bool = false,\n         metric   :: Metric = Euclidean,\n         w        :: Vector = [],\n         âœ“w       :: Bool = true,\n         init1    :: SorHo = nothing,\n         init2    :: SorHo = nothing,\n         tol      :: Real = 0.,\n         verbose  :: Bool = false)\n\nReturn a LinearFilter object:\n\n(1) Common spatio-temporal pattern with nm mean data matrix X, mm covariance matrices Câ‚â‚â‚ and nn covariance matrix Câ‚â‚‚â‚ as input.\n\neVar, eVarC and eVarMeth are keyword optional arguments for defining the subspace dimension p. Particularly:\n\neVarC is used for defining the subspace dimension of  the whitening step. The default is 0.999.\neVar is the keyword optional argument for defining the  subspace dimension p using the .arev vector  given by [cstp.5]. The default is given in [cstp.6] here above.\neVarMeth applies to both eVarC and eVar. The default value is  evarMeth=searchsortedfirst.\n\nIf simple is set to true, p is set equal to n and only the fields .F and .iF are written in the constructed object. This option is provided for low-level work when you don't need to define a subspace dimension or you want to define it by your own methods.\n\n(2) Common spatio-temporal pattern with a set of k data matrices ğ— as input.\n\nThe k matrices in ğ— are real or complex data matrices. They must all have the same size.\n\ncovEst, meanXdâ‚ and meanXdâ‚‚ are optional keyword arguments to regulate the estimation of the covariance matrices of the data matrices in ğ—, to be used to compute the mean covariance matrices in [cstp.2] here above. See covariance matrix estimations. meanXdâ‚ and meanXdâ‚‚ are the means along dimension 1 and 2, respectively, of the data matrices in ğ—.\n\nThe mean covariance matrices C_(1) and C_(1) in [cstp.2] are computed using optional keywords arguments metric, w, âœ“w, init1, init2, tol and verbose, which allow to compute non-Euclidean means. Particularly (see mean covariance matrix estimations),\n\ninit1 is the initialization for C_(1),\ninit2 is the initialization for C_(2).\n\nBy default, the arithmetic means [cstp.2] are computed.\n\nOnce the two covariance matrices C_(1) and C_(2) estimated, method (1) is invoked with optional keyword arguments eVar, eVarC, eVarMeth and simple. See method (1) for details.\n\nExamples:\n\nusing Diagonalizations, LinearAlgebra, PosDefManifold, Test\n\n# Method (1) real\nt, n, k=10, 20, 10\nXset = [genDataMatrix(t, n) for i = 1:k]\nXfixed=randn(t, n)./1\nfor i=1:length(Xset) Xset[i]+=Xfixed end\nC1=Hermitian( mean((X'*X)/t for XâˆˆXset) )\nC2=Hermitian( mean((X*X')/n for XâˆˆXset) )\nXbar=mean(Xset)\nc=cstp(Xbar, C1, C2; simple=true)\n@test c.F[1]'*C2*c.F[1]â‰ˆI\n@test c.F[2]'*C1*c.F[2]â‰ˆI\nZ=c.F[1]'*Xbar*c.F[2]\nn=minimum(size(Z))\n@test norm(Z[1:n, 1:n]-Diagonal(Z[1:n, 1:n]))+1. â‰ˆ 1.\ncX=cstp(Xset; simple=true)\n@test c==cX\n\n# Method (1) complex\nt, n, k=10, 20, 10\nXcset = [genDataMatrix(ComplexF64, t, n) for i = 1:k]\nXcfixed=randn(ComplexF64, t, n)./1\nfor i=1:length(Xcset) Xcset[i]+=Xcfixed end\nC1c=Hermitian( mean((Xc'*Xc)/t for XcâˆˆXcset) )\nC2c=Hermitian( mean((Xc*Xc')/n for XcâˆˆXcset) )\nXcbar=mean(Xcset)\ncc=cstp(Xcbar, C1c, C2c; simple=true)\n@test cc.F[1]'*C2c*cc.F[1]â‰ˆI\n@test cc.F[2]'*C1c*cc.F[2]â‰ˆI\nZc=cc.F[1]'*Xcbar*cc.F[2]\nn=minimum(size(Zc))\n@test norm(Zc[1:n, 1:n]-Diagonal(Zc[1:n, 1:n]))+1. â‰ˆ 1.\ncXc=cstp(Xcset; simple=true)\n@test cc==cXc\n\n# Method (2) real\nc=cstp(Xset)\n\n# ... selecting subspace dimension allowing an explained variance = 0.9\nc=cstp(Xset; eVar=0.9)\n\n# ... giving weights `w` to the covariance matrices\nc=cstp(Xset; w=abs2.(randn(k)), eVar=0.9)\n\n# ... subtracting the means\nc=cstp(Xset; meanXdâ‚=nothing, meanXdâ‚‚=nothing, w=abs2.(randn(k)), eVar=0.9)\n\n# explained variance\nc.eVar\n\n# name of the filter\nc.name\n\nusing Plots\n# plot the original covariance matrices and the transformed counterpart\nc=cstp(Xset)\n\nC1Max=maximum(abs.(C1));\n h1 = heatmap(C1, clim=(-C1Max, C1Max), title=\"C1\", yflip=true, c=:bluesreds);\n D1=c.F[1]'*C2*c.F[1];\n D1Max=maximum(abs.(D1));\n h2 = heatmap(D1, clim=(0, D1Max), title=\"F[1]'*C2*F[1]\", yflip=true, c=:amp);\n C2Max=maximum(abs.(C2));\n h3 = heatmap(C2, clim=(-C2Max, C2Max), title=\"C2\", yflip=true, c=:bluesreds);\n D2=c.F[2]'*C1*c.F[2];\n D2Max=maximum(abs.(D2));\n h4 = heatmap(D2, clim=(0, D2Max), title=\"F[2]'*C1*F[2]\", yflip=true, c=:amp);\n\nXbarMax=maximum(abs.(Xbar));\n h5 = heatmap(Xbar, clim=(-XbarMax, XbarMax), title=\"Xbar\", yflip=true, c=:bluesreds);\n DX=c.F[1]'*Xbar*c.F[2];\n DXMax=maximum(abs.(DX));\n h6 = heatmap(DX, clim=(0, DXMax), title=\"F[1]'*Xbar*F[2]\", yflip=true, c=:amp);\n ğŸ“ˆ=plot(h1, h3, h5, h2, h4, h6, size=(800,400))\n# savefig(ğŸ“ˆ, homedir()*\"\\Documents\\Code\\julia\\Diagonalizations\\docs\\src\\assets\\FigCSTP.png\")\n\n\n(Image: Figure CSTP)\n\n\n# Method (2) complex\ncc=cstp(Xcset)\n\n# ... selecting subspace dimension allowing an explained variance = 0.9\ncc=cstp(Xcset; eVar=0.9)\n\n# ... giving weights `w` to the covariance matrices\ncc=cstp(Xcset; w=abs2.(randn(k)), eVar=0.9)\n\n# ... subtracting the mean\ncc=cstp(Xcset; meanXdâ‚=nothing, meanXdâ‚‚=nothing,\n        w=abs2.(randn(k)), eVar=0.9)\n\n# explained variance\nc.eVar\n\n# name of the filter\nc.name\n\n\n\n\n\n\n","category":"function"},{"location":"#Documentation","page":"Documentation","title":"Documentation","text":"","category":"section"},{"location":"#Requirements-and-Installation","page":"Documentation","title":"Requirements & Installation","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Julia: version â‰¥ 1.7","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Packages: see the dependencies of the main module.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"To install the package execute the following command in Julia's REPL:","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"]add CovarianceEstimation PosDefManifold Diagonalizations","category":"page"},{"location":"#Quick-Start","page":"Documentation","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Copy and execute the examples .jl units provided in the examples folder or walk throught the documentation and run the examples you will find there.","category":"page"},{"location":"#Disclaimer","page":"Documentation","title":"Disclaimer","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"This version is throughoutly tested for both the case of real and complex data input. Independent reviewers for both the code and the documentation are welcome.","category":"page"},{"location":"#TroubleShoothing","page":"Documentation","title":"TroubleShoothing","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Problem Solution\nNo problem has been reported so far ","category":"page"},{"location":"#About-the-Authors","page":"Documentation","title":"About the Authors","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Marco Congedo, corresponding author, is a Research Director of CNRS (Centre National de la Recherche Scientifique), working at UGA (University of Grenoble Alpes). Contact: first name dot last name at gmail dot com","category":"page"},{"location":"#Overview","page":"Documentation","title":"Overview","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Diagonalizations.jl implements the following multivariate linear filters:","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Table 1","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Acronym Full Name Datasets ( m ) Observations ( k )\nPCA Principal Component Analysis 1 1\nWhitening Whitening (Sphering) 1 1\nMCA Maximum Covariance Analysis 2 1\nCCA Canonical Correlation Analysis 2 1\ngMCA generalized MCA >1 1\ngCCA generalized CCA >1 1\nCSP Common Spatial Pattern 1 2\nCSTP Common Spatio-Temporal Pattern 1 >1\nAJD Approximate Joint Diagonalization 1 >1\nmAJD multiple AJD >1 >1","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"All these filters are obtained by diagonalization procedures and can be all understood as particular solutions to the same optimization problem, which corresponds to the mAJD problem. (Congedo, 2013 ğŸ“). They can be classified in several way. For instance,","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"the MCA can be seen as a bilinear version and the gMCA as a multi-linear version of the PCA.\nthe CCA can be seen as a bilinear version and the gMCA as a multi-linear version of Whitening.\nthe AJD can be seen as a generalization of the PCA and of the CSP.\nthe mAJD can be seen as a generalization of all other filters.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Also,","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"PCA and Whitening are based on the eigenvalues decomposition\nMCA and PCA are based on the singular values decomposition\nCSP and CSTP are based on the generalized eigenvalue decomposition.\ngMCA, gCCA, AJD and mAJD are solved by iterative algorithms generalizing the above well-known procedures.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"With respect to the number of datasets ( m ) and observations ( k )   the implemented filters fill the entries of the following table:","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"(Image: Figure 1)","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Figure 1 Taxonomy of several diagonalization procedures and   signal procssing methods that make use of them, depending on the number of observations and data sets involved. Legend: see acronyms.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Future versions shall concentrate on implementing other iterative algorithms for solving the generalized problems. Also, this package will be used as the base of packages implementing practical signal processing methods such as blind source separation.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"As compared to MultivariateStats.jl this package supports :","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"the dims keyword\nshrinkage covariance matrix estimations throught package CovarianceEstimation\naverage covariance estimations with metrics for the manifold of positive definite matrices using the PosDefManifold package\nautomatic procedures to set the subspace dimension\ndiagonalization procedures for the case m2 and k2 (see Fig. 1).","category":"page"},{"location":"#Code-units","page":"Documentation","title":"Code units","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Diagonalizations.jl includes twelve code units (.jl files):","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Unit Description\nDiagonalizations.jl Main module, declaring constants, types and structs\npca.jl Unit implementing the PCA and the Whitening\ncca.jl Unit implementing the MCA and the CCA\ngcca.jl Unit implementing the gMCA and the gCCA\ncsp.jl Unit implementing the CSP and CSTP\najd.jl Unit implementing the AJD and the mAJD\ntools.jl Unit containing general tools and internal functions\nGajd.jl Unit implementing the GAJD and GLogLike iterative algorithms (in the 'optim' folder)\nJade.jl Unit implementing the JADE and JADEmax iterative algorithms (in the 'optim' folder)\nJoB.jl Unit implementing the OJoB and NoJoB iterative algorithms (in the 'optim' folder)\nLogLike.jl Unit implementing the Log-Likelihood iterative algorithms (in the 'optim' folder)\nQnLogLike.jl Unit implementing the quasi-Newton Log-Likelihood iterative algorithm (in the 'optim' folder)","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"Furthermore, all examples given at the end of the documentation of the filter constructors are collected as .jl units in the 'examples' folder.","category":"page"},{"location":"#","page":"Documentation","title":"ğŸ“","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"References","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"P. Ablin, J.F. Cardoso, A. Gramfort (2019) Beyond Pham's algorithm for joint diagonalization, Proc. ESANN Conference.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"A. Belouchrani, K. Abed-Meraim, J. Cardoso, E. Moulines (1997), A blind source separation technique using second order statistics, IEEE Transactions on Signal Processing, 45(2), 434â€“444.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"J.-F. Cardoso (1989) Source separation using higher order moments. In Proc. IEEE ICASSP, 4, 2109-2112","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"J.-F. Cardoso and A. Souloumiac (1993) Blind beamforming for non gaussian signals. In IEE Proceedings-F, 140, 362â€“370.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"J.-F. Cardoso, A. Souloumiac (1996) Jacobi angles for simultaneous diagonalization. SIAM Journal on Matrix Analysis and Applications, 17(1), 161â€“164.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"M. Congedo (2013) EEG Source Analysis Thesis submitted in partial fulfillment of the requirements to obtain the H.D.R degree presented at doctoral school EDISCE, University of Grenoble Alpes, France.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"M. Congedo M, A. Barachant A, R. Bhatia (2017) Riemannian Geometry for EEG-based Brain-Computer Interfaces; a Primer and a Review, Brain-Computer Interfaces, 4(3), 155-174.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"M. Congedo, C. Gouy-Pailler, C. Jutten (2008) On the blind source separation of human electroencephalogram by approximate joint diagonalization of second order statistics. Clinical Neurophysiology 119, 2677-2686.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"M. Congedo, L. Korczowski, A. Delorme, F. Lopes Da Silva (2016) Spatio-Temporal Common Pattern; a Companion Method for ERP Analysis in the Time Domain, Journal of Neuroscience Methods, 267, 74-88.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"M. Congedo, R. Phlypo, J. Chatel-Goldman (2012) Orthogonal and Non-Orthogonal Joint Blind Source Separation in the Least-Squares Sense, 20th European Signal Processing Conference (EUSIPCO), 1885-9.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"M. Congedo, R. Phlypo, D.-T. Pham (2011) Approximate joint singular value decomposition of an asymmetric rectangular matrix set, IEEE Trans Signal Process, 59(1), 415-424.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"B.N. Flury, W. Gautschi (1986) An algorithm for the simultaneous orthogonal transformation of several positive de\ffinite symmetric matrices to nearly orthogonal form. Siam J. of Sci. Stat. Comp., 7(1), 169-184.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"K. Fukunaga (1990) Introduction to Statistical Pattern Recognition (2nd Ed.), Academic Press, London.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"I.J. Good (1969) Some Applications of the Singular Decomposition of a Matrix. Technometrics, 11(4), 823-831.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"H. Hotelling (1936) Relations between two sets of variates. Biometrika, 27, 321-77.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"O. Ledoit, M. Wolf (2004) A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices, Journal of Multivariate Analysis, 88(2), 365-411.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"O. Ledoit, M. Wolf (2018) Analytical Nonlinear Shrinkage of Large-Dimensional Covariance Matrices, Working Paper No. 264, University of Zurich.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"L Molgedey, H.G. Schuster (1994) Separation of a Mixture of Independent Signals using Time Delayed Correlations. Physical Review Letters, 72, 3634-3636.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"K. Pearson (1901) On Lines and Planes of Closest Fit to Systems of Points in Space. Philosophical Magazine, 2(11), 559â€“572.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"D.-T. Pham (2000) Joint approximate diagonalization of positive definite matrices, SIAM Journal on Matrix Analysis and Applications, 22(4), 1136â€“1152.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"J.R. Schott (1997) Matrix Analysis for statistics, John Wiley & Sons, New York.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"P. Tichavsky, A. Yeredor (2009), Fast approximate joint diagonalization incorporating weight matrices, IEEE Trans. Signal Process., 57(3), 878â€“891. https://staff.utia.cas.cz/files/Soutez_09/Tichavsky/WEDGE09.pdf","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"L. Tong, V. Soon, Y. Huang, R.W. Liu (1991), A necessary and sufficient condition Waveform-Preserving Blind Estimation of Multiple Independent Sources. IEEE Transactions on Signal Processing, 41(7), 2461-2470.","category":"page"},{"location":"","page":"Documentation","title":"Documentation","text":"K. Usevich, J. Li, P. Comon (2020), Approximate matrix and tensor diagonalization by unitary transformations: convergence of Jacobi-type algorithms, preprint: hal-01998900v3f.","category":"page"},{"location":"#Contents","page":"Documentation","title":"Contents","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"Pages = [       \"index.md\",\n                \"Diagonalizations.md\",\n                \"pca.md\",\n\t\t\t\t\t\t\t\t\"whitening.md\",\n\t\t\t\t\t\t\t\t\"mca.md\",\n                \"cca.md\",\n\t\t\t\t\t\t\t\t\"gmca.md\",\n\t\t\t\t\t\t\t\t\"gcca.md\",\n\t\t\t\t\t\t\t\t\"csp.jl\",\n\t\t\t\t\t\t\t\t\"cstp.jl\",\n\t\t\t\t\t\t\t\t\"ajd.md\",\n\t\t\t\t\t\t\t\t\"majd.md\",\n\t\t\t\t\t\t\t\t\"tools.md\",\n\t\t\t\t\t\t\t\t\"algorithms.md\"\n\t\t]\nDepth = 1","category":"page"},{"location":"#Index","page":"Documentation","title":"Index","text":"","category":"section"},{"location":"","page":"Documentation","title":"Documentation","text":"","category":"page"}]
}
